[
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.255", 
    "title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sequence-to-sequence models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypothesize that these encoder outputs can be compressed to shorten the sequence delivered for decoding. We take Transformer as the testbed and introduce a layer of stochastic gates in-between the encoder and the decoder. The gates are regularized using the expected value of the sparsity-inducing L0penalty, resulting in completely masking-out a subset of encoder outputs. In other words, via joint training, the L0DROP layer forces Transformer to route information through a subset of its encoder states. We investigate the effects of this sparsification on two machine translation and two summarization tasks. Experiments show that, depending on the task, around 40-70% of source encodings can be pruned without significantly compromising quality. The decrease of the output length endows L0DROP with the potential of improving decoding efficiency, where it yields a speedup of up to 1.65x on document summarization tasks against the standard Transformer. We analyze the L0DROP behaviour and observe that it exhibits systematic preferences for pruning certain word types, e.g., function words and punctuation get pruned most. Inspired by these observations, we explore the feasibility of specifying rule-based patterns that mask out encoder outputs based on information such as part-of-speech tags, word frequency and word position.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 47, 
    "authors": [
      "Biao Zhang", 
      "Ivan Titov", 
      "Rico Sennrich"
    ], 
    "topics": [
      "Encoder", 
      "Transformer", 
      "Automatic summarization", 
      "Logic programming", 
      "Machine translation", 
      "Testbed", 
      "Sparse matrix", 
      "Part-of-speech tagging", 
      "Word lists by frequency", 
      "Speedup", 
      "Asch conformity experiments"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.200", 
    "title": "To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%. Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 18, 
    "authors": [
      "Sinong Wang", 
      "Madian Khabsa", 
      "Hao Ma"
    ], 
    "topics": [
      "Supervised learning", 
      "Language model", 
      "Document classification", 
      "Downstream (software development)", 
      "Scratch (programming language)", 
      "Long short-term memory", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-0211", 
    "title": "Utilization of Nganasan digital resources: a statistical approach to vowel harmony", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "According to the wide-spread belief, although Nganasan has vowel harmony, the harmonic class of a given stem is unpredictable, completely lexicalized. The research made on two different digital sources of Nganasan (a lexicon of a morphological analyzer with harmonic class of the stems tagged and a morphologically annotated corpus) shows that in most of the cases the harmonic class of stems is well predictable based on the vowels in it. Nganasan vowels belong to two harmonic classes except for one neutral vowel.", 
    "year": 2018, 
    "venue": "", 
    "references": 8, 
    "authors": [
      "L. Fejes"
    ], 
    "topics": [
      "Rounding", 
      "Lexicon", 
      "Tracer", 
      "Body of uterus", 
      "Class", 
      "Analyzer Device Component"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1413", 
    "title": "A large-scale study of the effects of word frequency and predictability in naturalistic reading", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A number of psycholinguistic studies have factorially manipulated words\u2019 contextual predictabilities and corpus frequencies and shown separable effects of each on measures of human sentence processing, a pattern which has been used to support distinct mechanisms underlying prediction on the one hand and lexical retrieval on the other. This paper examines the generalizability of this finding to more realistic conditions of sentence processing by studying effects of frequency and predictability in three large-scale naturalistic reading corpora. Results show significant effects of word frequency and predictability in isolation but no effect of frequency over and above predictability, and thus do not provide evidence of distinct mechanisms. The non-replication of separable effects in a naturalistic setting raises doubts about the existence of such a distinction in everyday sentence comprehension. Instead, these results are consistent with previous claims that apparent effects of frequency are underlyingly effects of predictability.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 52, 
    "authors": [
      "Cory Shain"
    ], 
    "topics": [
      "Word lists by frequency", 
      "Text corpus", 
      "Sentence boundary disambiguation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.168", 
    "title": "Amherst685 at SemEval-2021 Task 7: Joint Modeling of Classification and Regression for Humor and Offense", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our submission to theSemEval\u201921: Task 7- HaHackathon: Detecting and Rating Humor and Offense. In this challenge, we explore intermediate finetuning, backtranslation augmentation, multitask learning, and ensembling of different language models. Curiously, intermediate finetuning and backtranslation do not improve performance, while multitask learning and ensembling do improve performance. We explore why intermediate finetuning and backtranslation do not provide the same benefit as other natural language processing tasks and offer insight into the errors that our model makes. Our best performing system ranks 7th on Task 1bwith an RMSE of 0.5339", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 17, 
    "authors": [
      "Brian Zylich", 
      "Akshay Gugnani", 
      "Gabriel Brookman", 
      "Nicholas Samoray"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.case-1.18", 
    "title": "IBM MNLP IE at CASE 2021 Task 1: Multigranular and Multilingual Event Detection on Protest News", 
    "fields_of_study": null, 
    "abstract": "In this paper, we present the event detection models and systems we have developed for Multilingual Protest News Detection - Shared Task 1 at CASE 2021. The shared task has 4 subtasks which cover event detection at different granularity levels (from document level to token level) and across multiple languages (English, Hindi, Portuguese and Spanish). To handle data from multiple languages, we use a multilingual transformer-based language model (XLM-R) as the input text encoder. We apply a variety of techniques and build several transformer-based models that perform consistently well across all the subtasks and languages. Our systems achieve an average F_1 score of 81.2. Out of thirteen subtask-language tracks, our submissions rank 1st in nine and 2nd in four tracks.", 
    "year": 2021, 
    "venue": "CASE", 
    "references": 38, 
    "authors": [
      "Parul Awasthy", 
      "Jian Ni", 
      "Ken Barker", 
      "Radu Florian"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075228", 
    "title": "Robust Temporal Processing of News", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news. The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (F-measure) against hand-annotated data. Some initial steps towards tagging event chronologies are also described.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 19, 
    "authors": [
      "I. Mani", 
      "D. G. Wilson"
    ], 
    "topics": [
      "Algorithm", 
      "Temporal annotation", 
      "Temporal expressions", 
      "Regular expression"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1572", 
    "title": "Predicting Humorousness and Metaphor Novelty with Gaussian Process Preference Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language\u2014namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman\u2019s \u03c1 of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best\u2013worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 74, 
    "authors": [
      "Edwin Simpson", 
      "E. Dinh", 
      "Tristan Miller", 
      "Iryna Gurevych"
    ], 
    "topics": [
      "Gaussian process", 
      "Preference learning", 
      "Natural language understanding", 
      "Bigram", 
      "Crowdsourcing", 
      "Tf\u2013idf", 
      "N-gram", 
      "Test set", 
      "Experiment", 
      "Sparse matrix", 
      "Word embedding", 
      "Theory", 
      "Grams", 
      "XOR gate", 
      "Image scaling"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4208", 
    "title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1. An extension in RDF format, including also scripts for data processing, is under development.", 
    "year": 2015, 
    "venue": "LDL@IJCNLP", 
    "references": 34, 
    "authors": [
      "Enrico Santus", 
      "Frances Yung", 
      "Alessandro Lenci", 
      "Chu-Ren Huang"
    ], 
    "topics": [
      "Crowdsourcing", 
      "WordNet", 
      "Open Mind Common Sense", 
      "Silo (dataset)", 
      "BabelNet", 
      "Resource Description Framework", 
      "Consistency (database systems)", 
      "Point of sale", 
      "Linked data", 
      "Word lists by frequency", 
      "Interoperability", 
      "Extraction", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.240", 
    "title": "Controlling Dialogue Generation with Semantic Exemplars", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 63, 
    "authors": [
      "Prakhar Gupta", 
      "Jeffrey P. Bigham", 
      "Yulia Tsvetkov", 
      "Amy Pavel"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1207", 
    "title": "A Computational Cognitive Model of Novel Word Generalization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate for a word. The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals. We present the first computational study of word generalization integrated within a word-learning model. The model simulates child and adult patterns of word generalization in a word-learning task. These patterns arise due to the interaction of type and token frequencies in the input data, an influence often observed in people\u2019s generalization of linguistic categories.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Aida Nematzadeh", 
      "Erin Grant", 
      "Suzanne Stevenson"
    ], 
    "topics": [
      "WordNet", 
      "Type\u2013token distinction", 
      "Computational linguistics", 
      "Computation", 
      "Cognitive model", 
      "Taxonomy (general)", 
      "Vocabulary", 
      "Microsoft Word for Mac", 
      "Exponent bias", 
      "Norm (social)", 
      "Self-replicating machine", 
      "CHI", 
      "Attribute-based encryption"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119303.1119310", 
    "title": "Patent Claim Processing for Readability - Structure Analysis and Term Explanation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Patent corpus processing should be centered around patent claim processing because claims are the most important part in patent specifications. It is common that claims written in Japanese are described in one sentence with peculiar style and wording and are difficult to understand for ordinary people. The peculiarity is caused by structural complexity of the sentences and many difficult terms used in the description. We have already proposed a framework to represent the structure of patent claims and a method to automatically analyze it. We are currently investigating a method to clarify terms in patent claims and to find the explanatory portions from the detailed description part of the patent specifications. Through both approaches, we believe we can improve readability of patent claims.", 
    "year": 2003, 
    "venue": "ACL 2003", 
    "references": 19, 
    "authors": [
      "Akihiro Shinmori", 
      "M. Okumura", 
      "Y. Marukawa", 
      "Makoto Iwayama"
    ], 
    "topics": [
      "Structural complexity (applied mathematics)", 
      "explanation", 
      "Patent ductus arteriosus", 
      "Body of uterus", 
      "Specification", 
      "sentence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.758", 
    "title": "A Multi-Perspective Architecture for Semantic Code Search", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code\u2013text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 13, 
    "authors": [
      "Rajarshi Haldar", 
      "Lingfei Wu", 
      "Jinjun Xiong", 
      "J. Hockenmaier"
    ], 
    "topics": [
      "Software repository", 
      "Natural language user interface", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.conll-1.15", 
    "title": "Acquiring language from speech by learning to remember and predict", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.", 
    "year": 2020, 
    "venue": "CONLL", 
    "references": 114, 
    "authors": [
      "Cory Shain", 
      "M. Elsner"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-1163", 
    "title": "Template Kernels for Dependency Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A common approach to dependency parsing is scoring a parse via a linear function of a set of indicator features. These features are typically manually constructed from templates that are applied to parts of the parse tree. The templates define which properties of a part should combine to create features. Existing approaches consider only a small subset of the possible combinations, due to statistical and computational efficiency considerations. In this work we present a novel kernel which facilitates efficient parsing with feature representations corresponding to a much larger set of combinations. We integrate the kernel into a parse reranking system and demonstrate its effectiveness on four languages from the CoNLL-X shared task. 1", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 28, 
    "authors": [
      "Hillel Taub-Tabib", 
      "Yoav Goldberg", 
      "A. Globerson"
    ], 
    "topics": [
      "Parsing", 
      "Parse tree", 
      "Feature vector", 
      "Estimation theory", 
      "Linear function", 
      "Maxima and minima", 
      "Kernel (operating system)", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.279", 
    "title": "Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 33, 
    "authors": [
      "Yufan Zhao", 
      "Can Xu", 
      "Wei Wu"
    ], 
    "topics": [
      "Local optimum", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5057", 
    "title": "LasigeBioTM at MEDIQA 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to diseases, drugs, symptoms, or medical procedures. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three tasks concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each task. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of accuracy, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman\u2019s rank correlation values using the entities recognized by MER.", 
    "year": 2019, 
    "venue": "BioNLP@ACL", 
    "references": 9, 
    "authors": [
      "Andre Lamurias", 
      "F. Couto"
    ], 
    "topics": [
      "Question answering", 
      "Named-entity recognition", 
      "Transformer", 
      "Test set", 
      "Mer", 
      "Transformers", 
      "Named entity", 
      "Relationship extraction", 
      "Deep learning", 
      "Software quality assurance", 
      "Native-language identification", 
      "Natural language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4311", 
    "title": "IHS_RD: Lexical Normalization for English Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the Twitter lexical normalization system submitted by IHS R&D Belarus team for the ACL 2015 workshop on noisy user-generated text. The proposed system consists of two components: a CRFbased approach to identify possible normalization candidates, and a post-processing step in an attempt to normalize words that do not have normalization variants in the lexicon. Evaluation on the test data set showed that our unconstrained system achieved the Fmeasure of 0.8272 (rank 1 out of 5 submissions for the unconstrained mode, rank 2 out of all 11 submissions).", 
    "year": 2015, 
    "venue": "NUT@IJCNLP", 
    "references": 10, 
    "authors": [
      "Dmitry Supranovich", 
      "Viachaslau Patsepnia"
    ], 
    "topics": [
      "Lexicon", 
      "Video post-processing", 
      "User-generated content", 
      "Test data", 
      "Conditional random field", 
      "Normalize"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.470", 
    "title": "News Editorials: Towards Summarizing Long Argumentative Texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The automatic summarization of argumentative texts has hardly been explored. This paper takes a further step in this direction, targeting news editorials, i.e., opinionated articles with a well-defined argumentation structure. With Webis-EditorialSum-2020, we present a corpus of 1330 carefully curated summaries for 266 news editorials. We evaluate these summaries based on a tailored annotation scheme, where a high-quality summary is expected to be thesis-indicative, persuasive, reasonable, concise, and self-contained. Our corpus contains at least three high-quality summaries for about 90% of the editorials, rendering it a valuable resource for the development and evaluation of summarization technology for long argumentative texts. We further report details of both, an in-depth corpus analysis, and the evaluation of two extractive summarization models.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 60, 
    "authors": [
      "Shahbaz Syed", 
      "Roxanne El Baff", 
      "Johannes Kiesel", 
      "Khalid Al Khatib", 
      "Benno Stein", 
      "Martin Potthast"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-5808", 
    "title": "SAWT: Sequence Annotation Web Tool", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present SAWT, a web-based tool for the annotation of token sequences with an arbitrary set of labels. The key property of the tool is simplicity and ease of use for both annotators and administrators. SAWT runs in any modern browser, including browsers on mobile devices, and only has minimal server-side requirements.", 
    "year": 2016, 
    "venue": "CodeSwitch@EMNLP", 
    "references": 18, 
    "authors": [
      "Younes Samih", 
      "Wolfgang Maier", 
      "Laura Kallmeyer"
    ], 
    "topics": [
      "Server-side", 
      "Client-side", 
      "Requirement", 
      "Usability", 
      "Web application", 
      "Server (computing)", 
      "Mobile device", 
      "Text Encoding Initiative", 
      "Annotation", 
      "Server (computer)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3314", 
    "title": "The DCU-ICTCAS MT system at WMT 2014 on German-English Translation Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the DCU submission to WMT 2014 on German-English translation task. Our system uses phrasebased translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation. Our final submission is the result of system combination on several systems which have different pre-processing and alignments.", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 20, 
    "authors": [
      "Liangyou Li", 
      "Xiaofeng Wu", 
      "S. C. Va\u00edllo", 
      "Jun Xie", 
      "Andy Way", 
      "Qun Liu"
    ], 
    "topics": [
      "Language model", 
      "Interpolation", 
      "Preprocessor", 
      "Sequence alignment", 
      "Moses"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-1407", 
    "title": "Anaphora Resolution for Improving Spatial Relation Extraction from Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Spatial relation extraction from generic text is a challenging problem due to the ambiguity of the prepositions spatial meaning as well as the nesting structure of the spatial descriptions. In this work, we highlight the difficulties that the anaphora can make in the extraction of spatial relations. We use external multi-modal (here visual) resources to find the most probable candidates for resolving the anaphoras that refer to the landmarks of the spatial relations. We then use global inference to decide jointly on resolving the anaphora and extraction of the spatial relations. Our preliminary results show that resolving anaphora improves the state-ofthe-art results on spatial relation extraction.", 
    "year": 2018, 
    "venue": "", 
    "references": 29, 
    "authors": [
      "Umar Manzoor", 
      "Parisa Kordjamshidi"
    ], 
    "topics": [
      "Anaphora (linguistics)", 
      "Relationship extraction", 
      "F1 score", 
      "Description", 
      "Spatial anti-aliasing", 
      "Modal logic", 
      "Probability", 
      "Numerous", 
      "Inference", 
      "Physical object", 
      "Generic Drugs"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075178.1075183", 
    "title": "Finding Non-local Dependencies: Beyond Pattern Matching", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The pattern-matching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a non-local dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in (Johnson, 2002).", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 15, 
    "authors": [
      "V. Jijkoun"
    ], 
    "topics": [
      "Pattern matching", 
      "Algorithm", 
      "Precision and recall", 
      "Treebank", 
      "Machine learning", 
      "Statistical classification", 
      "Phrase structure rules"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.212", 
    "title": "Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 54, 
    "authors": [
      "Kunal Chawla", 
      "Diyi Yang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1229", 
    "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public {this http URL}. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 28, 
    "authors": [
      "Baotian Hu", 
      "Q. Chen", 
      "Fangze Zhu"
    ], 
    "topics": [
      "Automatic summarization", 
      "Recurrent neural network", 
      "Text corpus", 
      "Relevance", 
      "Text-based (computing)", 
      "Artificial neural network", 
      "Web resource", 
      "Baseline (configuration management)", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073012.1073021", 
    "title": "Alternative Phrases and Natural Languages Information Retrieval", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a formal analysis for a large class of words called alternative markers, which includes other(than), such(as), and besides. These words appear frequently enough in dialog to warrant serious attention, yet present natural language search engines perform poorly on queries containing them. I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine's operational semantics. The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible.", 
    "year": 2001, 
    "venue": "ACL", 
    "references": 29, 
    "authors": [
      "Gann Bierner"
    ], 
    "topics": [
      "Operational semantics", 
      "Natural language user interface", 
      "Web search engine", 
      "Information retrieval", 
      "Pattern matching", 
      "Heuristic", 
      "Anaphora (linguistics)", 
      "dialog", 
      "Approximation", 
      "Assertion (software development)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.90", 
    "title": "When Do You Need Billions of Words of Pretraining Data?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "NLP is currently dominated by general-purpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? We adopt four probing methods---classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks---and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 53, 
    "authors": [
      "Yian Zhang", 
      "Alex Warstadt", 
      "Haau-Sing Li", 
      "Samuel R. Bowman"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.26", 
    "title": "Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in Iran using NLP", 
    "fields_of_study": [
      "Computer Science", 
      "History"
    ], 
    "abstract": "Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, and feelings about a wide range of issues. In this study, using more than 530,000 original tweets in Persian/Farsi on COVID-19, we analyzed the topics discussed among users, who are mainly Iranians, to gauge and track the response to the pandemic and how it evolved over time. We applied a combination of manual annotation of a random sample of tweets and topic modeling tools to classify the contents and frequency of each category of topics. We identified the top 25 topics among which living experience under home quarantine emerged as a major talking point. We additionally categorized broader content of tweets that shows satire, followed by news, is the dominant tweet type among the Iranian users. While this framework and methodology can be used to track public response to ongoing developments related to COVID-19, a generalization of this framework can become a useful framework to gauge Iranian public reaction to ongoing policy measures or events locally and internationally.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 15, 
    "authors": [
      "Pedram Hosseini", 
      "Poorya Hosseini", 
      "David A. Broniatowski"
    ], 
    "topics": [
      "Natural language processing", 
      "Topic model", 
      "Iranian.com", 
      "Categorization"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.gebnlp-1.6", 
    "title": "Sexism in the Judiciary: The Importance of Bias Definition in NLP and In Our Courts", 
    "fields_of_study": null, 
    "abstract": "We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms\u2019 inconsistent results are consequences of prior research\u2019s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., \u2018salary,\u2019 \u2018job,\u2019 and \u2018boss\u2019 to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers\u2019 own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method\u2019s performance, we regress our results of bias in case law against U.S census data of women\u2019s participation in the workforce in the last 100 years.", 
    "year": 2021, 
    "venue": "GEBNLP", 
    "references": 19, 
    "authors": [
      "Noa Baker Gillis"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075096.1075100", 
    "title": "Fast Methods for Kernel-Based Text Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "Taku Kudo", 
      "Yuji Matsumoto"
    ], 
    "topics": [
      "Natural language processing", 
      "Linear classifier", 
      "Kernel method", 
      "Association rule learning", 
      "Support vector machine", 
      "Algorithm", 
      "Heuristic", 
      "Kernel (operating system)", 
      "Computation", 
      "Shallow parsing", 
      "Text segmentation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1071", 
    "title": "Explicit Cross-lingual Pre-training for Unsupervised Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our method can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 31, 
    "authors": [
      "Shuo Ren", 
      "Yu Wu", 
      "Shujie Liu", 
      "M. Zhou", 
      "Shuai Ma"
    ], 
    "topics": [
      "Machine translation", 
      "N-gram", 
      "Language model", 
      "Teaching method", 
      "Encoder", 
      "Randomness", 
      "Grams", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1345", 
    "title": "Domain adaptation for part-of-speech tagging of noisy user-generated text", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "The performance of a Part-of-speech (POS) tagger is highly dependent on the domain of the processed text, and for many domains there is no or only very little training data available. This work addresses the problem of POS tagging noisy user-generated text using a neural network. We propose an architecture that trains an out-of-domain model on a large newswire corpus, and transfers those weights by using them as a prior for a model trained on the target domain (a data-set of German Tweets) for which there is very little annotations available. The neural network has a standard bidirectional LSTM at its core. However, we find it crucial to also encode a set of task-specific features, and to obtain reliable (source-domain and target-domain) word representations. Experiments with different regularization techniques such as early stopping, dropout and fine-tuning the domain adaptation prior weights are conducted. Our best model uses external weights from the out-of-domain model, as well as feature embeddings, pre-trained word and sub-word embeddings and achieves a tagging accuracy of slightly over 90%, improving on the previous state of the art for this task.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 19, 
    "authors": [
      "Luisa M\u00e4rz", 
      "Dietrich Trautmann", 
      "Benjamin Roth"
    ], 
    "topics": [
      "Domain adaptation", 
      "Part-of-speech tagging", 
      "Artificial neural network", 
      "Early stopping", 
      "Domain model", 
      "Deep learning", 
      "Dropout (neural networks)", 
      "User-generated content", 
      "Experiment", 
      "Brill tagger", 
      "Manifold regularization", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1219840.1219897", 
    "title": "Log-Linear Models for Word Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models.", 
    "year": 2005, 
    "venue": "ACL", 
    "references": 22, 
    "authors": [
      "Yang Liu", 
      "Qun Liu", 
      "Shouxun Lin"
    ], 
    "topics": [
      "Linear model", 
      "Bitext word alignment", 
      "Log-linear model", 
      "FO (complexity)", 
      "Statistical machine translation", 
      "Text corpus", 
      "Bilingual dictionary", 
      "Search algorithm", 
      "Brown Corpus", 
      "Unsupervised learning", 
      "Supervised learning", 
      "Statistical model", 
      "Compiler", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Mathematical model", 
      "Data structure alignment", 
      "Serial Digital Video Out", 
      "Unified Model", 
      "Microsoft Word for Mac"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.467", 
    "title": "CosMo: Conditional Seq2Seq-based Mixture Model for Zero-Shot Commonsense Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Commonsense reasoning refers to the ability of evaluating a social situation and acting accordingly. Identification of the implicit causes and effects of a social context is the driving capability which can enable machines to perform commonsense reasoning. The dynamic world of social interactions requires context-dependent on-demand systems to infer such underlying information. However, current approaches in this realm lack the ability to perform commonsense reasoning upon facing an unseen situation, mostly due to incapability of identifying a diverse range of implicit social relations. Hence they fail to estimate the correct reasoning path. In this paper, we present Conditional Seq2Seq-based Mixture model (CosMo), which provides us with the capabilities of dynamic and diverse content generation. We use CosMo to generate context-dependent clauses, which form a dynamic Knowledge Graph (KG) on-the-fly for commonsense reasoning. To show the adaptability of our model to context-dependant knowledge generation, we address the task of zero-shot commonsense question answering. The empirical results indicate an improvement of up to +5.2% over the state-of-the-art models.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 36, 
    "authors": [
      "Farhad Moghimifar", 
      "Lizhen Qu", 
      "Yue Zhuo", 
      "Mahsa Baktashmotlagh", 
      "Gholamreza Haffari"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.3115/981251.981287", 
    "title": "Salience: the Key to the Selection Problem in Natural Language Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We argue that in domains where a strong notion of salience can be defined, it can be used to provide: (1) an elegant solution to the selection problem, i.e. the problem of how to decide whether a given fact should or should not be mentioned in the text; and (2) a simple and direct control framework for the entire deep generation process, coordinating proposing, planning, and realization. (Deep generation involves reasoning about conceptual and rhetorical facts, as opposed to the narrowly linguistic reasoning that takes place during realization.) We report on an empirical study of salience in pictures of natural scenes, and its use in a computer program that generates descriptive paragraphs comparable to those produced by people.", 
    "year": 1982, 
    "venue": "ACL", 
    "references": 14, 
    "authors": [
      "E. J. Conklin", 
      "David D. McDonald"
    ], 
    "topics": [
      "Natural language generation", 
      "Selection algorithm", 
      "Computer program", 
      "Heuristic", 
      "Data structure", 
      "Planning", 
      "Information science", 
      "Domain-driven design", 
      "Information and Computer Science", 
      "Database", 
      "Online and offline", 
      "Compiler", 
      "Nick McKeown", 
      "Dart (programming language)", 
      "Backward chaining", 
      "Business object"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6541", 
    "title": "BENGAL: An Automatic Benchmark Generator for Entity Recognition and Linking", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The manual creation of gold standards for named entity recognition and entity linking is time- and resource-intensive. Moreover, recent works show that such gold standards contain a large proportion of mistakes in addition to being difficult to maintain. We hence present Bengal, a novel automatic generation of such gold standards as a complement to manually created benchmarks. The main advantage of our benchmarks is that they can be readily generated at any time. They are also cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on benchmarks in English generated by Bengal and on 16 benchmarks created manually. We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both Brazilian Portuguese and Spanish. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing benchmarks. Our approach is open-source. Our experimental results are available at http://faturl.com/bengalexpinlg and the code at https://github.com/dice-group/BENGAL.", 
    "year": 2018, 
    "venue": "INLG", 
    "references": 46, 
    "authors": [
      "A. N. Ngomo", 
      "Michael R\u00f6der", 
      "Diego Moussallem", 
      "Ricardo Usbeck", 
      "Ren\u00e9 Speck"
    ], 
    "topics": [
      "Benchmark (computing)", 
      "Named-entity recognition", 
      "Entity linking", 
      "Hypertext Transfer Protocol", 
      "Open-source software", 
      "Web standards"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1162", 
    "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 40, 
    "authors": [
      "He He", 
      "Anusha Balakrishnan", 
      "Mihail Eric", 
      "Percy Liang"
    ], 
    "topics": [
      "Knowledge Graph", 
      "Dialog system", 
      "Nonlinear gameplay", 
      "Logic programming", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2088", 
    "title": "Contextualized Word Representations for Reading Comprehension", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Reading a document and extracting an answer to a question about its content has attracted substantial attention recently. While most work has focused on the interaction between the question and the document, in this work we evaluate the importance of context when the question and document are processed independently. We take a standard neural architecture for this task, and show that by providing rich contextualized word representations from a large pre-trained language model as well as allowing the model to choose between context-dependent and context-independent word representations, we can obtain dramatic improvements and reach performance comparable to state-of-the-art on the competitive SQuAD dataset.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 55, 
    "authors": [
      "Shimi Salant", 
      "Jonathan Berant"
    ], 
    "topics": [
      "List comprehension", 
      "Language model", 
      "Machine learning", 
      "Open reading frame"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1445", 
    "title": "EigenSent: Spectral sentence embeddings using higher-order Dynamic Mode Decomposition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. Most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. In this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. In particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order Dynamic Mode Decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. It is empirically observed that this approach, which we call EigenSent, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. To the best of the authors\u2019 knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. We test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. Moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by EigenSent with simple word vector averaging achieves state-of-the-art results.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 34, 
    "authors": [
      "S. Kayal", 
      "G. Tsatsaronis"
    ], 
    "topics": [
      "Word embedding", 
      "Algorithm", 
      "Concatenation", 
      "Spectral method", 
      "Sentence extraction", 
      "Sentence boundary disambiguation", 
      "Unsupervised learning", 
      "Multidimensional signal processing", 
      "Quasiperiodicity", 
      "Design rationale", 
      "Singular value decomposition", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4308", 
    "title": "Pitfalls in the Evaluation of Sentence Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Deep learning models continuously break new records across different NLP tasks. At the same time, their success exposes weaknesses of model evaluation. Here, we compile several key pitfalls of evaluation of sentence embeddings, a currently very popular NLP paradigm. These pitfalls include the comparison of embeddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks. Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research. Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings.", 
    "year": 2019, 
    "venue": "RepL4NLP@ACL", 
    "references": 31, 
    "authors": [
      "Steffen Eger", 
      "Andreas R\u00fcckl\u00e9", 
      "Iryna Gurevych"
    ], 
    "topics": [
      "Sentence extraction", 
      "Deep learning", 
      "Programming paradigm", 
      "Compiler", 
      "Weakness", 
      "Natural language processing", 
      "NINL gene", 
      "Evaluation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1078", 
    "title": "Biography-Dependent Collaborative Entity Archiving for Slot Filling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Knowledge Base Population (KBP) tasks, such as slot filling, show the particular importance of entity-oriented automatic relevant document acquisition. Rich, diverse and reliable relevant documents satisfy the fundamental requirement that a KBP system explores the nature of an entity. Towards the bottleneck problem between comprehensiveness and definiteness of acquisition, we propose a collaborative archiving method. In particular we introduce topic modeling methodologies into entity biography profiling, so as to build a bridge between fuzzy and exact matching. On one side, we employ the topics in a small-scale high-quality relevant documents (i.e., exact matching results) to summarize the life slices of a target entity (i.e., biography), and on the other side, we use the biography as a reliable reference material to detect new truly relevant documents from a large-scale partially complete pseudo-feedback (i.e., fuzzy matching results). We leverage the archiving method to enhance slot filling systems. Experiments on KBP corpus show significant improvement over stateof-the-art.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 60, 
    "authors": [
      "Yu Hong", 
      "Xiaobin Wang", 
      "Yadong Chen", 
      "J. Wang", 
      "Tongtao Zhang", 
      "Heng Ji"
    ], 
    "topics": [
      "Archive", 
      "Entity", 
      "Topic model", 
      "Knowledge acquisition", 
      "Causal filter", 
      "Text corpus", 
      "Knowledge base", 
      "Verification and validation", 
      "Interference (communication)", 
      "ACE", 
      "File archiver"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2601", 
    "title": "Words: Evaluative, Emotional, Colourful, Musical!", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "of the talk Beyond literal meaning, words have associations with sentiment, emotion, colour, and even music. Identifying such associations is of substantial benefit for information visualization, data sonification, and sentiment analysis, which in turn have applications in commerce, education, art, and health. I will present methods to generate highcoverage resources that capture such associations. I will show how these resources can be used for analyzing emotions in text, detecting personality from essays, and generating music from novels. Finally, I will show how word-sentiment association lexicons have helped create the top-ranking systems in recent SemEval competitions on the sentiment analysis of social media posts.", 
    "year": 2014, 
    "venue": "WASSA@ACL", 
    "references": 0, 
    "authors": [
      "Saif M. Mohammad"
    ], 
    "topics": [
      "Sentiment analysis", 
      "SemEval", 
      "Information visualization", 
      "Color", 
      "Sonification", 
      "Social media", 
      "Sensor", 
      "Lexicon", 
      "Literal (mathematical logic)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.408", 
    "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the \u2018reasoning\u2019 behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 75, 
    "authors": [
      "Jay DeYoung", 
      "Sarthak Jain", 
      "Nazneen Rajani", 
      "Eric P. Lehman", 
      "Caiming Xiong", 
      "R. Socher", 
      "Byron C. Wallace"
    ], 
    "topics": [
      "Benchmark (computing)", 
      "Natural language processing", 
      "Deep learning", 
      "Documentation", 
      "Align (company)", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1208", 
    "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 67, 
    "authors": [
      "Amir Zadeh", 
      "Paul Pu Liang", 
      "Soujanya Poria", 
      "E. Cambria", 
      "Louis-Philippe Morency"
    ], 
    "topics": [
      "Multimodal interaction", 
      "Sentiment analysis", 
      "Emotion recognition", 
      "Natural language processing", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.94", 
    "title": "Towards Modeling the Style of Translators in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 30, 
    "authors": [
      "Yue Wang", 
      "Cuong Hoang", 
      "Marcello Federico"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.sigmorphon-1.15", 
    "title": "Linguistic Knowledge in Multilingual Grapheme-to-Phoneme Conversion", 
    "fields_of_study": null, 
    "abstract": "This paper documents the UBC Linguistics team\u2019s approach to the SIGMORPHON 2021 Grapheme-to-Phoneme Shared Task, concentrating on the low-resource setting. Our systems expand the baseline model with simple modifications informed by syllable structure and error analysis. In-depth investigation of test-set predictions shows that our best model rectifies a significant number of mistakes compared to the baseline prediction, besting all other submissions. Our results validate the view that careful error analysis in conjunction with linguistic knowledge can lead to more effective computational modeling.", 
    "year": 2021, 
    "venue": "SIGMORPHON", 
    "references": 24, 
    "authors": [
      "Roger Yu-Hsiang Lo", 
      "Garrett Nicolai"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2504", 
    "title": "The ACL Anthology: Current State and Future Directions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Association of Computational Linguistic\u2019s Anthology is the open source archive, and the main source for computational linguistics and natural language processing\u2019s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology\u2019s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.", 
    "year": 2018, 
    "venue": "", 
    "references": 14, 
    "authors": [
      "D. Gildea", 
      "Min-Yen Kan", 
      "Nitin Madnani", 
      "Christoph Teichmann", 
      "Mart\u00edn Federico Villalba"
    ], 
    "topics": [
      "Docker", 
      "Natural language processing", 
      "Software engineering", 
      "Uptime", 
      "Computational linguistics", 
      "Archive", 
      "Software development", 
      "Open-source software", 
      "System administrator", 
      "Scientific literature", 
      "Documentation", 
      "Download", 
      "Server (computing)", 
      "Data Base Management", 
      "Adjective Check List", 
      "Paper", 
      "Computation", 
      "Antibodies, Anticardiolipin", 
      "interest"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4318", 
    "title": "USZEGED: Correction Type-sensitive Normalization of English Tweets Using Efficiently Indexed n-gram Statistics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the framework applied by team USZEGED at the \u201cLexical Normalisation for English Tweets\u201d shared task. Our approach first employs a CRFbased sequence labeling framework to decide the kind of corrections the individual tokens require, then performs the necessary modifications relying on external lexicons and a massive collection of efficiently indexed n-gram statistics from English tweets. Our solution is based on the assumption that from the context of the OOV words, it is possible to reconstruct its IV equivalent, as there are users who use the standard English form of the OOV word within the same context. Our approach achieved an F-score of 0.8052, being the second best one among the unconstrained submissions, the category our submission also belongs to.", 
    "year": 2015, 
    "venue": "NUT@IJCNLP", 
    "references": 19, 
    "authors": [
      "G\u00e1bor Berend", 
      "Ervin Tasn\u00e1di"
    ], 
    "topics": [
      "Sequence labeling", 
      "N-gram", 
      "Lexicon", 
      "F1 score"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.DASH-1.11", 
    "title": "Building Low-Resource NER Models Using Non-Speaker Annotations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In low-resource natural language processing (NLP), the key problems are a lack of target language training data, and a lack of native speakers to create it. Cross-lingual methods have had notable success in addressing these concerns, but in certain common circumstances, such as insufficient pre-training corpora or languages far from the source language, their performance suffers. In this work we propose a complementary approach to building low-resource Named Entity Recognition (NER) models using \u201cnon-speaker\u201d (NS) annotations, provided by annotators with no prior experience in the target language. We recruit 30 participants in a carefully controlled annotation experiment with Indonesian, Russian, and Hindi. We show that use of NS annotators produces results that are consistently on par or better than cross-lingual methods built on modern contextual representations, and have the potential to outperform with additional effort. We conclude with observations of common annotation patterns and recommended implementation practices, and motivate how NS annotations can be used in addition to prior methods for improved performance.", 
    "year": 2021, 
    "venue": "DASH", 
    "references": 44, 
    "authors": [
      "Tatiana Tsygankova", 
      "F. Marini", 
      "Stephen Mayhew", 
      "D. Roth"
    ], 
    "topics": [
      "Natural language processing", 
      "Named-entity recognition", 
      "Text corpus", 
      "Compiler", 
      "Named entity"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2807", 
    "title": "Rules, Analogy, and Social Factors Codetermine Past-tense Formation Patterns in English", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We investigate past-tense formation preferences for five irregular English verb classes. We gathered data on a large scale using a nonce probe study implemented on Amazon Mechanical Turk. We compare a Minimal Generalization Learner (which infers stochastic rules) with a Generalized Context Model (which evaluates new items via analogy with existing items) as models of participant choices. Overall, the GCM is a better predictor, but the the MGL provides some additional predictive power. Because variation across speakers is greater than variation across items, we also explore individual-level factors as predictors. Females exhibited significantly more categorical choices than males, a finding that can be related to results in sociolinguistics.", 
    "year": 2014, 
    "venue": "SIGMORPHON/SIGFSM", 
    "references": 29, 
    "authors": [
      "P. R\u00e1cz", 
      "C. Beckner", 
      "J. Hay", 
      "J. Pierrehumbert"
    ], 
    "topics": [
      "Amazon Mechanical Turk", 
      "Cryptographic nonce", 
      "Multiple granularity locking", 
      "Google Cloud Messaging", 
      "The Turk", 
      "Kerrison Predictor"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.361", 
    "title": "Adapting Coreference Resolution for Processing Violent Death Narratives", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals.In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English: violent death nar-ratives from the USA\u2019s Centers for DiseaseControl\u2019s (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 19, 
    "authors": [
      "Ankith Uppunda", 
      "S. Cochran", 
      "Jacob G. Foster", 
      "Alina Arseniev-Koehler", 
      "V. Mays", 
      "Kai-Wei Chang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-DEMOS.14", 
    "title": "Annobot: Platform for Annotating and Creating Datasets through Conversation with a Chatbot", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we introduce Annobot: a platform for annotating and creating datasets through conversation with a chatbot. This natural form of interaction has allowed us to create a more accessible and flexible interface, especially for mobile devices. Our solution has a wide range of applications such as data labelling for binary, multi-class/label classification tasks, preparing data for regression problems, or creating sets for issues such as machine translation, question answering or text summarization. Additional features include pre-annotation, active sampling, online learning and real-time inter-annotator agreement. The system is integrated with the popular messaging platform: Facebook Messanger. Usability experiment showed the advantages of the proposed platform compared to other labelling tools. The source code of Annobot is available under the GNU LGPL license at https://github.com/rafalposwiata/annobot.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 13, 
    "authors": [
      "Rafal Poswiata", 
      "Michal Perelkiewicz"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/S14-2004", 
    "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.", 
    "year": 2014, 
    "venue": "COLING 2014", 
    "references": 15, 
    "authors": [
      "Maria Pontiki", 
      "Dimitris Galanis", 
      "John Pavlopoulos", 
      "Haris Papageorgiou", 
      "Ion Androutsopoulos", 
      "S. Manandhar"
    ], 
    "topics": [
      "Sentiment analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1158", 
    "title": "Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Yubo Chen", 
      "Hang Yang", 
      "Kang Liu", 
      "Jun Zhao", 
      "Yantao Jia"
    ], 
    "topics": [
      "Interdependence", 
      "Regular expression", 
      "ACE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1115", 
    "title": "SemEval-2018 Task 9: Hypernym Discovery", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions.codalab.org/competitions/17119.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 58, 
    "authors": [
      "Jos\u00e9 Camacho-Collados", 
      "Claudio Delli Bovi", 
      "Luis Espinosa Anke", 
      "Sergio Oramas", 
      "Tommaso Pasini", 
      "Enrico Santus", 
      "Vered Shwartz", 
      "R. Navigli", 
      "Horacio Saggion"
    ], 
    "topics": [
      "SemEval", 
      "Binary classification", 
      "Performance", 
      "Natural language processing", 
      "Downstream (software development)", 
      "Benchmark (computing)", 
      "Text corpus", 
      "Supervised learning", 
      "Sergio Verd\u00fa", 
      "Network architecture", 
      "Discovery system", 
      "Artificial neural network", 
      "Semiconductor industry", 
      "Don Woods (programmer)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1093", 
    "title": "Hierarchical Pointer Net Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 34, 
    "authors": [
      "L. Liu", 
      "Xiang Lin", 
      "Shafiq R. Joty", 
      "Simeng Han", 
      "Lidong Bing"
    ], 
    "topics": [
      "Pointer (computer programming)", 
      "Inductive bias", 
      "Time complexity", 
      "Tree structure", 
      "Top-down parsing", 
      "Top-down and bottom-up design", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-2902", 
    "title": "Priming vs. Inhibition of Optional Infinitival \u201cto\u201d", 
    "fields_of_study": [
      "Chemistry"
    ], 
    "abstract": "The word to that precedes verbs in English infinitives is optional in at least two environments: in what Wasow et al. (2015) have called the \u201cdo-be construction\u201d, and in the complement of help, explored in the present work. Wasow et al. found that a preceding infinitival to increases the use of optional following to in the environment they examined, but the use of to in the complement of help is reduced following to help. We examine two hypotheses regarding why the same function word is primed by prior use in one construction and inhibited in another. We then test predictions made by the two hypotheses, finding support for one of them.", 
    "year": 2019, 
    "venue": "", 
    "references": 34, 
    "authors": [
      "R. Melnick", 
      "T. Wasow"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2701", 
    "title": "Findings of the Second Workshop on Neural Machine Translation and Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop\u2019s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.", 
    "year": 2018, 
    "venue": "NMT@ACL", 
    "references": 38, 
    "authors": [
      "Alexandra Birch", 
      "A. Finch", 
      "Minh-Thang Luong", 
      "Graham Neubig", 
      "Yusuke Oda"
    ], 
    "topics": [
      "Neural machine translation", 
      "Domain adaptation", 
      "Convolutional neural network", 
      "Computational linguistics", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2919", 
    "title": "Verb-centered Sentiment Inference with Description Logics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce description logics as a means to carry out sentiment inferences triggered by some verbs on their seman- tic roles. Verbs might impose polar effects on some roles, but also have polar expectations on other roles. For instance, an entity inherits a positive polarity just by being actor of some verb (\u201cshe succeeds\u201d). More complicated scenarios arise if we take subclause embeddings, negation and polarity conflicts into consideration. Polarity propagation and effect inversion need to be coped with, then. We have implemented a prototype in OWL covering a substantial subset of our verb lexicon covering about 140 German verbs.", 
    "year": 2015, 
    "venue": "WASSA@EMNLP", 
    "references": 17, 
    "authors": [
      "M. Klenner"
    ], 
    "topics": [
      "Description logic", 
      "Prototype", 
      "Experiment", 
      "Lexicon", 
      "Software propagation", 
      "CMA-ES", 
      "Switzerland"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2917", 
    "title": "Predicting Ratings for New Movie Releases from Twitter Content", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With microblogging platforms such as Twitter generating huge amounts of textual data every day, the possibilities of knowledge discovery through Twitter data becomes increasingly relevant. Similar to the public voting mechanism on websites such as the Internet Movie Database (IMDb) that aggregates movies ratings, Twitter content contains reflections of public opinion about movies. This study aims to explore the use of Twitter content as textual data for predictive text mining. In this study, a corpus of tweets was compiled to predict the rating scores of newly released movies on IMDb. Predictions were done with several different machine learning algorithms, exploring both regression and classification methods. In addition, this study explores the use of several different kinds of textual features in the machine learning tasks. Results show that prediction performance based on textual features derived from our corpus of tweets improved on the baseline for both regression and classification tasks.", 
    "year": 2015, 
    "venue": "WASSA@EMNLP", 
    "references": 15, 
    "authors": [
      "Wernard Schmit", 
      "S. Wubben"
    ], 
    "topics": [
      "Machine learning", 
      "Text corpus", 
      "Internet Movie Database (IMDb)", 
      "Text mining", 
      "Predictive text", 
      "Statistical classification", 
      "Amiga Reflections", 
      "Baseline (configuration management)", 
      "Compiler", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-6013", 
    "title": "Commonsense inference in human-robot communication", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Natural language communication between machines and humans are still constrained. The article addresses a gap in natural language understanding about actions, specifically that of understanding commands. We propose a new method for commonsense inference (grounding) of high-level natural language commands into specific action commands for further execution by a robotic system. The method allows to build a knowledge base that consists of a large set of commonsense inferences. The preliminary results have been presented.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 32, 
    "authors": [
      "Aliaksandr Huminski", 
      "Yan Bin Ng", 
      "Kenneth Kwok", 
      "Francis Bond"
    ], 
    "topics": [
      "Natural language understanding", 
      "High- and low-level", 
      "Cognitive robotics", 
      "Knowledge base", 
      "Commonsense knowledge (artificial intelligence)", 
      "WordNet", 
      "Robot", 
      "Natural language processing", 
      "Commonsense reasoning", 
      "Part-of-speech tagging", 
      "Natural language user interface", 
      "Human\u2013robot interaction", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00295", 
    "title": "Paraphrase-Sense-Tagged Sentences", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Abstract Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense-Tagged Sentences (PSTS). Built on the premise that a word\u2019s paraphrases instantiate its fine-grained meanings (i.e., bug has different meanings corresponding to its paraphrases fly and microbe) the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the target word takes on the meaning of the paraphrase. We describe an automatic method based on bilingual pivoting used to enumerate sentences for PSTS, and present two models for ranking PSTS sentences based on their quality. Finally, we demonstrate the utility of PSTS by using it to build a dataset for the task of hypernym prediction in context. Training a model on this automatically generated dataset produces accuracy that is competitive with a model trained on smaller datasets crafted with some manual effort.", 
    "year": 2019, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 64, 
    "authors": [
      "Anne Cocos", 
      "Chris Callison-Burch"
    ], 
    "topics": [
      "Test set", 
      "Lexical substitution", 
      "Natural language processing", 
      "Transformer", 
      "Enumerated type", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-3607", 
    "title": "Keynote - More than meets the ear: Processes that shape dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "What is dialogue, anyway\u2014language produced in alternating turns by two or more speakers? A way to collaboratively accomplish a task or transaction with an agent, whether human or computer? An interactive process by which two people entrain and coordinate their behaviors and mental states? A corpus that can be analyzed to answer a research question? The ways in which researchers conceptualize dialogue affect the assumptions and decisions they make about how to design an experiment, collect or code a corpus, or build a system. Often such assumptions are not explicit. Researchers may decide to characterize, stage, control, or entirely ignore such potentially key factors as the task two people are charged with, their identities, their common ground, or the medium in which dialogue is conducted. Such decisions, especially when left implicit, can affect the products and processes of dialogue in substantial but unanticipated ways; in fact, they can change the results of an experiment. As one example, spoken dialogue experiments often use a simulated partner or confederate in the role of speaker or addressee; just how the confederate is deployed reflects the researcher's explicit theory and implicit assumptions about the nature of dialogue. As another example, sometimes experiments place people in infelicitous situations; this can change the kind of language game people think they're playing. I will cover some implicit assumptions about the nature of dialogue that affect the risks researchers take, and highlight pairs of studies that have found different results, perhaps due to these assumptions. Speaker's Bio: Susan Brennan is Professor of Psychology in the Cognitive Science Program at Stony Brook University (State University of New York), with joint appointments in the Departments of Linguistics and Computer Science. She received her Ph.D. in Cognitive Psychology from Stanford University with a focus on psycholinguistics; her M.S. is from the MIT Media Lab, where she worked on computer-generated caricature and teleconferencing interfaces; and her B.A. is in cultural anthropology from Cornell University. She has worked in industry at Atari Research, Hewlett-Packard Labs, and Apple Computer. Her research interests span language processing in conversation, joint attention, partner-specific adaptation during interactive dialogue, the production and comprehension of referring expressions, lexical entrainment, discourse functions of prosody and intonation, speech disfluencies, multimodal communication, social/ cognitive neuroscience, natural language and speech interfaces to computers, spoken dialogue systems, and repair in human and human-computer dialogue. She has used eye-tracking both as a method for studying the incremental comprehension and production of spontaneous speech and as a channel in computer-mediated communication. A currently funded project is \"Communication in the Global University: A Longitudinal Study of Language Adaptation at Multiple Timescales in Nativeand Non-Native Speakers.\" She is temporarily on leave from Stony Brook University in order to serve as Program Director for NSF's oldest program, the Graduate Research Fellowship Program in the Division of Graduate", 
    "year": 2016, 
    "venue": "SIGDIAL Conference", 
    "references": 0, 
    "authors": [
      "S. Brennan"
    ], 
    "topics": [
      "Experiment", 
      "Eye tracking", 
      "Natural language", 
      "Computer-mediated communication", 
      "BrookGPU", 
      "Cognitive science", 
      "Text corpus", 
      "Atari", 
      "Dialog system", 
      "Computer-generated holography", 
      "Mental state", 
      "Spontaneous order", 
      "Semantic prosody", 
      "Computer science", 
      "Multimodal interaction", 
      "Brainwave entrainment", 
      "Susan Schneider (philosopher)", 
      "IBM Notes", 
      "List comprehension"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1143", 
    "title": "Improving Users\u2019 Demographic Prediction via the Videos They Talk about", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we improve microblog users\u2019 demographic prediction by fully utilizing their video related behaviors. First, we collect the describing words of currently popular videos, including video names, actor names and video keywords, from video websites. Secondly, we search these describing words in users\u2019 microblogs, and build the direct relationships between users and the appeared words. After that, to make the sparse relationship denser, we propose a Bayesian method to calculate the probability of connections between users and other video describing words. Lastly, we build two models to predict users\u2019 demographics with the obtained direct and indirect relationships. Based on a large realworld dataset, experiment results show that our method can significantly improve these words\u2019 demographic predictive ability.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 45, 
    "authors": [
      "Y. Wang", 
      "Yangzhe Xiao", 
      "Chao Ma", 
      "Zhen Xiao"
    ], 
    "topics": [
      "Scalability", 
      "Streaming media", 
      "Sparse matrix"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.529", 
    "title": "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Yong Cheng", 
      "Lu Jiang", 
      "Wolfgang Macherey", 
      "Jacob Eisenstein"
    ], 
    "topics": [
      "Neural machine translation", 
      "Interpolation", 
      "Transformer", 
      "Robustness (computer science)", 
      "Algorithm", 
      "BLEU", 
      "Convolutional neural network", 
      "Test set", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.sigmorphon-1.4", 
    "title": "One-Size-Fits-All Multilingual Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents DeepSPIN\u2019s submissions to Tasks 0 and 1 of the SIGMORPHON 2020 Shared Task. For both tasks, we present multilingual models, training jointly on data in all languages. We perform no language-specific hyperparameter tuning \u2013 each of our submissions uses the same model for all languages. Our basic architecture is the sparse sequence-to-sequence model with entmax attention and loss, which allows our models to learn sparse, local alignments while still being trainable with gradient-based techniques. For Task 1, we achieve strong performance with both RNN- and transformer-based sparse models. For Task 0, we extend our RNN-based model to a multi-encoder set-up in which separate modules encode the lemma and inflection sequences. Despite our models\u2019 lack of language-specific tuning, they tie for first in Task 0 and place third in Task 1.", 
    "year": 2020, 
    "venue": "SIGMORPHON", 
    "references": 41, 
    "authors": [
      "Ben Peters", 
      "Andr\u00e9 F. T. Martins"
    ], 
    "topics": [
      "FITS"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-3030", 
    "title": "ParaQG: A System for Generating Questions and Answers from Paragraphs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Generating syntactically and semantically valid and relevant questions from paragraphs is useful with many applications. Manual generation is a labour-intensive task, as it requires the reading, parsing and understanding of long passages of text. A number of question generation models based on sequence-to-sequence techniques have recently been proposed. Most of them generate questions from sentences only, and none of them is publicly available as an easy-to-use service. In this paper, we demonstrate ParaQG, a Web-based system for generating questions from sentences and paragraphs. ParaQG incorporates a number of novel functionalities to make the question generation process user-friendly. It provides an interactive interface for a user to select answers with visual insights on generation of questions. It also employs various faceted views to group similar questions as well as filtering techniques to eliminate unanswerable questions.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 21, 
    "authors": [
      "Vishwajeet Kumar", 
      "Sivaanandh Muneeswaran", 
      "Ganesh Ramakrishnan", 
      "Yuan-Fang Li"
    ], 
    "topics": [
      "Parsing", 
      "User interface", 
      "Google Questions and Answers", 
      "Usability", 
      "Faceted classification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-7701", 
    "title": "SyntaxFest 2019 Invited talk - Inductive biases and language emergence in communicative agents", 
    "fields_of_study": [
      "Psychology"
    ], 
    "abstract": "Despite spectacular progress in language modeling tasks, neural networks still fall short of the performance of human infants when it comes to learning a language from scarce and noisy data. Such performance presumably stems from human-specific inductive biases in the neural networks sustaining language acquisitions in the child. Here, we use two paradigms to study experimentally such inductive biases in artificial neural networks. The first one relies on iterative learning, where a sequence of agents learn from each other, simulating historical linguistic transmission. We find evidence that sequence to sequence neural models have some of the human inductive biases (like the preference for local dependencies), but lack others (like the preference for nonredundant markers of argument structure). The second paradigm relies on language emergence, where two agents engage in a communicative game. Here we find that sequence to sequence networks lack the preference for efficient communication found in humans, and in fact display an anti-Zipfian law of abbreviation. We conclude that the study of the inductive biases of neural networks is an important topic to improve the data efficiency of current systems.", 
    "year": 2019, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Emmanuel Dupoux"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1062", 
    "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems at our company.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 46, 
    "authors": [
      "J. Williams", 
      "Kavosh Asadi", 
      "G. Zweig"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Supervised learning", 
      "Recurrent neural network", 
      "Dialog system", 
      "Data-intensive computing", 
      "End-to-end encryption", 
      "End-to-end principle", 
      "Artificial neural network", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4007", 
    "title": "Expanding the Language model in a low-resource hybrid MT system", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The present article investigates the fusion of different language models to improve translation accuracy. A hybrid MT system, recentlydeveloped in the European Commissionfunded PRESEMT project that combines example-based MT and Statistical MT principles is used as a starting point. In this article, the syntactically-defined phrasal language models (NPs, VPs etc.) used by this MT system are supplemented by n-gram language models to improve translation accuracy. For specific structural patterns, n-gram statistics are consulted to determine whether the pattern instantiations are corroborated. Experiments indicate improvements in translation accuracy.", 
    "year": 2014, 
    "venue": "SSST@EMNLP", 
    "references": 28, 
    "authors": [
      "G. Tambouratzis", 
      "Sokratis Sofianopoulos", 
      "Marina Vassiliou"
    ], 
    "topics": [
      "Language model", 
      "N-gram", 
      "Structural pattern", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/W17-0107", 
    "title": "Work With What You\u2019ve Got", 
    "fields_of_study": [
      "Art"
    ], 
    "abstract": "It is a race against time when revitalizing an endangered language isolate. We learned an important lesson over the years: we learned to work with what we\u2019ve got. We do not have time to reinvent the wheel or be indecisive in our language revitalization. With our fluent elders passing away at such a fast rate and new learners scrambling to learn Xaad Kil, we have to be efficient. We have to share resources between the dialects and value and learn from the precious resources we have each created. By sharing across the dialects, we have been able to quickly create new resources like a digital Haida phrasebook. The Haida language is dangerously close to extinction. With only a couple handfuls of fluent speakers from Haida Gwaii, British Columbia and Southeast Alaska, it is a race against time to pass Xaad Kil, the Haida language isolate to the next generation. As with many endangered languages, it is a struggle to create resources, interest and fluency on limited budgets. The Haida language is no exception. Sharing resources is such an important part of indigenous language revitalization. Aboriginal languages are not like English, French, Spanish or German. There are not a ton of resources or a ton of speakers in the world. Grassroots language revitalization was happening in southeast Alaska and on Haida Gwaii. There have been strong and very different efforts in all communities. For instance, in Skidegate, the Skidegate Haida Immersion Program focused on recording elders. In Massett, we developed a strong cohort of immediate language learners through accredited courses. In Alaska there were courses and publications happening. The International Haida Language conferences became a way to share resources, plan and work together on new projects. This became a valuable part of Xaad Kil revitalization. There was some sporadic communication between the communities but it wasn\u2019t until the International Haida language conferences that the sharing of resources and working towards a common orthography began. Once we began sharing dictionaries, glossaries, recordings and lessons our library of resources blossomed. One of our successful cross-dialect initiatives was an adult accredited class offered to a cohort of 30+ students in Old Massett and in Ketchikan Alaska. The students in Alaska gathered with linguist Dr. Jordan Lachler. The Old Massett students gathered with elder Claude Jones. We worked from the Dii Tawlang lessons done by Dr. Lachler in the Alaskan dialect and orthography. We used the Haida health centre\u2019s video technology to connect to the Alaskans three evenings per week. The system was great when the technology worked. Having an elder in Massett present helped us tweak the lesson plans and all of the students were able to learn the subtleties between the two dialects. Not only did the students\u2019 language improve greatly, but our understanding of the dialect similarities and differences improved as well. This course was a good lesson in the importance of crosscommunication between dialects.", 
    "year": 2017, 
    "venue": "", 
    "references": 1, 
    "authors": [
      "L. Bell", 
      "Lawrence H. Bell"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2010.36.2.09054", 
    "title": "Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. We present three modifications to the MT training data to improve the accuracy of a state-of-the-art syntax MT system: re-structuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and re-aligning unifies word alignment across sentences to remove bad word alignments and refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm. We show that each individual technique leads to improvement as measured by BLEU, and we also show that the greatest improvement is achieved by combining them. We report an overall 1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English translation.", 
    "year": 2010, 
    "venue": "Computational Linguistics", 
    "references": 49, 
    "authors": [
      "W. Wang", 
      "Jonathan May", 
      "Kevin Knight", 
      "D. Marcu"
    ], 
    "topics": [
      "Statistical machine translation", 
      "BLEU", 
      "Baseline (configuration management)", 
      "Parsing", 
      "Expectation\u2013maximization algorithm", 
      "Parse tree", 
      "Context (computing)", 
      "Bitext word alignment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3709", 
    "title": "Ling@CASS Solution to the NLP-TEA CGED Shared Task 2018", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this study, we employ the sequence to sequence learning to model the task of grammar error correction. The system takes potentially erroneous sentences as inputs, and outputs correct sentences. To breakthrough the bottlenecks of very limited size of manually labeled data, we adopt a semi-supervised approach. Specifically, we adapt correct sentences written by native Chinese speakers to generate pseudo grammatical errors made by learners of Chinese as a second language. We use the pseudo data to pre-train the model, and the CGED data to fine-tune it. Being aware of the significance of precision in a grammar error correction system in real scenarios, we use ensembles to boost precision. When using inputs as simple as Chinese characters, the ensembled system achieves a precision at 86.56% in the detection of erroneous sentences, and a precision at 51.53% in the correction of errors of Selection and Missing types.", 
    "year": 2018, 
    "venue": "NLP-TEA@ACL", 
    "references": 30, 
    "authors": [
      "Q. Hu", 
      "Yongwei Zhang", 
      "F. Liu", 
      "Yueguo Gu"
    ], 
    "topics": [
      "Error detection and correction", 
      "Precision and recall", 
      "Tea", 
      "Semi-supervised learning", 
      "Computer multitasking", 
      "Natural language processing", 
      "Semiconductor industry"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-4020", 
    "title": "Marian: Fast Neural Machine Translation in C++", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 18, 
    "authors": [
      "Marcin Junczys-Dowmunt", 
      "Roman Grundkiewicz", 
      "Tomasz Dwojak", 
      "Hieu T. Hoang", 
      "Kenneth Heafield", 
      "Tom Neckermann", 
      "F. Seide", 
      "Ulrich Germann", 
      "A. F. Aji", 
      "Nikolay Bogoychev", 
      "Andr\u00e9 F. T. Martins", 
      "Alexandra Birch"
    ], 
    "topics": [
      "C++", 
      "Neural machine translation", 
      "Automatic differentiation", 
      "Computation", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-TUTORIALS.2", 
    "title": "Fine-grained Interpretation and Causation Analysis in Deep NLP Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern. In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) fine-grained interpretation, and ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 65, 
    "authors": [
      "Hassan Sajjad", 
      "Narine Kokhlikyan", 
      "Fahim Dalvi", 
      "Nadir Durrani"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.sigmorphon-1.12", 
    "title": "Low-Resource G2P and P2G Conversion with Synthetic Training Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the University of Alberta systems and results in the SIGMORPHON 2020 Task 1: Multilingual Grapheme-to-Phoneme Conversion. Following previous SIGMORPHON shared tasks, we define a low-resource setting with 100 training instances. We experiment with three transduction approaches in both standard and low-resource settings, as well as on the related task of phoneme-to-grapheme conversion. We propose a method for synthesizing training data using a combination of diverse models.", 
    "year": 2020, 
    "venue": "SIGMORPHON", 
    "references": 22, 
    "authors": [
      "B. Hauer", 
      "Amir Ahmad Habibi", 
      "Yixing Luan", 
      "Arnob Mallik", 
      "Grzegorz Kondrak"
    ], 
    "topics": [
      "Synthetic intelligence"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.293", 
    "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our labelfree training results in a style vector space encoding many facets of style, we recast transfers as \u201ctargeted restyling\u201d vector operations that adjust specific attributes of the input while preserving others. We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 39, 
    "authors": [
      "Parker Riley", 
      "Noah Constant", 
      "Mandy Guo", 
      "Girish Kumar", 
      "David C. Uthus", 
      "Zarana Parekh"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4645", 
    "title": "A statistical approach for Non-Sentential Utterance Resolution for Interactive QA System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Non-Sentential Utterances (NSUs) are short utterances that do not have the form of a full sentence but nevertheless convey a complete sentential meaning in the context of a conversation. NSUs are frequently used to ask follow up questions during interactions with question answer (QA) systems resulting into in-correct answers being presented to their users. Most of the current methods for resolving such NSUs have adopted rule or grammar based approach and have limited applicability. In this paper, we present a data driven statistical method for resolving such NSUs. Our method is based on the observation that humans identify keyword appearing in an NSU and place them in the context of conversation to construct a meaningful sentence. We adapt the keyword to question (K2Q) framework to generate natural language questions using keywords appearing in an NSU and its context. The resulting questions are ranked using different scoring methods in a statistical framework. Our evaluation on a data-set collected using mTurk shows that the proposed method perform significantly better than the previous work that has largely been rule based.", 
    "year": 2015, 
    "venue": "SIGDIAL Conference", 
    "references": 15, 
    "authors": [
      "Dinesh Raghu", 
      "Sathish Indurthi", 
      "J. Ajmera", 
      "Sachindra Joshi"
    ], 
    "topics": [
      "Question answering", 
      "Learning to rank", 
      "Amazon Mechanical Turk", 
      "Natural language generation", 
      "Computer performance", 
      "Interaction", 
      "Statistical model"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.1007/11562214_17", 
    "title": "PP-Attachment Disambiguation Boosted by a Gigantic Volume of Unambiguous Examples", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a PP-attachment disambiguation method based on a gigantic volume of unambiguous examples extracted from raw corpus. The unambiguous examples are utilized to acquire precise lexical preferences for PP-attachment disambiguation. Attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences. Our experiments indicate that the precise lexical preferences work effectively.", 
    "year": 2005, 
    "venue": "IJCNLP", 
    "references": 27, 
    "authors": [
      "Daisuke Kawahara", 
      "S. Kurohashi"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Attachments", 
      "Machine learning", 
      "Experiment", 
      "Text corpus", 
      "PP (complexity)", 
      "Quadruple-precision floating-point format"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1141", 
    "title": "HCCL at SemEval-2018 Task 8: An End-to-End System for Sequence Labeling from Cybersecurity Reports", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes HCCL team systems that participated in SemEval 2018 Task 8: SecureNLP (Semantic Extraction from cybersecurity reports using NLP). To solve the problem, our team applied a neural network architecture that benefits from both word and character level representaions automatically, by using combination of Bi-directional LSTM, CNN and CRF (Ma and Hovy, 2016). Our system is truly end-to-end, requiring no feature engineering or data preprocessing, and we ranked 4th in the subtask 1, 7th in the subtask2 and 3rd in the SubTask2-relaxed.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 18, 
    "authors": [
      "Mingming Fu", 
      "Xuemin Zhao", 
      "Yonghong Yan"
    ], 
    "topics": [
      "SemEval", 
      "Sequence labeling", 
      "Computer security", 
      "Feature engineering", 
      "End system", 
      "Network architecture", 
      "Data pre-processing", 
      "Preprocessor", 
      "Artificial neural network", 
      "End-to-end principle", 
      "Long short-term memory", 
      "Conditional random field", 
      "Natural language processing", 
      "Cyber security standards"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2083", 
    "title": "Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a method for improving word alignments using word similarities. This method is based on encouraging common alignment links between semantically similar words. We use word vectors trained on monolingual data to estimate similarity. Our experiments on translating fifteen languages into English show consistent BLEU score improvements across the languages.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 26, 
    "authors": [
      "Nima Pourdamghani", 
      "Marjan Ghazvininejad", 
      "Kevin Knight"
    ], 
    "topics": [
      "Word embedding", 
      "Machine translation", 
      "BLEU", 
      "Experiment", 
      "Microsoft Word for Mac", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.321", 
    "title": "A Simple and Effective Unified Encoder for Document-Level Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Shuming Ma", 
      "Dongdong Zhang", 
      "M. Zhou"
    ], 
    "topics": [
      "Machine translation", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.80", 
    "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation", 
    "fields_of_study": [
      "Computer Science", 
      "Engineering"
    ], 
    "abstract": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours. We provide speech recognition baselines and validate the versatility of VoxPopuli unlabelled data in semi-supervised learning under challenging out-of-domain settings. We will release the corpus at this https URL under an open license.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 53, 
    "authors": [
      "Changhan Wang", 
      "M. Rivi\u00e8re", 
      "Ann Lee", 
      "Anne Wu", 
      "C. Talnikar", 
      "Daniel Haziza", 
      "Mary Williamson", 
      "J. Pino", 
      "Emmanuel Dupoux"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-1011", 
    "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 35, 
    "authors": [
      "Rie Johnson", 
      "Tong Zhang"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Categorization", 
      "Convolution", 
      "Artificial neural network", 
      "Document classification", 
      "Text corpus", 
      "Performance", 
      "Word embedding", 
      "Experiment", 
      "Neural Networks", 
      "N-gram", 
      "IBM Notes"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.repl4nlp-1.16", 
    "title": "Are All Languages Created Equal in Multilingual BERT?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.", 
    "year": 2020, 
    "venue": "REPL4NLP", 
    "references": 37, 
    "authors": [
      "Shijie Wu", 
      "Mark Dredze"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.258", 
    "title": "FENAS: Flexible and Expressive Neural Architecture Search", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using a weight sharing-based approach called Efficient Neural Architecture Search (ENAS). In this work, we propose a novel architecture search algorithm called Flexible and Expressible Neural Architecture Search (FENAS), with more flexible and expressible search space than ENAS, in terms of more activation functions, input edges, and atomic operations. Also, our FENAS approach is able to reproduce the well-known LSTM and GRU architectures (unlike ENAS), and is also able to initialize with them for finding architectures more efficiently. We explore this extended search space via evolutionary search and show that FENAS performs significantly better on several popular text classification tasks and performs similar to ENAS on standard language model benchmark. Further, we present ablations and analyses on our FENAS approach.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 50, 
    "authors": [
      "Ramakanth Pasunuru", 
      "M. Bansal"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075096.1075134", 
    "title": "Self-Organizing Markov Models and Their Application to Part-of-Speech Tagging", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory) Markov models. The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm. A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "Jin-Dong Kim", 
      "Hae-Chang Rim", 
      "Junichi Tsujii"
    ], 
    "topics": [
      "Markov model", 
      "Markov chain", 
      "Part-of-speech tagging", 
      "Decision tree learning", 
      "Algorithm", 
      "Experiment", 
      "Organizing (structure)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.385", 
    "title": "Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method\u2014\u2014LayerDrop.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 11, 
    "authors": [
      "Q. Wang", 
      "Tong Xiao", 
      "Jingbo Zhu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118935.1118943", 
    "title": "Learning Bilingual Translations from Comparable Corpora to Cross-Language Information Retrieval: Hybrid Statistics-based and Linguistics-based Approach", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent years saw an increased interest in the use and the construction of large corpora. With this increased interest and awareness has come an expansion in the application to knowledge acquisition and bilingual terminology extraction. The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, combination to linguistics-based pruning and evaluations on Cross-Language Information Retrieval. We propose and explore a two-stages translation model for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives on the basis of their morphological knowledge. Evaluations using a large-scale test collection on Japanese-English and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on Cross-Language Information Retrieval.", 
    "year": 2003, 
    "venue": "", 
    "references": 21, 
    "authors": [
      "F. Sadat", 
      "M. Yoshikawa", 
      "S. Uemura"
    ], 
    "topics": [
      "Text corpus", 
      "Cross-language information retrieval", 
      "Terminology extraction", 
      "Bilingual dictionary", 
      "Knowledge acquisition", 
      "Word-sense disambiguation", 
      "Lexicon", 
      "Thesaurus", 
      "Vocabulary", 
      "Goto", 
      "WWW", 
      "Machine translation", 
      "SMART", 
      "Data dictionary", 
      "Web page", 
      "General-purpose markup language", 
      "Parallel text"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980845.980902", 
    "title": "A Text Input Front-end Processor as an Information Access Platform", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a practical foreign language writing support tool which makes it much easier to utilize dictionary and example sentence resources. Like a Kana-Kanji conversion front-end processor used to input Japanese language text, this tool is also implemented as a front-end processor and can be combined with a wide variety of applications. A morphological analyzer automatically extracts key words from text as it is being input into the tool, and these words are used to locate information relevant to the input text. This information is then automatically displayed to the user. With this tool, users can concentrate better on their writing because much less interruption of their work is required for the consulting of dictionaries or for the retrieval of reference sentences. Retrieval and display may be conducted in any three ways: 1) relevant information is retrieved and displayed automatically; 2) information is retrieved automatically but displayed only on user command; 3) information is both retrieved and displayed only on user command. The extent to which the retrieval and display of information proceeds automatically depends on the type of information being referenced; this element of the design adds to system efficiency. Further, by combining this tool with a stepped-level interactive machine translation function, we have created a PC support tool to help Japanese people write in English.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 5, 
    "authors": [
      "Shinichi Doi", 
      "S. Kamei", 
      "Kiyoshi Yamabana"
    ], 
    "topics": [
      "Interactive machine translation", 
      "Dictionary", 
      "Front-end processor", 
      "Information access", 
      "Bilingual dictionary", 
      "Interrupt", 
      "Alloy Analyzer", 
      "Japanese input methods"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.622", 
    "title": "Fortifying Toxic Speech Detectors against Disguised Toxicity", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 29, 
    "authors": [
      "Xiaochuang Han", 
      "Yulia Tsvetkov"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3014", 
    "title": "Illegal is not a Noun: Linguistic Form for Detection of Pejorative Nominalizations", 
    "fields_of_study": [
      "Computer Science", 
      "Psychology"
    ], 
    "abstract": "This paper focuses on a particular type of abusive language, targeting expressions in which typically neutral adjectives take on pejorative meaning when used as nouns - compare \u2018gay people\u2019 to \u2018the gays\u2019. We first collect and analyze a corpus of hand-curated, expert-annotated pejorative nominalizations for four target adjectives: female, gay, illegal, and poor. We then collect a second corpus of automatically-extracted and POS-tagged, crowd-annotated tweets. For both corpora, we find support for the hypothesis that some adjectives, when nominalized, take on negative meaning. The targeted constructions are non-standard yet widely-used, and part-of-speech taggers mistag some nominal forms as adjectives. We implement a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via nominalization into abusive language.", 
    "year": 2017, 
    "venue": "ALW@ACL", 
    "references": 15, 
    "authors": [
      "Alexis Palmer", 
      "Melissa Robinson", 
      "Kristy K. Phillips"
    ], 
    "topics": [
      "Linguistics", 
      "Text corpus", 
      "Sentiment analysis", 
      "Body of uterus", 
      "Sensor", 
      "Mock object", 
      "Tracer", 
      "Part-of-speech tagging", 
      "anatomical layer", 
      "Extraction", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.519", 
    "title": "A Unified MRC Framework for Named Entity Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question \u201cwhich person is mentioned in the text\".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation. We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 57, 
    "authors": [
      "Xiaoya Li", 
      "Jingrong Feng", 
      "Yuxian Meng", 
      "Qinghong Han", 
      "Fei Wu", 
      "Jiwei Li"
    ], 
    "topics": [
      "Sequence labeling", 
      "Named entity", 
      "Shroud of the Avatar:", 
      "Unified Framework", 
      "Natural language understanding", 
      "Performance", 
      "Information", 
      "Experiment", 
      "Internet backbone"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00147", 
    "title": "Latent Structures for Coreference Resolution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Machine learning approaches to coreference resolution vary greatly in the modeling of the problem: while early approaches operated on the mention pair level, current research focuses on ranking architectures and antecedent trees. We propose a unified representation of different approaches to coreference resolution in terms of the structure they operate on. We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities.", 
    "year": 2015, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 45, 
    "authors": [
      "Sebastian Martschat", 
      "M. Strube"
    ], 
    "topics": [
      "Machine learning", 
      "Anaphora (linguistics)", 
      "Latent variable model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1159", 
    "title": "Modeling Biological Processes for Reading Comprehension", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora. In this paper, we focus on a new reading comprehension task that requires complex reasoning over a single document. The input is a paragraph describing a biological process, and the goal is to answer questions that require an understanding of the relations between entities and events in the process. To answer the questions, we first predict a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 42, 
    "authors": [
      "Jonathan Berant", 
      "Vivek Srikumar", 
      "P. Chen", 
      "A. V. Linden", 
      "Brittany Harding", 
      "Brad Huang", 
      "Peter Clark", 
      "Christopher D. Manning"
    ], 
    "topics": [
      "List comprehension", 
      "Baseline (configuration management)", 
      "Entity", 
      "Text corpus", 
      "Natural language understanding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-1003", 
    "title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most recent unsupervised methods in vector space semantics for assessing thematic fit (e.g. Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014) create prototypical rolefillers without performing word sense disambiguation. This leads to a kind of sparsity problem: candidate role-fillers for different senses of the verb end up being measured by the same \u201cyardstick\u201d, the single prototypical role-filler. In this work, we use three different feature spaces to construct robust unsupervised models of distributional semantics. We show that correlation with human judgements on thematic fit estimates can be improved consistently by clustering typical role-fillers and then calculating similarities of candidate rolefillers with these cluster centroids. The suggested methods can be used in any vector space model that constructs a prototype vector from a non-trivial set of typical vectors.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 32, 
    "authors": [
      "C. Greenberg", 
      "Asad B. Sayeed", 
      "V. Demberg"
    ], 
    "topics": [
      "Prototype", 
      "Cluster analysis", 
      "Star filler", 
      "Unsupervised learning", 
      "Distributional semantics", 
      "Word-sense disambiguation", 
      "Word sense", 
      "Sparse matrix"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N16-1036", 
    "title": "Recurrent Memory Networks for Language Modeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.", 
    "year": 2016, 
    "venue": "NAACL", 
    "references": 43, 
    "authors": [
      "Ke M. Tran", 
      "Arianna Bisazza", 
      "Christof Monz"
    ], 
    "topics": [
      "Language model", 
      "Natural language processing", 
      "Long short-term memory", 
      "Recurrent neural network", 
      "Perplexity", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6403", 
    "title": "Findings of the WMT 2018 Biomedical Translation Shared Task: Evaluation on Medline test sets", 
    "fields_of_study": [
      "Computer Science", 
      "Psychology"
    ], 
    "abstract": "Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish). We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year\u2019s BLEU scores were somewhat higher that last year\u2019s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 25, 
    "authors": [
      "M. Neves", 
      "Antonio Jimeno-Yepes", 
      "Aur\u00e9lie N\u00e9v\u00e9ol", 
      "C. Grozea", 
      "A. Siu", 
      "Madeleine Kittner", 
      "Karin M. Verspoor"
    ], 
    "topics": [
      "MEDLINE", 
      "Machine translation", 
      "Test set", 
      "Parallel text", 
      "BLEU", 
      "Scientific literature", 
      "Text corpus", 
      "Transformer", 
      "SciELO", 
      "Freedom of information laws by country", 
      "Algorithm", 
      "Electronic data processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4603", 
    "title": "POS Tagging for Improving Code-Switching Identification in Arabic", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Arabic (MSA) and Egyptian Arabic (EA). We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9% on the development set and 1.0% on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language.", 
    "year": 2019, 
    "venue": "WANLP@ACL 2019", 
    "references": 63, 
    "authors": [
      "Mohammed A. Attia", 
      "Younes Samih", 
      "Ali El-Kahky", 
      "Hamdy Mubarak", 
      "Ahmed Abdelali", 
      "Kareem Darwish"
    ], 
    "topics": [
      "Part-of-speech tagging", 
      "Deep learning", 
      "Artificial neural network", 
      "Brown Corpus", 
      "Enter the Matrix", 
      "Test set", 
      "Embedded system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.480", 
    "title": "Implicit Discourse Relation Classification: We Need to Talk about Evaluation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0. Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0. We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 34, 
    "authors": [
      "Najoung Kim", 
      "Song Feng", 
      "R. Chulaka Gunasekara", 
      "L. Lastras"
    ], 
    "topics": [
      "Discourse relation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-0404", 
    "title": "Some Issues on the Normalization of a Corpus of Products Reviews in Portuguese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the analysis of different kinds of noises in a corpus of products reviews in Brazilian Portuguese. Case folding, punctuation, spelling and the use of internet slang are the major kinds of noise we face. After noting the effect of these noises on the POS tagging task, we propose some procedures to minimize them.", 
    "year": 2014, 
    "venue": "WaC@EACL", 
    "references": 62, 
    "authors": [
      "Magali Sanches Duran", 
      "L. Avanco", 
      "Sandra M. Alu\u00edsio", 
      "T. Pardo", 
      "M. G. V. Nunes"
    ], 
    "topics": [
      "Text corpus", 
      "Internet slang", 
      "Part-of-speech tagging"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2008.34.1.129", 
    "title": "Incremental Conceptualization for Language Production Markus Guhe (University of Edinburgh) Mahwah, NJ: Lawrence Erlbaum Associates (distributed by Psychology Press), 2007, xii+260 pp; hardbound, ISBN 978-0-8058-5624-8, $75.00", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A process for producing high-quality alpha -olefin polymers with a higher yield in a long lasting stabilized manner even in the case of gas phase polymerization is provided. In the production process of alpha -olefin polymers which comprises reacting a trivalent metal halide with a divalent metal compound to obtain a solid product (I); reacting this product with at least one electron donor (ED) and at least one electron acceptor (EA), once to 10 times, and at that time, using TiCl4 at least once as the (EA) to obtain a solid product (II); combining this product with an organoaluminum compound (OAl) and an (ED) (these three being referred to as catalyst components), the improvement which comprises subjecting a part or the whole of the catalyst components to polymerization treatment with a small amount of an alpha -olefin, at least in the coexistence of the solid product (II) and (OAl), in the combination of the catalyst components to obtain a preactivated catalyst, and subjecting alpha -olefin(s) to gas phase polymerization or bulk or slurry polymerization followed by gas phase polymerization, using the catalyst.", 
    "year": 2008, 
    "venue": "Computational Linguistics", 
    "references": 7, 
    "authors": [
      "P. Piwek"
    ], 
    "topics": [
      "Conceptualization (information science)", 
      "Natural language generation", 
      "International Standard Book Number", 
      "Glossary", 
      "Theory of computation", 
      "Energy (psychological)", 
      "Situated", 
      "Online and offline", 
      "Model checking"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4209", 
    "title": "Linguistic Linked Data in Chinese: The Case of Chinese Wordnet", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The present study describes recent developments of Chinese Wordnet, which has been reformatted using the lemon model and published as part of the Linguistic Linked Open Data Cloud. While lemon suffices for modeling most of the structures in Chinese Wordnet at the lexical level, the model does not allow for finergrained distinction of a word sense, or meaning facets, a linguistic feature also attended to in Chinese Wordnet. As for the representation of synsets, we use the WordNet RDF ontology for integration\u2019s sake. Also, we use another ontology proposed by the Global WordNet Association to show how Chinese Wordnet as Linked Data can be integrated into the Global WordNet Grid.", 
    "year": 2015, 
    "venue": "LDL@IJCNLP", 
    "references": 25, 
    "authors": [
      "Chih-yao Lee", 
      "S. Hsieh"
    ], 
    "topics": [
      "WordNet", 
      "Linked data", 
      "Linguistics", 
      "Linguistic Linked Open Data", 
      "Synonym ring", 
      "Word sense", 
      "Vocabulary", 
      "Resource Description Framework", 
      "Tag cloud", 
      "Scientific Publication", 
      "Ontology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.532", 
    "title": "Tagged Back-translation Revisited: Why Does It Really Work?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 21, 
    "authors": [
      "Benjamin Marie", 
      "Rapha\u00ebl Rubino", 
      "Atsushi Fujita"
    ], 
    "topics": [
      "Overfitting", 
      "Neural machine translation", 
      "Elegant degradation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1208", 
    "title": "Kurdish Interdialect Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This research suggests a method for machine translation among two Kurdish dialects. We chose the two widely spoken dialects, Kurmanji and Sorani, which are considered to be mutually unintelligible. Also, despite being spoken by about 30 million people in different countries, Kurdish is among less-resourced languages. The research used bi-dialectal dictionaries and showed that the lack of parallel corpora is not a major obstacle in machine translation between the two dialects. The experiments showed that the machine translated texts are comprehensible to those who do not speak the dialect. The research is the first attempt for inter-dialect machine translation in Kurdish and particularly could help in making online texts in one dialect comprehensible to those who only speak the target dialect. The results showed that the translated texts are in 71% and 79% cases rated as understandable for Kurmanji and Sorani respectively. They are rated as slightly-understandable in 29% cases for Kurmanji and 21% for Sorani.", 
    "year": 2017, 
    "venue": "VarDial", 
    "references": 53, 
    "authors": [
      "Hossein Hassani"
    ], 
    "topics": [
      "Machine translation", 
      "Parallel text", 
      "Dictionary", 
      "Experiment", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.256", 
    "title": "Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters\u2019 written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios. Experiments across different applications show the significant effectiveness of our model.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Hong-You Chen", 
      "Sz-Han Yu", 
      "Shou-de Lin"
    ], 
    "topics": [
      "Glyph", 
      "Vocabulary", 
      "Word embedding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1318", 
    "title": "Relational Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 49, 
    "authors": [
      "Jos\u00e9 Camacho-Collados", 
      "Luis Espinosa Anke", 
      "S. Schockaert"
    ], 
    "topics": [
      "Word embedding", 
      "Complementarity theory", 
      "Unsupervised learning", 
      "Microsoft Word for Mac", 
      "Document classification", 
      "Natural language understanding", 
      "Knowledge base", 
      "ENCODE", 
      "Natural language processing", 
      "Information repository"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1639", 
    "title": "Constraint-based Learning of Phonological Processes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Phonological processes are context-dependent sound changes in natural languages. We present an unsupervised approach to learning human-readable descriptions of phonological processes from collections of related utterances. Our approach builds upon a technique from the programming languages community called *constraint-based program synthesis*. We contribute a novel encoding of the learning problem into Boolean Satisfiability constraints, which enables both data efficiency and fast inference. We evaluate our system on textbook phonology problems and datasets from the literature, and show that it achieves high accuracy at interactive speeds.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 39, 
    "authors": [
      "Shraddha Barke", 
      "Rose Kunkel", 
      "N. Polikarpova", 
      "Eric Meinhardt", 
      "Eric Bakovic", 
      "Leon Bergen"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Program synthesis", 
      "Natural language", 
      "Human-readable medium", 
      "Solver", 
      "Programming language", 
      "Context-sensitive language", 
      "Database"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1044", 
    "title": "Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Shuhei Kurita", 
      "Daisuke Kawahara", 
      "S. Kurohashi"
    ], 
    "topics": [
      "Text corpus", 
      "Entity\u2013relationship model", 
      "Task analysis", 
      "Artificial neural network", 
      "Semiconductor industry"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1429", 
    "title": "Towards a Better Metric for Evaluating Question Generation Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "There has always been criticism for using n-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on n-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 44, 
    "authors": [
      "Preksha Nema", 
      "Mitesh M. Khapra"
    ], 
    "topics": [
      "Knowledge base", 
      "Knowledge Graph", 
      "N-gram", 
      "Evaluation of machine translation", 
      "BLEU", 
      "Nonlinear system", 
      "Entity", 
      "NEMA (machine)", 
      "Evaluation function", 
      "Viz: The Computer Game", 
      "Natural language generation", 
      "Question answering", 
      "Software quality assurance", 
      "Scoring functions for docking"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2075", 
    "title": "ELiRF-UPV at SemEval-2017 Task 7: Pun Detection and Interpretation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the participation of ELiRF-UPV team at task 7 (subtask 2: homographic pun detection and subtask 3: homographic pun interpretation) of SemEval2017. Our approach is based on the use of word embeddings to find related words in a sentence and a version of the Lesk algorithm to establish relationships between synsets. The results obtained are in line with those obtained by the other participants and they encourage us to continue working on this problem.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 6, 
    "authors": [
      "L. Hurtado", 
      "E. Segarra", 
      "Ferran Pl\u00e0", 
      "Pascual Carrasco", 
      "Jos\u00e9-\u00c1ngel Gonz\u00e1lez"
    ], 
    "topics": [
      "Lesk algorithm", 
      "Synonym ring", 
      "Web Services for Devices"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1132", 
    "title": "Predicting Word Association Strengths", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper looks at the task of predicting word association strengths across three datasets; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features. Furthermore, we examine the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting word association strength, the first work to do so.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "Andrew Cattle", 
      "Xiaojuan Ma"
    ], 
    "topics": [
      "WordNet", 
      "Word embedding", 
      "Word2vec", 
      "Gaussian (software)", 
      "Thesaurus", 
      "Ambiguous grammar", 
      "Word sense", 
      "Microsoft Word for Mac"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1095", 
    "title": "SAARSHEFF at SemEval-2016 Task 1: Semantic Textual Similarity with Machine Translation Evaluation Metrics and (eXtreme) Boosted Tree Ensembles", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the SAARSHEFF systems that participated in the English Semantic Textual Similarity (STS) task in SemEval2016. We extend the work on using machine translation (MT) metrics in the STS task by automatically annotating the STS datasets with a variety of MT scores for each pair of text snippets in the STS datasets. We trained our systems using boosted tree ensembles and achieved competitive results that outperforms he median Pearson correlation scores from all participating systems.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 49, 
    "authors": [
      "Liling Tan", 
      "Carolina Scarton", 
      "Lucia Specia", 
      "Josef van Genabith"
    ], 
    "topics": [
      "Machine translation", 
      "SemEval", 
      "Baseline (configuration management)", 
      "Ensemble forecasting"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-2108", 
    "title": "Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision-Making", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.", 
    "year": 2019, 
    "venue": "", 
    "references": 56, 
    "authors": [
      "Elijah Mayfield", 
      "A. Black"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1569", 
    "title": "Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a method based on neural networks to identify the sentiment polarity of opinion words expressed on a specific aspect of a sentence. Although a large majority of works typically focus on leveraging the expressive power of neural networks in handling this task, we explore the possibility of integrating dependency trees with neural networks for representation learning. To this end, we present a convolution over a dependency tree (CDT) model which exploits a Bi-directional Long Short Term Memory (Bi-LSTM) to learn representations for features of a sentence, and further enhance the embeddings with a graph convolutional network (GCN) which operates directly on the dependency tree of the sentence. Our approach propagates both contextual and dependency information from opinion words to aspect words, offering discriminative properties for supervision. Experimental results ranks our approach as the new state-of-the-art in aspect-based sentiment classification.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 33, 
    "authors": [
      "Kai Sun", 
      "Richong Zhang", 
      "Samuel Mensah", 
      "Yongyi Mao", 
      "Xudong Liu"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Convolution"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00092", 
    "title": "Efficient Structured Inference for Transition-Based Parsing with Neural Networks and Error States", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transition-based approaches based on local classification are attractive for dependency parsing due to their simplicity and speed, despite producing results slightly below the state-of-the-art. In this paper, we propose a new approach for approximate structured inference for transition-based parsing that produces scores suitable for global scoring using local models. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Using neural networks for our local classifiers, our approach achieves 93.61% accuracy for transition-based dependency parsing in English.", 
    "year": 2016, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 38, 
    "authors": [
      "Ashish Vaswani", 
      "Kenji Sagae"
    ], 
    "topics": [
      "Parsing", 
      "Artificial neural network", 
      "Sequence labeling", 
      "Approximation algorithm", 
      "Structured prediction", 
      "Recurrent neural network", 
      "Concatenation", 
      "Integer factorization", 
      "Word embedding", 
      "Perceptron", 
      "Sampling (signal processing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-0515", 
    "title": "And That\u2019s A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of \"factual\" and \"feeling\" debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannotated forum posts. This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language. Finally, we analyze the most discriminating patterns to better understand the defining characteristics of factual and emotional arguments.", 
    "year": 2015, 
    "venue": "ArgMining@HLT-NAACL", 
    "references": 64, 
    "authors": [
      "Shereen Oraby", 
      "Lena I. Reed", 
      "Ryan Compton", 
      "E. Riloff", 
      "M. Walker", 
      "S. Whittaker"
    ], 
    "topics": [
      "Context-free grammar", 
      "Schmitt trigger", 
      "Reed\u2013Solomon error correction"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2505", 
    "title": "The Role of Expectedness in the Implicitation and Explicitation of Discourse Relations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Translation of discourse connectives varies more in human translations than in machine translations. Building on Murray\u2019s (1997) continuity hypothesis and Sanders\u2019 (2005) causality-by-default hypothesis we investigate whether expectedness influences the degree of implicitation and explicitation of discourse relations. We manually analyze how source text connectives are translated, and where connectives in target texts come from. We establish whether relations are explicitly signaled in the other language as well, or whether they have to be reconstructed by inference. We demonstrate that the amount of implicitation and explicitation of connectives in translation is influenced by the expectedness of the relation a connective signals. In addition, we show that the types of connectives most often added in translation are also the ones most often deleted.", 
    "year": 2015, 
    "venue": "DiscoMT@EMNLP", 
    "references": 25, 
    "authors": [
      "J. Hoek", 
      "J. Evers-Vermeul", 
      "T. Sanders"
    ], 
    "topics": [
      "Logical connective", 
      "Causality", 
      "Machine translation", 
      "Scott continuity", 
      "COMEFROM"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.175", 
    "title": "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce \u201ctypicality\u201d, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics. Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties. Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater systemlevel insight into captioner differences. Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 49, 
    "authors": [
      "Joshua Forster Feinglass", 
      "Yezhou Yang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00217", 
    "title": "Multiple Adjunction in Feature-Based Tree-Adjoining Grammar", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In parsing with Tree Adjoining Grammar (TAG), independent derivations have been shown by Schabes and Shieber (1994) to be essential for correctly supporting syntactic analysis, semantic interpretation, and statistical language modeling. However, the parsing algorithm they propose is not directly applicable to Feature-Based TAGs (FB-TAG). We provide a recognition algorithm for FB-TAG that supports both dependent and independent derivations. The resulting algorithm combines the benefits of independent derivations with those of Feature-Based grammars. In particular, we show that it accounts for a range of interactions between dependent vs. independent derivation on the one hand, and syntactic constraints, linear ordering, and scopal vs. nonscopal semantic dependencies on the other hand.", 
    "year": 2015, 
    "venue": "Computational Linguistics", 
    "references": 35, 
    "authors": [
      "Claire Gardent", 
      "Shashi Narayan"
    ], 
    "topics": [
      "Parsing", 
      "Algorithm", 
      "Tree-adjoining grammar", 
      "Language model", 
      "Semantic interpretation", 
      "Interaction", 
      "Identifier", 
      "Unification (computer science)", 
      "Unique key", 
      "Compiler", 
      "Tag cloud"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4102", 
    "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems.", 
    "year": 2017, 
    "venue": "SWCN@EMNLP", 
    "references": 47, 
    "authors": [
      "Anoop Kunchukuttan", 
      "P. Bhattacharyya"
    ], 
    "topics": [
      "Byte pair encoding", 
      "Orthographic projection", 
      "Statistical machine translation", 
      "Parallel text", 
      "BLEU", 
      "Domain adaptation", 
      "File spanning", 
      "Viz: The Computer Game", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118108.1118120", 
    "title": "Teaching Computational Linguistics at the University of Tartu: Experience, Perspectives and Challenges", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The paper gives a review of teaching Computational Linguistics (CL) at the University of Tartu. The current curriculum foresees the possibility of studying CL as an independent 4-year subject in the Faculty of Philosophy on the bachelor stage. In connection with the higher education reform in Estonia, new curricula will be introduced from the next study year where the 3-year bachelor stage will be followed by a 2-year master's stage. It will then be possible to study CL proceeding from two paths: in the Faculty of Philosophy, and additionally also in the Faculty of Mathematics and Computer Science. This way two types of specialists will be trained who will hopefully be able to complement each other in team-work.", 
    "year": 2002, 
    "venue": "ACL 2002", 
    "references": 2, 
    "authors": [
      "M. Koit", 
      "Tiit Roosmaa", 
      "H. \u00d5im"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation", 
      "Real life", 
      "Language technology", 
      "Software industry", 
      "Labor (Childbirth)", 
      "Educational Curriculum", 
      "Universities", 
      "Preparation", 
      "computer science", 
      "Complement System Proteins"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-2010", 
    "title": "Evaluation of Morphological Embeddings for English and Russian Languages", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "This paper evaluates morphology-based embeddings for English and Russian languages. Despite the interest and introduction of several morphology based word embedding models in the past and acclaimed performance improvements on word similarity and language modeling tasks, in our experiments, we did not observe any stable preference over two of our baseline models - SkipGram and FastText. The performance exhibited by morphological embeddings is the average of the two baselines mentioned above.", 
    "year": 2021, 
    "venue": "ArXiv", 
    "references": 21, 
    "authors": [
      "V. Romanov", 
      "A. Khusainova"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-2315", 
    "title": "Biomedical Event Extraction using Abstract Meaning Representation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset1 (Kim et al., 2013) and show promising results.", 
    "year": 2017, 
    "venue": "BioNLP", 
    "references": 40, 
    "authors": [
      "Sudha Rao", 
      "D. Marcu", 
      "Kevin Knight", 
      "Hal Daum\u00e9"
    ], 
    "topics": [
      "Recurrent neural network", 
      "BioPAX", 
      "Adaptive Multi-Rate audio codec", 
      "Artificial neural network", 
      "Interaction", 
      "Parsing", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.110", 
    "title": "A New Approach to Overgenerating and Scoring Abstractive Summaries", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users\u2019 needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 72, 
    "authors": [
      "Kaiqiang Song", 
      "Bingqing Wang", 
      "Z. Feng", 
      "Fei Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075245", 
    "title": "Minimally Supervised Morphological Analysis by Multimodal Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a corpus-based algorithm capable of inducing inflectional morphological analyses of both regular and highly irregular forms (such as brought\u2192bring) from distributional patterns in large monolingual text with no direct supervision. The algorithm combines four original alignment models based on relative corpus frequency, contextual similarity, weighted string similarity and incrementally retrained inflectional transduction probabilities. Starting with no paired examples for training and no prior seeding of legal morphological transformations, accuracy of the induced analyses of 3888 past-tense test cases in English exceeds 99.2% for the set, with currently over 80% accuracy on the most highly irregular forms and 99.7% accuracy on forms exhibiting non-concatenative suffixation.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 21, 
    "authors": [
      "David Yarowsky", 
      "R. Wicentowski"
    ], 
    "topics": [
      "Algorithm", 
      "Text corpus", 
      "String metric", 
      "Multimodal interaction", 
      "Transduction (machine learning)", 
      "Test case"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.476", 
    "title": "Debunking Rumors on Twitter with Tree Transformer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Rumors are manufactured with no respect for accuracy, but can circulate quickly and widely by \u201cword-of-post\u201d through social media conversations. Conversation tree encodes important information indicative of the credibility of rumor. Existing conversation-based techniques for rumor detection either just strictly follow tree edges or treat all the posts fully-connected during feature learning. In this paper, we propose a novel detection model based on tree transformer to better utilize user interactions in the dialogue where post-level self-attention plays the key role for aggregating the intra-/inter-subtree stances. Experimental results on the TWITTER and PHEME datasets show that the proposed approach consistently improves rumor detection performance.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 34, 
    "authors": [
      "Jing Ma", 
      "Wei Gao"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-0502", 
    "title": "Natural Language Inference with Monotonicity", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a working system which performs natural language inference using polarity-marked parse trees. The system handles all of the instances of monotonicity inference in the FraCaS data set. Except for the initial parse, it is entirely deterministic. It handles multi-premise arguments, and the kind of inference performed is essentially \u201clogical\u201d, but it goes beyond what is representable in first-order logic. In any case, the system works on surface forms rather than on representations of any kind.", 
    "year": 2019, 
    "venue": "IWCS", 
    "references": 25, 
    "authors": [
      "Hai Hu", 
      "Qi Chen", 
      "Larry Moss"
    ], 
    "topics": [
      "Natural language", 
      "Parsing", 
      "Parse tree", 
      "First-order logic", 
      "Combinatory categorial grammar", 
      "First-order predicate"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1059", 
    "title": "Parameter-free Sentence Embedding via Orthogonal Basis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word\u2019s novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Our model shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 37, 
    "authors": [
      "Ziyi Yang", 
      "Chenguang Zhu", 
      "Weizhu Chen"
    ], 
    "topics": [
      "Word embedding", 
      "Basis (linear algebra)", 
      "Downstream (software development)", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5036", 
    "title": "Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The availability of large-scale and real-time data on social media has motivated research into adverse drug reactions (ADRs). ADR classification helps to identify negative effects of drugs, which can guide health professionals and pharmaceutical companies in making medications safer and advocating patients\u2019 safety. Based on the observation that in social media, negative sentiment is frequently expressed towards ADRs, this study presents a neural model that combines sentiment analysis with transfer learning techniques to improve ADR detection in social media postings. Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus. We then apply transfer learning to adapt the model to the task of detecting ADRs in social media postings. We show that, in combination with rich representations of words and their contexts, transfer learning is beneficial, especially given the large degree of vocabulary overlap between the current affairs posts in the SemEval17-task4A corpus and posts about ADRs. We compare our results with previous approaches, and show that our model can outperform them by up to 3% F-score.", 
    "year": 2019, 
    "venue": "BioNLP@ACL", 
    "references": 32, 
    "authors": [
      "Hassan Alhuzali", 
      "S. Ananiadou"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Social media", 
      "F1 score", 
      "Real-time data", 
      "Vocabulary", 
      "Sensor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.13", 
    "title": "A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 17, 
    "authors": [
      "Shyh-Shiun Hung", 
      "Hen-Hsen Huang", 
      "Hsin-Hsi Chen"
    ], 
    "topics": [
      "Parser"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1903", 
    "title": "Conversing with the elderly in Latin America: a new cohort for multimodal, multilingual longitudinal studies on aging", 
    "fields_of_study": [
      "Geography"
    ], 
    "abstract": "Many studies have found that language alterations can aid in the detection of certain medical afflictions. In this work, we present an ongoing project for recollecting multilingual conversations with the elderly in Latin America. This project, so far, involves the combined efforts of psychogeriatricians, linguists, computer scientists, research nurses and geriatric caregivers from six institutions across USA, Canada, Mexico and Ecuador. The recollections are being made available to the international research community. They consist of conversations with adults aged sixty and over, with different nationalities and socio-economic backgrounds. Conversations are recorded on video, transcribed and time-aligned. Additionally, we are in the process of receiving written texts\u2014recent or old\u2014authored by the participants, provided voluntarily. Each participant is recorded at least twice a year to allow longitudinal studies. Furthermore, information such as medical history, educational background, economic level, occupation, medications and treatments is being registered to aid conducting research on treatment progress and pharmacological effects. Potential studies derived from this work include speech, voice, writing, discourse, and facial and corporal expression analysis. We believe that our recollections incorporate complementary data that can aid researchers in further understanding the progression of cognitive degenerative diseases of the elderly.", 
    "year": 2016, 
    "venue": "", 
    "references": 24, 
    "authors": [
      "Laura Hern\u00e1ndez-Dom\u00ednguez", 
      "S. Ratt\u00e9", 
      "B. Davis", 
      "C. Pope"
    ], 
    "topics": [
      "Multimodal interaction", 
      "Multilingualism", 
      "Dermox", 
      "latin language", 
      "Video", 
      "Speech recognition", 
      "Color gradient", 
      "Behavior", 
      "Neurodegenerative Disorders", 
      "Abnormal degeneration", 
      "Computer scientist", 
      "Pharmacology", 
      "Registration", 
      "Sixty", 
      "Alignment"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NUSE-1.3", 
    "title": "Learning Similarity between Movie Characters and Its Potential Implications on Understanding Human Experiences", 
    "fields_of_study": [
      "Psychology", 
      "Computer Science"
    ], 
    "abstract": "While many different aspects of human experiences have been studied by the NLP community, none has captured its full richness. We propose a new task to capture this richness based on an unlikely setting: movie characters. We sought to capture theme-level similarities between movie characters that were community-curated into 20,000 themes. By introducing a two-step approach that balances performance and efficiency, we managed to achieve 9-27% improvement over recent paragraph-embedding based methods. Finally, we demonstrate how the thematic information learnt from movie characters can potentially be used to understand themes in the experience of people, as indicated on Reddit posts.", 
    "year": 2021, 
    "venue": "NUSE", 
    "references": 39, 
    "authors": [
      "Zhiling Wang", 
      "Weizhe Lin", 
      "Xiaodong Wu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1295", 
    "title": "A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We argue that external commonsense knowledge and linguistic constraints need to be incorporated into neural network models for mitigating data sparsity issues and further improving the performance of discourse parsing. Realizing that external knowledge and linguistic constraints may not always apply in understanding a particular context, we propose a regularization approach that tightly integrates these constraints with contexts for deriving word representations. Meanwhile, it balances attentions over contexts and constraints through adding a regularization term into the objective function. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset PDTB for discourse parsing.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 61, 
    "authors": [
      "Zeyu Dai", 
      "Ruihong Huang"
    ], 
    "topics": [
      "Parsing", 
      "Matrix regularization"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.353", 
    "title": "Real-Valued Logics for Typological Universals: Framework and Application", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes a framework for the expression of typological statements which uses real-valued logics to capture the empirical truth value (truth degree) of a formula on a given data source, e.g. a collection of multilingual treebanks with comparable annotation. The formulae can be arbitrarily complex expressions of propositional logic. To illustrate the usefulness of such a framework, we present experiments on the Universal Dependencies treebanks for two use cases: (i) empirical (re-)evaluation of established formulae against the spectrum of available treebanks and (ii) evaluating new formulae (i.e. potential candidates for universals) generated by a search algorithm.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 36, 
    "authors": [
      "Tillmann D\u00f6nicke", 
      "Xiang Yu", 
      "Jonas Kuhn"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3537", 
    "title": "Analysing Data-To-Text Generation Benchmarks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A generation system can only be as good as the data it is trained on. In this short paper, we propose a methodology for analysing data-to-text corpora used for training Natural Language Generation (NLG) systems. We apply this methodology to three existing benchmarks. We conclude by eliciting a set of criteria for the creation of a data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text generators.", 
    "year": 2017, 
    "venue": "INLG", 
    "references": 16, 
    "authors": [
      "Laura Perez-Beltrachini", 
      "Claire Gardent"
    ], 
    "topics": [
      "Benchmark (computing)", 
      "Stemming", 
      "Crowdsourcing", 
      "Text corpus", 
      "Backup", 
      "Trusted Computer System Evaluation Criteria", 
      "Spatial variability"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.437", 
    "title": "Decompose, Fuse and Generate: A Formation-Informed Method for Chinese Definition Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (\u201c\u6843\u82b1\u201d, peach-blossom) is formed by formation components (\u201c\u6843\u201d, peach; \u201c\u82b1\u201d, flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 23, 
    "authors": [
      "Hua Zheng", 
      "Damai Dai", 
      "Lei Li", 
      "Tianyu Liu", 
      "Zhifang Sui", 
      "Baobao Chang", 
      "Yang Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1113", 
    "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 36, 
    "authors": [
      "Suncong Zheng", 
      "Feng Wang", 
      "Hongyun Bao", 
      "Yuexing Hao", 
      "P. Zhou", 
      "Bo Xu"
    ], 
    "topics": [
      "Entity", 
      "Softmax function", 
      "Information extraction", 
      "End-to-end principle", 
      "Pipeline (computing)", 
      "Experiment", 
      "Refinement (computing)", 
      "Triplet state", 
      "Entity\u2013relationship model"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.435", 
    "title": "Do RNN States Encode Abstract Phonological Alternations?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation\u2014a complex set of sound changes triggered in some words by certain suffixes. We find that our models often\u2014though not always\u2014encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 26, 
    "authors": [
      "Miikka Silfverberg", 
      "Francis M. Tyers", 
      "Garrett Nicolai", 
      "Mans Hulden"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075280", 
    "title": "Utilizing the World Wide Web as an Encyclopedia: Extracting Term Descriptions from Semi-Structured Texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose a method to extract descriptions of technical terms from Web pages in order to utilize the World Wide Web as an encyclopedia. We use linguistic patterns and HTML text structures to extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 18, 
    "authors": [
      "Atsushi Fujii", 
      "T. Ishikawa"
    ], 
    "topics": [
      "World Wide Web", 
      "Experiment", 
      "Language model", 
      "Cluster analysis", 
      "HTML", 
      "Web page", 
      "Structural pattern", 
      "Relevance", 
      "Resultant", 
      "Natural language processing", 
      "Semiconductor industry"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K17-3012", 
    "title": "TurkuNLP: Delexicalized Pre-training of Word Embeddings for Dependency Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the TurkuNLP entry in the CoNLL 2017 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies. The system is based on the UDPipe parser with our focus being in ex- \nploring various techniques to pre-train the word embeddings used by the parser in order to improve its performance especially on languages with small training sets. The system ranked 11th among the 33 participants overall, being 8th on the small treebanks, 10th on the large treebanks, 12th on the parallel test sets, and 26th on the sur- \nprise languages.", 
    "year": 2017, 
    "venue": "CoNLL Shared Task", 
    "references": 14, 
    "authors": [
      "Jenna Kanerva", 
      "Juhani Luotolahti", 
      "Filip Ginter"
    ], 
    "topics": [
      "Parsing", 
      "Treebank", 
      "N-gram", 
      "Word embedding", 
      "Word2vec", 
      "Sampling (signal processing)", 
      "Hoc (programming language)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/D19-5827", 
    "title": "Generalizing Question Answering System with Pre-trained Language Model Fine-tuning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC)tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these mod-els and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our model is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our methods, with an average Exact Match score of 56.59 and an average F1 score of 68.98, which significantly improves the BERT-Large baseline by8.39 and 7.22, respectively", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Dan Su", 
      "Yan Xu", 
      "Genta Indra Winata", 
      "Peng Xu", 
      "Hyeon-Jin Kim", 
      "Zihan Liu", 
      "Pascale Fung"
    ], 
    "topics": [
      "Question answering", 
      "Language model", 
      "F1 score", 
      "Multi-task learning", 
      "Computer multitasking", 
      "Baseline (configuration management)", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1287", 
    "title": "Document-Level Event Factuality Identification via Adversarial Neural Network", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP). Previous studies mainly suffer from the scarcity of suitable corpus and effective methods. To solve these two issues, we first construct a corpus annotated with both document- and sentence-level event factuality information on both English and Chinese texts. Then we present an LSTM neural network based on adversarial training with both intra- and inter-sequence attentions to identify document-level event factuality. Experimental results show that our neural network model can outperform various baselines on the constructed corpus.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 38, 
    "authors": [
      "Zhong Qian", 
      "Peifeng Li", 
      "Qiaoming Zhu", 
      "Guodong Zhou"
    ], 
    "topics": [
      "Artificial neural network", 
      "Natural language processing", 
      "Long short-term memory", 
      "Text corpus", 
      "Baseline (configuration management)", 
      "Effective method", 
      "Network model", 
      "EarthBound"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1106", 
    "title": "RICOH at SemEval-2016 Task 1: IR-based Semantic Textual Similarity Estimation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our IR (Information Retrieval) based method for SemEval 2016 task 1, Semantic Textual Similarity (STS). The main feature of our approach is to extend a conventional IR-based scheme by incorporating word alignment information. This enables us to develop a more fine-grained similarity measurement. In the evaluation results, we have seen that the proposed method improves upon a conventional IR-based method on average. In addition, one of our submissions achieved the best performance for the \u201cpostediting\u201d data set.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 13, 
    "authors": [
      "Hideo Itoh"
    ], 
    "topics": [
      "SemEval", 
      "Postediting", 
      "Bitext word alignment", 
      "S\u00f8rensen\u2013Dice coefficient", 
      "Information retrieval", 
      "Similarity measure", 
      "Semantic similarity", 
      "Coefficient of determination", 
      "Overlap\u2013add method", 
      "Data structure alignment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.5", 
    "title": "TICO-19: the Translation Initiative for Covid-19", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, \"pivot\" languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 25, 
    "authors": [
      "Antonios Anastasopoulos", 
      "A. Cattelan", 
      "Zi-Yi Dou", 
      "Marcello Federico", 
      "C. Federman", 
      "Dmitriy Genzel", 
      "Francisco Guzm'an", 
      "Junjie Hu", 
      "Macduff Hughes", 
      "Philipp Koehn", 
      "Rosie Lazar", 
      "Will Lewis", 
      "Graham Neubig", 
      "Meng Niu", 
      "A. Oktem", 
      "Eric Paquin", 
      "G. Tang", 
      "Sylwia Tur"
    ], 
    "topics": [
      "Population", 
      "Translation memory", 
      "Freedom of information laws by country", 
      "Stemming", 
      "Mike Lesser"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5301", 
    "title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 34, 
    "authors": [
      "Prithviraj Ammanabrolu", 
      "Mark O. Riedl"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Knowledge Graph", 
      "Complexity", 
      "Stepping level", 
      "Question answering"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6217", 
    "title": "Aspect Based Sentiment Analysis into the Wild", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we test state-of-the-art Aspect Based Sentiment Analysis (ABSA) systems trained on a widely used dataset on actual data. We created a new manually annotated dataset of user generated data from the same domain as the training dataset, but from other sources and analyse the differences between the new and the standard ABSA dataset. We then analyse the results in performance of different versions of the same system on both datasets. We also propose light adaptation methods to increase system robustness.", 
    "year": 2018, 
    "venue": "WASSA@EMNLP", 
    "references": 23, 
    "authors": [
      "C. Brun", 
      "Vassilina Nikoulina"
    ], 
    "topics": [
      "Sentiment analysis", 
      "F1 score", 
      "Performance", 
      "End-to-end principle", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075233", 
    "title": "A Unified Statistical Model for the Identification of English BaseNP", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a novel statistical model for automatic identification of English baseNP. It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences. Unlike the other approaches where the two steps are separated, we integrate them into a unified statistical framework. Our model also integrates lexical information. Finally, Viterbi algorithm is applied to make global search in the entire sentence, allowing us to obtain linear complexity for the entire process. Compared with other methods using the same testing set, our approach achieves 92.3% in precision and 93.2% in recall. The result is comparable with or better than the previously reported results.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 14, 
    "authors": [
      "Endong Xun", 
      "C. Huang", 
      "M. Zhou"
    ], 
    "topics": [
      "Statistical model", 
      "Viterbi algorithm", 
      "Part-of-speech tagging", 
      "Lexicon", 
      "Automatic identification and data capture", 
      "Preprocessor", 
      "Experiment", 
      "Identifier"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli_a_00342", 
    "title": "Learning an Executable Neural Semantic Parser", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.", 
    "year": 2019, 
    "venue": "CL", 
    "references": 124, 
    "authors": [
      "Jianpeng Cheng", 
      "Siva Reddy", 
      "V. Saraswat", 
      "Mirella Lapata"
    ], 
    "topics": [
      "Executable", 
      "Natural language", 
      "Knowledge base", 
      "Recurrent neural network", 
      "Parser", 
      "Database", 
      "Algorithm", 
      "Map", 
      "Artificial neural network", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00191", 
    "title": "Squibs: On the Universal Generation Problem for Unification Grammars", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The universal generation problem for unification grammars is the problem of determining whether a given grammar derives any terminal string with a given feature structure. It is known that the problem is decidable for LFG and PATR grammars if only acyclic feature structures are taken into consideration. In this brief note, we show that the problem is undecidable for cyclic structures. This holds even for grammars that are off-line parsable.", 
    "year": 2014, 
    "venue": "CL", 
    "references": 7, 
    "authors": [
      "J\u00fcrgen Wedekind"
    ], 
    "topics": [
      "PATR-II", 
      "Unification (computer science)", 
      "Undecidable problem", 
      "Parsing", 
      "Online and offline", 
      "Directed acyclic graph", 
      "Han unification", 
      "Lexical functional grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6220", 
    "title": "Dual Memory Network Model for Biased Product Review Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle user profile and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two representations are used jointly for sentiment prediction. The use of separate models aims to capture user profiles and product information more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6%, 1.2%, and 0.9%, respectively. The improvements are also deemed very significant measured by p-values.", 
    "year": 2018, 
    "venue": "WASSA@EMNLP", 
    "references": 29, 
    "authors": [
      "Yunfei Long", 
      "Mingyu Derek Ma", 
      "Q. Lu", 
      "Rong Xiang", 
      "Chu-Ren Huang"
    ], 
    "topics": [
      "User profile", 
      "Sentiment analysis", 
      "Network model", 
      "Unified Model", 
      "Internet Movie Database (IMDb)", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.298", 
    "title": "Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 25, 
    "authors": [
      "Gaku Morio", 
      "Hiroaki Ozaki", 
      "Terufumi Morishita", 
      "Yuta Koreeda", 
      "K. Yanai"
    ], 
    "topics": [
      "Parsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.spnlp-1.2", 
    "title": "CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.", 
    "year": 2020, 
    "venue": "SPNLP", 
    "references": 29, 
    "authors": [
      "Abhinav Singh", 
      "Patrick Xia", 
      "Guanghui Qin", 
      "M. Yarmohammadi", 
      "Benjamin Van Durme"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118253.1118275", 
    "title": "Planning word-order dependant focus assignments", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word order and accent placement are the primary linguistic means to indicate focus/background structures in German. This paper presents a pipelined architecture for the generation of German monologues with contextually appropriate word order and accent placements for the realization of focus/background structures. Our emphasis is on the sentence planner that extends the respective propositional contents with discourse-relational features and decides which part will be focused. Such an enriched semantic input for an HPSG-based formulator allows word order variations and the placement of prenucleus and nucleus accents. Word order is realized by grammatical competition based on linear precedence (LP) rules which are based on the discourse-relational features. Accent placement is realized by a syntax-driven focus principle that determines the focus exponent and possible bearers of prenucleus accents within the syntactically realized focus, the so-called focus domain.", 
    "year": 2000, 
    "venue": "INLG", 
    "references": 46, 
    "authors": [
      "C. Endriss", 
      "R. Klabunde"
    ], 
    "topics": [
      "Head-driven phrase structure grammar", 
      "Interaction", 
      "State (computer science)", 
      "Top-down and bottom-up design", 
      "Natural language generation", 
      "Focus group"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1232", 
    "title": "Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In Semantic Dependency Parsing (SDP), semantic relations form directed acyclic graphs, rather than trees. We propose a new iterative predicate selection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. We train the IPS model using a combination of multi-task learning and task-specific policy gradient training. Trained this way, IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we observe that policy gradient training learns an easy-first strategy.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 43, 
    "authors": [
      "Shuhei Kurita", 
      "Anders S\u00f8gaard"
    ], 
    "topics": [
      "Parsing", 
      "Gradient", 
      "SemEval", 
      "Directed acyclic graph", 
      "Semantic Differential", 
      "Algorithm", 
      "Multi-task learning", 
      "Numerous", 
      "Computer multitasking", 
      "Iterative method", 
      "emotional dependency", 
      "Trees (plant)", 
      "Graph - visual representation", 
      "Thymic T-Cell Selection"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1117769.1117780", 
    "title": "A Trainable Method for Extracting Chinese Entity Names and Their Relations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we propose a trainable method for extracting Chinese entity names and their relations. We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them. Preliminary results show that this method is efficient, flexible and promising to achieve better performance than other existing methods.", 
    "year": 2000, 
    "venue": "ACL 2000", 
    "references": 11, 
    "authors": [
      "Yimin Zhang", 
      "Joe F. Zhou"
    ], 
    "topics": [
      "Name", 
      "Instance-based learning"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.74", 
    "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Large generative language models have been very successful for English, but other languages lag behind due to data and computational limitations. We propose a method that may overcome these problems by adapting existing pre-trained language models to new languages. Specifically, we describe the adaptation of English GPT-2 to Italian and Dutch by retraining lexical embeddings without tuning the Transformer layers. As a result, we obtain lexical embeddings for Italian and Dutch that are aligned with the original English lexical embeddings and induce a bilingual lexicon from this alignment. Additionally, we show how to scale up complexity by transforming relearned lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch, but on average these sentences are still identifiable as artificial by humans. Based on perplexity scores and human judgements, we find that generated sentences become more realistic with some additional full model finetuning, especially for Dutch. For Italian, we see that they are evaluated on par with sentences generated by a GPT-2 model fully trained from scratch. Our work can be conceived as a blueprint for training GPT-2s for other languages, and we provide a 'recipe' to do so.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 44, 
    "authors": [
      "Wietse de Vries", 
      "M. Nissim"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-2024", 
    "title": "Simple PPDB: A Paraphrase Database for Simplification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simplification. We train a supervised model to associate simplification scores with each phrase pair, producing rankings competitive with state-of-theart lexical simplification models. Our new simplification database contains 4.5 million paraphrase rules, making it the largest available resource for lexical simplification.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 27, 
    "authors": [
      "Ellie Pavlick", 
      "Chris Callison-Burch"
    ], 
    "topics": [
      "Text simplification", 
      "Lexical simplification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.ACL-MAIN.649", 
    "title": "Predicting the Growth of Morphological Families from Social and Linguistic Factors", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as \u201ctrump\u201d, \u201cantitrumpism\u201d, and \u201cdetrumpify\u201d, in social media. We introduce the novel task of Morphological Family Expansion Prediction (MFEP) as predicting the increase in the size of a morphological family. We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark. Our experiments demonstrate very good performance on MFEP.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 58, 
    "authors": [
      "Valentin Hofmann", 
      "J. Pierrehumbert", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "Social media", 
      "Experiment", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4736", 
    "title": "The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2017", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2017, 
    "venue": "WMT", 
    "references": 25, 
    "authors": [
      "Ngoc-Quan Pham", 
      "J. Niehues", 
      "Thanh-Le Ha", 
      "Eunah Cho", 
      "Matthias Sperber", 
      "A. Waibel"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/V1/W14-4412", 
    "title": "Adapting SimpleNLG for Brazilian Portuguese realisation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the ongoing implementation and the current coverage of SimpleNLG-BP, an adaptation of SimpleNLG-EnFr (Vaudry and Lapalme, 2013) for Brazilian Portuguese.", 
    "year": 2014, 
    "venue": "INLG", 
    "references": 4, 
    "authors": [
      "Rodrigo Gomes de Oliveira", 
      "S. Sripada"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.69", 
    "title": "Syntax-Aware Graph Attention Network for Aspect-Level Sentiment Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Aspect-level sentiment classification aims to distinguish the sentiment polarities over aspect terms in a sentence. Existing approaches mostly focus on modeling the relationship between the given aspect words and their contexts with attention, and ignore the use of more elaborate knowledge implicit in the context. In this paper, we exploit syntactic awareness to the model by the graph attention network on the dependency tree structure and external pre-training knowledge by BERT language model, which helps to model the interaction between the context and aspect words better. And the subwords of BERT are integrated into the dependency tree graphs, which can obtain more accurate representations of words by graph attention. Experiments demonstrate the effectiveness of our model.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 39, 
    "authors": [
      "Lianzhe Huang", 
      "Xin Sun", 
      "Sujian Li", 
      "Linhao Zhang", 
      "Houfeng Wang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1129", 
    "title": "Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In syntax-based machine translation, rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side. We define a discriminative rule selection model for systems that have syntactic annotation on the target language side (stringto-tree). This is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model. We release our implementation as part of Moses.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 35, 
    "authors": [
      "Fabienne Braune", 
      "Nina Seemann", 
      "Alexander M. Fraser"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Moses", 
      "Selection algorithm", 
      "Compiler", 
      "Definition"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.495", 
    "title": "WSL-DS: Weakly Supervised Learning with Distant Supervision for Query Focused Multi-Document Abstractive Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the Query Focused Multi-Document Summarization (QF-MDS) task, a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However, one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue, in this paper, we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular, we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then, we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents (i.e., long sequences) at once. Experimental results on the Document Understanding Conferences (DUC) datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 49, 
    "authors": [
      "Md Tahmid Rahman Laskar", 
      "Enamul Hoque", 
      "J. Huang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.446", 
    "title": "Weakly-Supervised Modeling of Contextualized Event Embedding for Discourse Relations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrative graph and learn contextualized event representations over it using a relational graph neural network model. We train our model to capture event relations, derived from the Penn Discourse Tree Bank, on a huge corpus, and show that our multi-relational contextualized event representation can improve performance when learning script knowledge without direct supervision and provide a better representation for the implicit discourse sense classification task.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 46, 
    "authors": [
      "I-Ta Lee", 
      "Maria Leonor Pacheco", 
      "Dan Goldwasser"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-7811", 
    "title": "tweeDe \u2013 A Universal Dependencies treebank for German tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce the first German treebank for Twitter microtext, annotated within the framework of Universal Dependencies. The new treebank includes over 12,000 tokens from over 500 tweets, independently annotated by two human coders. In the paper, we describe the data selection and annotation process and present baseline parsing results for the new testsuite.", 
    "year": 2019, 
    "venue": "", 
    "references": 23, 
    "authors": [
      "Ines Rehbein", 
      "Josef Ruppenhofer", 
      "Bich-Ngoc Do"
    ], 
    "topics": [
      "Treebank", 
      "Parsing", 
      "Test suite", 
      "Point of sale", 
      "Microprinting", 
      "Baseline (configuration management)", 
      "Urban Dictionary", 
      "Galaxy morphological classification", 
      "USB Attached SCSI"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-2098", 
    "title": "Parser Training with Heterogeneous Treebanks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without fine-tuning. We go on to propose a new method based on treebank embeddings. We perform experiments for several languages and show that in many cases fine-tuning and treebank embeddings lead to substantial improvements over single treebanks or concatenation, with average gains of 2.0\u20133.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 21, 
    "authors": [
      "Sara Stymne", 
      "Miryam de Lhoneux", 
      "Aaron Smith", 
      "Joakim Nivre"
    ], 
    "topics": [
      "Treebank", 
      "Concatenation", 
      "Parser", 
      "Extensibility", 
      "Experiment", 
      "Mathematical optimization"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.210", 
    "title": "Ruddit: Norms of Offensiveness for English Reddit Comments", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Warning: This paper contains comments that may be offensive or upsetting. On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best\u2013Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 62, 
    "authors": [
      "Rishav Hada", 
      "S. Sudhir", 
      "Pushkar Mishra", 
      "H. Yannakoudakis", 
      "Saif M. Mohammad", 
      "Ekaterina Shutova"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.clinicalnlp-1.11", 
    "title": "Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the sequence of the notes. We evaluated our models on prolonged mechanical ventilation prediction problem and our experiments demonstrated that Clinical XLNet outperforms the best baselines consistently. The models and scripts are made publicly available.", 
    "year": 2020, 
    "venue": "CLINICALNLP", 
    "references": 37, 
    "authors": [
      "Kexin Huang", 
      "Abhishek Singh", 
      "Sitong Chen", 
      "E. Moseley", 
      "Chih-ying Deng", 
      "Naomi George", 
      "C. Lindvall"
    ], 
    "topics": [
      "Predictive modelling", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981131.981161", 
    "title": "Linguistic Coherence: a Plan-Based Alternative", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To fully understand a sequence of utterances, one must be able to infer implicit relationships between the utterances. Although the identification of sets of utterance relationships forms the basis for many theories of discourse, the formalization and recognition of such relationships has proven to be an extremely difficult computational task.This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances. Relationships are formulated as discourse plans, which allows their representation in terms of planning operators and their computation via a plan recognition process. By incorporating complex inferential processes relating utterances into a plan-based framework, a formalization and computability not available in the earlier works is provided.", 
    "year": 1986, 
    "venue": "ACL", 
    "references": 33, 
    "authors": [
      "D. Litman"
    ], 
    "topics": [
      "Domain of discourse", 
      "Linear algebra", 
      "Constraint satisfaction", 
      "Computability", 
      "Model of computation", 
      "Computation", 
      "Theory", 
      "Inferential theory of learning", 
      "Ext JS JavaScript Framework"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1817", 
    "title": "Automatic Generation of Challenging Distractors Using Context-Sensitive Inference Rules", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatically generating challenging distractors for multiple-choice gap-fill items is still an unsolved problem. We propose to employ context-sensitive lexical inference rules in order to generate distractors that are semantically similar to the gap target word in some sense, but not in the particular sense induced by the gap-fill context. We hypothesize that such distractors should be particularly hard to distinguish from the correct answer. We focus on verbs as they are especially difficult to master for language learners and find that our approach is quite effective. In our test set of 20 items, our proposed method decreases the number of invalid distractors in 90% of the cases, and fully eliminates all of them in 65%. Further analysis on that dataset does not support our hypothesis regarding item difficulty as measured by average error rate of language learners. We conjecture that this may be due to limitations in our evaluation setting, which we plan to address in future work.", 
    "year": 2014, 
    "venue": "BEA@ACL", 
    "references": 18, 
    "authors": [
      "Torsten Zesch", 
      "Oren Melamud"
    ], 
    "topics": [
      "Context-sensitive grammar", 
      "Experiment", 
      "Emily Howell", 
      "Frequency band", 
      "Test set", 
      "Randomness", 
      "Star filler", 
      "Bit error rate"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1594", 
    "title": "Text Genre and Training Data Size in Human-like Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Domain-specific training typically makes NLP systems work better. We show that this extends to cognitive modeling as well by relating the states of a neural phrase-structure parser to electrophysiological measures from human participants. These measures were recorded as participants listened to a spoken recitation of the same literary text that was supplied as input to the neural parser. Given more training data, the system derives a better cognitive model \u2014 but only when the training examples come from the same textual genre. This finding is consistent with the idea that humans adapt syntactic expectations to particular genres during language comprehension (Kaan and Chun, 2018; Branigan and Pickering, 2017).", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 42, 
    "authors": [
      "John Hale", 
      "A. Kuncoro", 
      "Keith B. Hall", 
      "Chris Dyer", 
      "Jonathan Brennan"
    ], 
    "topics": [
      "Parsing", 
      "Test set"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2407", 
    "title": "Cooking with Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We are interested in the automatic interpretation of how-to instructions, such as cooking recipes, into semantic representations that can facilitate sophisticated question answering. Recent work has shown impressive results on semantic parsing of instructions with minimal supervision, but such techniques cannot handle much of the situated and ambiguous language used in instructions found on the web. In this paper, we suggest how to extend such methods using a model of pragmatics, based on a rich representation of world state.", 
    "year": 2014, 
    "venue": "ACL 2014", 
    "references": 26, 
    "authors": [
      "J. Malmaud", 
      "Earl J. Wagner", 
      "Nancy Chang", 
      "K. Murphy"
    ], 
    "topics": [
      "Question answering", 
      "Situated", 
      "Parsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2045", 
    "title": "Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While neural networks have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) \u2014 extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects \u2014 remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external \u201cmemory chains\u201d with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 20, 
    "authors": [
      "Fei Liu", 
      "Trevor Cohn", 
      "Timothy Baldwin"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Entity", 
      "Natural language understanding", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6538", 
    "title": "Task Proposal: The TL;DR Challenge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The TL;DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the web, which has been overlooked by summarization research so far. The challenge owes its name to the frequent practice of social media users to supplement long posts with a \u201cTL;DR\u201d\u2014for \u201ctoo long; didn\u2019t read\u201d\u2014followed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length. Posts featuring TL;DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from social media, opening the door to all kinds of generative models.", 
    "year": 2018, 
    "venue": "INLG", 
    "references": 10, 
    "authors": [
      "Shahbaz Syed", 
      "Michael V\u00f6lske", 
      "Martin Potthast", 
      "Nedim Lipka", 
      "Benno Stein", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "TL;DR", 
      "Social media", 
      "Text corpus", 
      "Generative model", 
      "Ground truth", 
      "Automatic summarization", 
      "Fastest", 
      "MinEd", 
      "Natural language generation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073012.1073065", 
    "title": "Automatic Detection of Syllable Boundaries Combining the Advantages of Treebank and Bracketed Corpora Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An approach to automatic detection of syllable boundaries is presented. We demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training. We investigate the effect of the training corpus size on the performance of our system. The evaluation shows that a hand-written grammar performs better on finding syllable boundaries than does a treebank grammar.", 
    "year": 2001, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "K. M\u00fcller"
    ], 
    "topics": [
      "Treebank", 
      "Syllable", 
      "Text corpus", 
      "Algorithm", 
      "Categorial grammar"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.MAIWORKSHOP-1.2", 
    "title": "On Randomized Classification Layers and Their Implications in Natural Language Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In natural language generation tasks, a neural language model is used for generating a sequence of words forming a sentence. The topmost weight matrix of the language model, known as the classification layer, can be viewed as a set of vectors, each representing a target word from the target dictionary. The target word vectors, along with the rest of the model parameters, are learned and updated during training. In this paper, we analyze the properties encoded in the target vectors and question the necessity of learning these vectors. We suggest to randomly draw the target vectors and set them as fixed so that no weights updates are being made during training. We show that by excluding the vectors from the optimization, the number of parameters drastically decreases with a marginal effect on the performance. We demonstrate the effectiveness of our method in image-captioning and machine-translation.", 
    "year": 2021, 
    "venue": "MAIWORKSHOP", 
    "references": 19, 
    "authors": [
      "Gal-Lev Shalev", 
      "Gabi Shalev", 
      "Joseph Keshet"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-1061", 
    "title": "Classifying Relations by Ranking with Convolutional Neural Networks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "C. D. Santos", 
      "Bing Xiang", 
      "Bowen Zhou"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Loss function", 
      "Artificial neural network", 
      "Softmax function", 
      "Precision and recall", 
      "SemEval", 
      "Effective method", 
      "Word embedding", 
      "Experiment", 
      "Neural network software"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974557.974606", 
    "title": "An Intelligent Multilingual Information Browsing and Retrieval System Using Information Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we describe our multilingual (or cross-linguistic) information browsing and retrieval system, which is aimed at monolingual users who are interested in information from multiple language sources. The system takes advantage of information extraction (IE) technology in novel ways to improve the accuracy of cross-linguistic retrieval and to provide innovative methods for browsing and exploring multilingual document collections. The system indexes texts in different languages (e.g., English and Japanese) and allows the users to retrieve relevant texts in their native language (e.g., English). The retrieved text is then presented to the users with proper names and specialized domain terms translated and hyperlinked. Moreover, the system allows interactive information discovery from a multilingual document collection.", 
    "year": 1997, 
    "venue": "ANLP", 
    "references": 9, 
    "authors": [
      "Chinatsu Aone", 
      "Nicholas Charocopos", 
      "James Gorlinsky"
    ], 
    "topics": [
      "Information extraction", 
      "Multilingualism", 
      "Web crawler", 
      "Graphical user interface", 
      "Information discovery", 
      "Machine translation", 
      "Browsing", 
      "Name", 
      "Linguistics", 
      "Archive", 
      "Programming Languages", 
      "Index", 
      "Just-in-time compilation", 
      "World Wide Web", 
      "Web search engine", 
      "User interface", 
      "Collections (publication)", 
      "Business software", 
      "HTML", 
      "Web application", 
      "Compiler", 
      "Multi-user", 
      "Hyperlink", 
      "Entity", 
      "Online and offline", 
      "Bi-directional text", 
      "Semiconductor industry", 
      "Open Database Connectivity", 
      "WWW", 
      "User Interface Device Component", 
      "Indexes", 
      "Ingestion", 
      "Page (document)", 
      "search a word", 
      "Question (inquiry)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-044-1_010", 
    "title": "POMELO: Medline corpus with manually annotated food-drug interactions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "When patients take more than one medication, they may be at risk of drug interactions, which means that a given drug can cause unexpected effects when taken in combination with other drugs. Similar effects may occur when drugs are taken together with some food or beverages. For instance, grapefruit has interactions with several drugs, because its active ingredients inhibit enzymes involved in the drugs metabolism and can then cause an excessive dosage of these drugs. Yet, information on food/drug interactions is poorly researched. The current research is mainly provided by the medical domain and a very tentative work is provided by computer sciences and NLP domains. One factor that motivates the research is related to the availability of the annotated corpora and the reference data. The purpose of our work is to describe the rationale and approach for creation and annotation of scientific corpus with information on food/drug interactions. This corpus contains 639 MEDLINE citations (titles and abstracts), corresponding to 5,752 sentences. It is manually annotated by two experts. The corpus is named POMELO. This annotated corpus will be made available for the research purposes.", 
    "year": 2017, 
    "venue": "BiomedicalNLP@RANLP", 
    "references": 30, 
    "authors": [
      "Thierry Hamon", 
      "Vincent Tabanou", 
      "Fleur Mougin", 
      "N. Grabar", 
      "F. Thiessard"
    ], 
    "topics": [
      "Interaction", 
      "MEDLINE", 
      "Text corpus", 
      "Computer science", 
      "Natural language processing", 
      "Design rationale"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6431", 
    "title": "The University of Maryland\u2019s Chinese-English Neural Machine Translation Systems at WMT18", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the University of Maryland\u2019s submission to the WMT 2018 Chinese\u2194English news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and reranking, we improve over the Transformer baseline by +1.4 BLEU for Chinese\u2192English and +3.97 BLEU for English\u2192Chinese on newstest2017. Our best systems reach BLEU scores of 24.4 for Chinese\u2192English and 39.0 for English\u2192Chinese on newstest2018.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 24, 
    "authors": [
      "Weijia Xu", 
      "Marine Carpuat"
    ], 
    "topics": [
      "Transformer", 
      "BLEU", 
      "Neural machine translation", 
      "Ensemble forecasting", 
      "Right-to-left", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981131.981132", 
    "title": "Tutorial Abstracts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Hofland, Knut and Johansson, Stig. 1982 Word Frequencies in British and American English. Norwegian Computing Centre for the Humanities, Bergen. Jansen, J.; Mergeai, J.P.; and Vanandroye, J. 1987 Controlling LDOCE's Controlled Vocabulary. In: Cowie, A.P., ed., The Dictionary and the Language Learner (Lexicographica, series major, 17). Niemeyer, Tiibingen, 78-94. Mittun, Roger. 1986 A partial dictionary of English in Computer-Usable Form. Literary and Linguistic Computing 1:214--215. Sampson, G.R. 1989 How Fully Does a Machine-Usable Dictionary Cover English Text7 Literary and Linguistic Computing. 4:29-35. Walker, D.E. and Amsler, R.A. 1986 The Use of Machine-Readable Dictionaries in Sublanguage Analysis. In: Grishman, Ralph and Kittredge, Richard, eds., Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Eribaum, Hillsdale, N J: 69-83.", 
    "year": 1986, 
    "venue": "ACL", 
    "references": 8, 
    "authors": [
      "R. Grishman"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/S19-2071", 
    "title": "HATEMINER at SemEval-2019 Task 5: Hate speech detection against Immigrants and Women in Twitter using a Multinomial Naive Bayes Classifier", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our participation in the SemEval 2019 Task 5 - Multilingual Detection of Hate. This task aims to identify hate speech against two specific targets, immigrants and women. We compare and contrast the performance of different word and sentence level embeddings on the state-of-the-art classification algorithms. Our final submission is a Multinomial binarized Naive Bayes model for both the subtasks in the English version.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 21, 
    "authors": [
      "Nikhil Chakravartula"
    ], 
    "topics": [
      "Naive Bayes classifier", 
      "SemEval", 
      "Multinomial logistic regression", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1004", 
    "title": "A Deep Generative Model of Vowel Formant Typology", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information\u2014the first two formant values\u2014rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 26, 
    "authors": [
      "Ryan Cotterell", 
      "Jason Eisner"
    ], 
    "topics": [
      "Biological anthropology", 
      "Generative model", 
      "Inventory", 
      "Text corpus", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1069", 
    "title": "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 51, 
    "authors": [
      "Kristen Marie Johnson", 
      "Di Jin", 
      "Dan Goldwasser"
    ], 
    "topics": [
      "F1 score", 
      "Supervised learning", 
      "Unsupervised learning", 
      "Framing (World Wide Web)", 
      "Framing (social sciences)", 
      "Collective intelligence", 
      "Baseline (configuration management)", 
      "N-gram"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-1302", 
    "title": "Paths for uncertainty: Exploring the intricacies of uncertainty identification for news", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Currently, news articles are produced, shared and consumed at an extremely rapid rate. Although their quantity is increasing, at the same time, their quality and trustworthiness is becoming fuzzier. Hence, it is important not only to automate information extraction but also to quantify the certainty of this information. Automated identification of expressions that affect certainty has been studied both in the scientific and newswire domains, but performance is considerably higher in tasks focusing on scientific text. We compare the differences in the definition and expression of uncertainty between a scientific domain, i.e., biomedicine, and newswire. We delve into the different aspects that affect the certainty of an extracted event in a news article and examine whether they can be easily identified by techniques already validated in the biomedical domain. Finally, we present a comparison of the syntactic and lexical differences between the the expression of certainty in the biomedical and newswire domains, using two annotated corpora.", 
    "year": 2018, 
    "venue": "", 
    "references": 63, 
    "authors": [
      "Chrysoula Zerva", 
      "S. Ananiadou"
    ], 
    "topics": [
      "Information extraction", 
      "Trust (emotion)", 
      "Text corpus", 
      "Biomedicine"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1461", 
    "title": "Revisiting Character-Based Neural Machine Translation with Capacity and Compression", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 22, 
    "authors": [
      "Colin Cherry", 
      "George F. Foster", 
      "Ankur Bapna", 
      "Orhan Firat", 
      "Wolfgang Macherey"
    ], 
    "topics": [
      "Neural machine translation", 
      "Computation", 
      "Testbed", 
      "Feature engineering", 
      "Time complexity", 
      "Call to action (marketing)", 
      "Tokenization (data security)", 
      "Algorithmic efficiency", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Text corpus", 
      "Long short-term memory"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.263", 
    "title": "Inspecting the concept knowledge graph encoded by modern language models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The field of natural language understanding has experienced exponential progress in the last few years, with impressive results in several tasks. This success has motivated researchers to study the underlying knowledge encoded by these models. Despite this, attempts to understand their semantic capabilities have not been successful, often leading to non-conclusive, or contradictory conclusions among different works. Via a probing classifier, we extract the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders. This probe is based on concept relatedness, grounded on WordNet. Our results reveal that all the models encode this knowledge, but suffer from several inaccuracies. Furthermore, we show that the different architectures and training strategies lead to different model biases. We conduct a systematic evaluation to discover specific factors that explain why some concepts are challenging. We hope our insights will motivate the development of models that capture concepts more precisely.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 72, 
    "authors": [
      "C. Aspillaga", 
      "Marcelo Mendoza", 
      "Alvaro Soto"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-5616", 
    "title": "News Sentiment and Cross-Country Fluctuations", 
    "fields_of_study": [
      "Economics", 
      "Computer Science"
    ], 
    "abstract": "What is the information content of news-based measures of sentiment? How are they related to economic fluctuations? I construct a sentiment index by measuring the net amount of positive expressions in the full corpus of Economic news articles produced by Reuters covering 12 countries over the period 1987-2013. The index successfully tracks fluctuations in GDP at the country level, is a leading indicator of GDP growth and contains information on future GDP growth which is not captured by consensus forecasts. This suggests that forecasters do not appropriately incorporate available information in predicting future states of the economy.", 
    "year": 2016, 
    "venue": "NLP+CSS@EMNLP", 
    "references": 18, 
    "authors": [
      "S. Fraiberger"
    ], 
    "topics": [
      "News analytics", 
      "Self-information", 
      "Norm (social)", 
      "Aggregate data", 
      "Economic complexity index", 
      "Real-time computing", 
      "Sparse matrix", 
      "Real-time locating system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4714", 
    "title": "Adapting Neural Machine Translation with Parallel Synthetic Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent works have shown that the usage of a synthetic parallel corpus can be effectively exploited by a neural machine translation system. In this paper, we propose a new method for adapting a general neural machine translation system to a specific task, by exploiting synthetic data. The method consists in selecting, from a large monolingual pool of sentences in the source language, those instances that are more related to a given test set. Next, this selection is automatically translated and the general neural machine translation system is fine-tuned with these data. For evaluating the adaptation method, we first conducted experiments in two controlled domains, with common and wellstudied corpora. Then, we evaluated our proposal on a real e-commerce task, yielding consistent improvements in terms of translation quality.", 
    "year": 2017, 
    "venue": "WMT", 
    "references": 47, 
    "authors": [
      "Mara Chinea-Rios", 
      "\u00c1lvaro Peris", 
      "F. Casacuberta"
    ], 
    "topics": [
      "Synthetic data", 
      "Neural machine translation", 
      "BLEU", 
      "Heuristic (computer science)", 
      "E-commerce", 
      "Robustness (computer science)", 
      "Parallel text", 
      "Synthetic intelligence", 
      "Experiment", 
      "Test set", 
      "Text corpus", 
      "Formal language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.spnlp-1.10", 
    "title": "On the Discrepancy between Density Estimation and Sequence Generation", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y\u02c6 given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y\u02c6, y* | x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets. Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.", 
    "year": 2020, 
    "venue": "SPNLP", 
    "references": 67, 
    "authors": [
      "Jason Lee", 
      "Dustin Tran", 
      "Orhan Firat", 
      "Kyunghyun Cho"
    ], 
    "topics": [
      "Latent variable", 
      "Autoregressive model", 
      "Machine translation", 
      "BLEU", 
      "Structured prediction", 
      "Downstream (software development)", 
      "Speech synthesis", 
      "Test set", 
      "Emoticon", 
      "Discrepancy function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980691.980794", 
    "title": "Aligning Articles in TV Newscasts and Newspapers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "It is important to use pattern information (e.g. TV newscasts) and textual information (e.g. newspapers) together. For this purpose, we describe a method for aligning articles in TV newscasts and newspapers. In order to align articles, the alignment system uses words extracted from telops in TV newscasts. The recall and the precision of the alignment process are 97% and 89%, respectively. In addition, using the results of the alignment process, we develop a browsing and retrieval system for articles in TV newscasts and newspapers.", 
    "year": 1998, 
    "venue": "ACL", 
    "references": 21, 
    "authors": [
      "Yasuhiko Watanabe", 
      "Yoshihiro Okada", 
      "Kengo Kaneji", 
      "M. Nagao"
    ], 
    "topics": [
      "Microprocessor", 
      "Lift table", 
      "Microcomputer", 
      "Control system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-2031", 
    "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 11, 
    "authors": [
      "Yusu Qian", 
      "Urwa Muaz", 
      "Ben Zhang", 
      "J. Hyun"
    ], 
    "topics": [
      "Loss function", 
      "Language model", 
      "Convolutional neural network", 
      "Perplexity", 
      "Word embedding", 
      "Effective method", 
      "Downstream (software development)", 
      "List of datasets for machine learning research", 
      "Natural language generation", 
      "End-to-end principle", 
      "Sion's minimax theorem", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1235", 
    "title": "EMNLP versus ACL: Analyzing NLP research over time", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The conferences ACL (Association for Computational Linguistics) and EMNLP (Empirical Methods in Natural Language Processing) rank among the premier venues that track the research developments in Natural Language Processing and Computational Linguistics. In this paper, we present a study on the research papers of approximately two decades from these two NLP conferences. We apply keyphrase extraction and corpus analysis tools to the proceedings from these venues and propose probabilistic and vector-based representations to represent the topics published in a venue for a given year. Next, similarity metrics are studied over pairs of venue representations to capture the progress of the two venues with respect to each other and over time.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 19, 
    "authors": [
      "Sujatha Das Gollapalli", 
      "X. Li"
    ], 
    "topics": [
      "Empirical Methods in Natural Language Processing", 
      "Access control list", 
      "Venue (sound system)", 
      "Topic model", 
      "Computational linguistics", 
      "Coherence (physics)", 
      "Information extraction", 
      "Computation", 
      "Semantic similarity"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.152", 
    "title": "Vec2Sent: Probing Sentence Embeddings with Natural Language Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence. We perceive of this as a new unsupervised probing task and show that it correlates well with downstream task performance. We also illustrate how the language generated from different encoders differs. We apply our approach to generate sentence analogies from sentence embeddings.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 33, 
    "authors": [
      "M. Kerscher", 
      "Steffen Eger"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.repl4nlp-1.23", 
    "title": "Supertagging with CCG primitives", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In CCG and other highly lexicalized grammars, supertagging a sentence\u2019s words with their lexical categories is a critical step for efficient parsing. Because of the high degree of lexicalization in these grammars, the lexical categories can be very complex. Existing approaches to supervised CCG supertagging treat the categories as atomic units, even when the categories are not simple; when they encounter words with categories unseen during training, their guesses are accordingly unsophisticated. In this paper, we make use of the primitives and operators that constitute the lexical categories of categorial grammars. Instead of opaque labels, we treat lexical categories themselves as linear sequences. We present an LSTM-based model that replaces standard word-level classification with prediction of a sequence of primitives, similarly to LSTM decoders. Our model obtains state-of-the-art word accuracy for single-task English CCG supertagging, increases parser coverage and F1, and is able to produce novel categories. Analysis shows a synergistic effect between this decomposed view and incorporation of prediction history.", 
    "year": 2020, 
    "venue": "REPL4NLP", 
    "references": 43, 
    "authors": [
      "A. Bhargava", 
      "Gerald Penn"
    ], 
    "topics": [
      "Long short-term memory", 
      "Parsing", 
      "Synergy", 
      "Categorial grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/089120103322145298", 
    "title": "A Model for Matching Semantic Maps between Languages (French/English, English/French)", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article describes a spatial model for matching semantic values between two languages, French and English. Based on semantic similarity links, the model constructs a map that represents a word in the source language. Then the algorithm projects the map values onto a space in the target language. The new space abides by the semantic similarity links specific to the second language. Then the two maps are projected onto the same plane in order to detect overlapping values. For instructional purposes, the different steps are presented here using a few examples. The entire set of results is available at the following address: http://dico.isc.cnrs.fr.", 
    "year": 2003, 
    "venue": "Computational Linguistics", 
    "references": 45, 
    "authors": [
      "Sabine Ploux", 
      "Hyungsuk Ji"
    ], 
    "topics": [
      "Semantic similarity", 
      "Semantic mapper", 
      "Map", 
      "Algorithm", 
      "Compiler"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981823.981846", 
    "title": "Lazy Unification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Unification-based NL parsers that copy argument graphs to prevent their destruction suffer from inefficiency. Copying is the most expensive operation in such parsers, and several methods to reduce copying have been devised with varying degrees of success. Lazy Unification is presented here as a new, conceptually elegant solution that reduces copying by nearly an order of magnitude. Lazy Unification requires no new slots in the structure of nodes, and only nominal revisions to the unification algorithm.", 
    "year": 1990, 
    "venue": "ACL", 
    "references": 11, 
    "authors": [
      "K. Godden"
    ], 
    "topics": [
      "Parsing", 
      "Unification (computer science)", 
      "Natural language", 
      "Lazy evaluation", 
      "Han unification", 
      "Algorithm", 
      "Time complexity", 
      "Computation", 
      "NL (complexity)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5714", 
    "title": "RACAI's System at PharmaCoNER 2019", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the Named Entity Recognition system of the Institute for Artificial Intelligence \u201cMihai Dr\u0103g\u0103nescu\u201d of the Romanian Academy (RACAI for short). Our best F1 score of 0.84984 was achieved using an ensemble of two systems: a gazetteer-based baseline and a RNN-based NER system, developed specially for PharmaCoNER 2019. We will describe the individual systems and the ensemble algorithm, compare the final system to the current state of the art, as well as discuss our results with respect to the quality of the training data and its annotation strategy. The resulting NER system is language independent, provided that language-dependent resources and preprocessing tools exist, such as tokenizers and POS taggers.", 
    "year": 2019, 
    "venue": "BioNLP-OST@EMNLP-IJNCLP", 
    "references": 33, 
    "authors": [
      "Radu Ion"
    ], 
    "topics": [
      "F1 score", 
      "Named-entity recognition", 
      "Artificial intelligence", 
      "Point of sale", 
      "Academy", 
      "Algorithm", 
      "Preprocessor", 
      "Baseline (configuration management)", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-2718", 
    "title": "Towards discourse annotation and sentiment analysis of the Basque Opinion Corpus", 
    "fields_of_study": [
      "Sociology"
    ], 
    "abstract": "Discourse information is crucial for a better understanding of the text structure and it is also necessary to describe which part of an opinionated text is more relevant or to decide how a text span can change the polarity (strengthen or weaken) of other span by means of coherence relations. This work presents the first results on the annotation of the Basque Opinion Corpus using Rhetorical Structure Theory (RST). Our evaluation results and analysis show us the main avenues to improve on a future annotation process. We have also extracted the subjectivity of several rhetorical relations and the results show the effect of sentiment words in relations and the influence of each relation in the semantic orientation value.", 
    "year": 2019, 
    "venue": "", 
    "references": 20, 
    "authors": [
      "J. Alkorta", 
      "Koldo Gojenola", 
      "Mikel Iruskieta"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-2013", 
    "title": "Scaling Multi-Domain Dialogue State Tracking via Query Reformulation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel approach to dialogue state tracking and referring expression resolution tasks. Successful contextual understanding of multi-turn spoken dialogues requires resolving referring expressions across turns and tracking the entities relevant to the conversation across turns. Tracking conversational state is particularly challenging in a multi-domain scenario when there exist multiple spoken language understanding (SLU) sub-systems, and each SLU sub-system operates on its domain-specific meaning representation. While previous approaches have addressed the disparate schema issue by learning candidate transformations of the meaning representation, in this paper, we instead model the reference resolution as a dialogue context-aware user query reformulation task \u2013 the dialog state is serialized to a sequence of natural language tokens representing the conversation. We develop our model for query reformulation using a pointer-generator network and a novel multi-task learning setup. In our experiments, we show a significant improvement in absolute F1 on an internal as well as a, soon to be released, public benchmark respectively.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 24, 
    "authors": [
      "Pushpendre Rastogi", 
      "Arpit Gupta", 
      "Tongfei Chen", 
      "Lambert Mathias"
    ], 
    "topics": [
      "Multi-task learning", 
      "Natural language understanding", 
      "Application programming interface", 
      "Downstream (software development)", 
      "Entity", 
      "Benchmark (computing)", 
      "dialog", 
      "Preprocessor", 
      "Computer multitasking", 
      "Experiment", 
      "Black box", 
      "Existential quantification", 
      "End-to-end principle", 
      "Pointer (computer programming)", 
      "Rewrite (programming)", 
      "eric", 
      "Conversational state (Java EE)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.351", 
    "title": "Neural Machine Translation Models with Back-Translation for the Extremely Low-Resource Indigenous Language Bribri", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a neural machine translation model and dataset for the Chibchan language Bribri, with an average performance of BLEU 16.9\u00b11.7. This was trained on an extremely small dataset (5923 Bribri-Spanish pairs), providing evidence for the applicability of NMT in extremely low-resource environments. We discuss the challenges entailed in managing training input from languages without standard orthographies, we provide evidence of successful learning of Bribri grammar, and also examine the translations of structures that are infrequent in major Indo-European languages, such as positional verbs, ergative markers, numerical classifiers and complex demonstrative systems. In addition to this, we perform an experiment of augmenting the dataset through iterative back-translation (Sennrich et al., 2016a; Hoang et al., 2018) by using Spanish sentences to create synthetic Bribri sentences. This improves the score by an average of 1.0 BLEU, but only when the new Spanish sentences belong to the same domain as the other Spanish examples. This contributes to the small but growing body of research on Chibchan NLP.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 54, 
    "authors": [
      "Isaac Feldman", 
      "Rolando Coto-Solano"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1186", 
    "title": "Incorporating Relation Paths in Neural Relation Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from https://github.com/thunlp/PathNRE.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 36, 
    "authors": [
      "Wenyuan Zeng", 
      "Yankai Lin", 
      "Zhiyuan Liu", 
      "Maosong Sun"
    ], 
    "topics": [
      "Relationship extraction", 
      "Entity", 
      "Recurrent neural network", 
      "Graphical model", 
      "Baseline (configuration management)", 
      "Kripke semantics", 
      "Artificial neural network", 
      "Signal-to-noise ratio", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6324", 
    "title": "Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We work on translation from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a translation system that addresses these challenges using eight European language families as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6% accuracy in qualitative evaluation where our translations are akin to human translations in a preliminary study.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 67, 
    "authors": [
      "Zhong Zhou", 
      "Matthias Sperber", 
      "A. Waibel"
    ], 
    "topics": [
      "BLEU", 
      "Binding problem", 
      "Machine translation", 
      "Multi-source", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4310", 
    "title": "Hallym: Named Entity Recognition on Twitter with Word Representation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Twitter is a type of social media that contains diverse user-generated texts. Traditional models are not applicable to tweet data because the text style is not as grammaticalized as that of newswire. In this paper, we construct word embeddings via canonical correlation analysis (CCA) on a considerable amount of tweet data and show the efficacy of word representation. Besides word embedding, we use partof-speech (POS) tags, chunks, and brown clusters induced from Wikipedia as features. Here, we describe our system and present the final results along with their analysis. Our model achieves an F1 score of 37.21% with entity types and distinguishes 53.01% of the entity boundaries.", 
    "year": 2015, 
    "venue": "NUT@IJCNLP", 
    "references": 25, 
    "authors": [
      "Eun-Suk Yang", 
      "Yu-Seop Kim"
    ], 
    "topics": [
      "Named-entity recognition", 
      "F1 score", 
      "Brown Corpus", 
      "Social media", 
      "Word embedding", 
      "canonical correlation analysis", 
      "Chunking (computing)", 
      "Wikipedia", 
      "User-generated content", 
      "Categorization", 
      "Baseline (configuration management)", 
      "Name", 
      "Speech Disorders", 
      "FBN2 wt Allele"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3216", 
    "title": "GWU-HASP-2015$@$QALB-2015 Shared Task: Priming Spelling Candidates with Probability", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we describe our system HASP-2015 (Hybrid Arabic Spelling and Punctuation Corrector) in which we introduce significant improvements over our previous version HASP-2014 and with which we participated in the QALB2015 Second Shared Task on Arabic Error Correction. Our system utilizes probabilistic information on errors and their possible corrections in the training data and combine that with an open-source reference dictionary (or word list) for detecting errors and generating and filtering candidates. We enhance our system further by allowing it to generate candidates for common semantic and grammatical errors. Eventually, an n-gram language model is used for selecting best candidates. We use a CRF (Conditional Random Fields) classifier for correcting punctuation errors in a two-pass process where first the system learns punctuation placement, and then it learns to identify punctuation types.", 
    "year": 2015, 
    "venue": "ANLP@ACL", 
    "references": 26, 
    "authors": [
      "Mohammed Attia", 
      "Mohamed Al-Badrashiny", 
      "Mona T. Diab"
    ], 
    "topics": [
      "N-gram", 
      "Open-source software", 
      "Houston Automatic Spooling Priority", 
      "Language model", 
      "Conditional random field", 
      "Algorithm", 
      "Sensor", 
      "Data dictionary", 
      "gram", 
      "Dictionary [Publication Type]", 
      "spelling", 
      "Two-Hybrid System Techniques"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1263", 
    "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classificationbased methods.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 50, 
    "authors": [
      "Parminder Bhatia", 
      "Yangfeng Ji", 
      "Jacob Eisenstein"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Parsing", 
      "Recursive neural network", 
      "Artificial neural network", 
      "Lexicon", 
      "Intel Matrix RAID", 
      "Bag-of-words model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.figlang-1.18", 
    "title": "Character aware models with similarity learning for metaphor detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent work on automatic sequential metaphor detection has involved recurrent neural networks initialized with different pre-trained word embeddings and which are sometimes combined with hand engineered features. To capture lexical and orthographic information automatically, in this paper we propose to add character based word representation. Also, to contrast the difference between literal and contextual meaning, we utilize a similarity network. We explore these components via two different architectures - a BiLSTM model and a Transformer Encoder model similar to BERT to perform metaphor identification. We participate in the Second Shared Task on Metaphor Detection on both the VUA and TOFEL datasets with the above models. The experimental results demonstrate the effectiveness of our method as it outperforms all the systems which participated in the previous shared task.", 
    "year": 2020, 
    "venue": "FIGLANG", 
    "references": 39, 
    "authors": [
      "T.Suresh Kumar", 
      "Yashvardhan Sharma"
    ], 
    "topics": [
      "Similarity learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-3209", 
    "title": "Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Identifying mentions of medical concepts in social media is challenging because of high variability in free text. In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts. The collocation and aggregation layers improve the model performance on the task of identifying mentions of adverse drug events (ADE) in tweets. Using the dataset made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation.", 
    "year": 2019, 
    "venue": "", 
    "references": 27, 
    "authors": [
      "Xinyan Zhao", 
      "Deahan Yu", 
      "V.G.Vinod Vydiswaran"
    ], 
    "topics": [
      "Collocation", 
      "Social media", 
      "Network architecture", 
      "Artificial neural network", 
      "Long short-term memory", 
      "Knowledge representation and reasoning", 
      "Experiment", 
      "Sensor", 
      "Attentive user interface", 
      "Spatial variability"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981436.981448", 
    "title": "On the Independence of Discourse Structure and Semantic Domain", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Traditionally, linguistics has been concerned with units at the level of the sentence or below, but recently, a body of research has emerged which demonstrates the existence and organization of linguistic units larger than the sentence. (Chafe, 1974; Goguen, Linde, and Weiner, to appear; Grosz, 1977; Halliday and Hasan, 1976; Labov, 1972; Linde, 1974, 1979, 1980a,198Cb; Linde and Goguen, 1978; Linde and Labov, 1975; Folanyi, 1978; Weiner, 1979.) Each such study raises a question about whether the structure discovered is a property of the organization of Language or whether it is entirely a property of the semantic domain. That is, are we discovering general facts about the structure of language at a level beyond the sentence, or are we discovering particular facts about apartment layouts, water pump repair, Watergate politics, etc? Such a crude question does not arise with regard to sentences. Although much of the last twenty years of research in sentential syntax and semantics has been devoted to the investigation of the degree to which syntactic structure can be described independently of semantics, to our knowledge, no one has attempted to argue that all observable regularities of sentential structure are attributable to the structure of the real world plus general cognitive abilities. Yet this claim is often made about regularities of linguistic structure at the discourse level. In order =o demonstrate that at leas= some of the structure found at the discourse level is independent of the structure of the semantic domain, we may show that there are discourse regularities across semantic domains. As primary data, we will use apartment layout description, small group planning, and explanation. These have all been found to be discourse units, that is, bounded linguistic units one level higher than the sentential level, and have all been described within the same formal theory. It should be noted that we do not claim that the structures found in these discourse units is entirely independent of structure of the semantic domain, because of course the structure of the domain has some effect.", 
    "year": 1980, 
    "venue": "ACL", 
    "references": 16, 
    "authors": [
      "C. Linde", 
      "J. Goguen"
    ], 
    "topics": [
      "Linde\u2013Buzo\u2013Gray algorithm", 
      "Internationalized domain name", 
      "Cognition", 
      "Observable", 
      "Emoticon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4625", 
    "title": "I Couldn\u2019t Agree More: The Role of Conversational Structure in Agreement and Disagreement Detection in Online Discussions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Determining when conversational participants agree or disagree is instrumental for broader conversational analysis; it is necessary, for example, in deciding when a group has reached consensus. In this paper, we describe three main contributions. We show how different aspects of conversational structure can be used to detect agreement and disagreement in discussion forums. In particular, we exploit information about meta-thread structure and accommodation between participants. Second, we demonstrate the impact of the features using 3-way classification, including sentences expressing disagreement, agreement or neither. Finally, we show how to use a naturally occurring data set with labels derived from the sides that participants choose in debates on createdebate.com. The resulting new agreement corpus, Agreement by Create Debaters (ABCD) is 25 times larger than any prior corpus. We demonstrate that using this data enables us to outperform the same system trained on prior existing in-domain smaller annotated datasets.", 
    "year": 2015, 
    "venue": "SIGDIAL Conference", 
    "references": 35, 
    "authors": [
      "Sara Rosenthal", 
      "K. McKeown"
    ], 
    "topics": [
      "Domain adaptation", 
      "PHP", 
      "Consensus (computer science)", 
      "Java annotation", 
      "Sensor", 
      "Error analysis (mathematics)", 
      "ABCD Schema"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.SIGTYP-1.13", 
    "title": "A ResNet-50-Based Convolutional Neural Network Model for Language ID Identification from Speech Recordings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the model built for the SIGTYP 2021 Shared Task aimed at identifying 18 typologically different languages from speech recordings. Mel-frequency cepstral coefficients derived from audio files are transformed into spectrograms, which are then fed into a ResNet-50-based CNN architecture. The final model achieved validation and test accuracies of 0.73 and 0.53, respectively.", 
    "year": 2021, 
    "venue": "SIGTYP", 
    "references": 9, 
    "authors": [
      "Celano Giuseppe"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/D19-5626", 
    "title": "Learning to Generate Word- and Phrase-Embeddings for Efficient Phrase-Based Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural machine translation (NMT) often fails in one-to-many translation, e.g., in the translation of multi-word expressions, compounds, and collocations. To improve the translation of phrases, phrase-based NMT systems have been proposed; these typically combine word-based NMT with external phrase dictionaries or with phrase tables from phrase-based statistical MT systems. These solutions introduce a significant overhead of additional resources and computational costs. In this paper, we introduce a phrase-based NMT model built upon continuous-output NMT, in which the decoder generates embeddings of words or phrases. The model uses a fertility module, which guides the decoder to generate embeddings of sequences of varying lengths. We show that our model learns to translate phrases better, performing on par with state of the art phrase-based NMT. Since our model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline.", 
    "year": 2019, 
    "venue": "NGT@EMNLP-IJCNLP", 
    "references": 27, 
    "authors": [
      "Chan Young Park", 
      "Yulia Tsvetkov"
    ], 
    "topics": [
      "Neural machine translation", 
      "Collocation", 
      "Softmax function", 
      "Computation", 
      "One-to-many (data model)", 
      "Dictionary", 
      "Vocabulary", 
      "Baseline (configuration management)", 
      "Overhead (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/issn.2603-2821.2019_010", 
    "title": "Corpora and Processing Tools for Non-standard Contemporary and Diachronic Balkan Slavic", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The paper describes three corpora of different varieties of BS that are currently being developed with the goal of providing data for the analysis of the diatopic and diachronic variation in non-standard Balkan Slavic. The corpora includes spoken materials from Torlak, Macedonian dialects, as well as the manuscripts of pre-standardized Bulgarian. Apart from the texts, tools for PoS annotation and lemmatization for all varieties are being created, as well as syntactic parsing for Torlak and Bulgarian varieties. The corpora are built using a unified methodology, relying on the pest practices and state-of-the-art methods from the field. The uniform methodology allows the contrastive analysis of the data from different varieties. The corpora under construction can be considered a crucial contribution to the linguistic research on the languages in the Balkans as they provide the lacking data needed for the studies of linguistic variation in the Balkan Slavic, and enable the comparison of the said varieties with other neighbouring languages.", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 30, 
    "authors": [
      "Teodora Vukovi\u0107", 
      "Nora Muheim", 
      "Olivier Winist\u00f6rfer", 
      "Ivan \u0160imko", 
      "A. Makarova", 
      "Sanja Bradjan"
    ], 
    "topics": [
      "Text corpus", 
      "Lemmatisation", 
      "Parsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1287", 
    "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose to decompose instruction execution to goal prediction and action generation. We design a model that maps raw visual observations to goals using LINGUNET, a language-conditioned image generation network, and then generates the actions required to complete them. Our model is trained from demonstration only without external resources. To evaluate our approach, we introduce two benchmarks for instruction following: LANI, a navigation task; and CHAI, where an agent executes household instructions. Our evaluation demonstrates the advantages of our model decomposition, and illustrates the challenges posed by our new benchmarks.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 55, 
    "authors": [
      "Dipendra Misra", 
      "Andrew Bennett", 
      "Valts Blukis", 
      "Eyvind Niklasson", 
      "Max Shatkhin", 
      "Yoav Artzi"
    ], 
    "topics": [
      "Benchmark (computing)", 
      "Glossary of computer graphics", 
      "Map"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1207", 
    "title": "Differentiable Sampling with Flexible Reference Word Order for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Despite some empirical success at correcting exposure bias in machine translation, scheduled sampling algorithms suffer from a major drawback: they incorrectly assume that words in the reference translations and in sampled sequences are aligned at each time step. Our new differentiable sampling algorithm addresses this issue by optimizing the probability that the reference can be aligned with the sampled output, based on a soft alignment predicted by the model itself. As a result, the output distribution at each time step is evaluated with respect to the whole predicted sequence. Experiments on IWSLT translation tasks show that our approach improves BLEU compared to maximum likelihood and scheduled sampling baselines. In addition, our approach is simpler to train with no need for sampling schedule and yields models that achieve larger improvements with smaller beam sizes.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 32, 
    "authors": [
      "Weijia Xu", 
      "Xing Niu", 
      "Marine Carpuat"
    ], 
    "topics": [
      "Sampling (signal processing)", 
      "Neural machine translation", 
      "BLEU", 
      "Backpropagation", 
      "Greedy algorithm", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.131", 
    "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Question: I have five fingers but I am not alive. What am I? Answer: a glove. Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there is currently no dataset aiming to test these abilities. In this paper, we present RIDDLESENSE1, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddlestyle commonsense questions. We systematically evaluate a wide range of models over the RIDDLESENSE challenge, and point out that there is a large gap between the bestsupervised model and human performance \u2014 suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 54, 
    "authors": [
      "Bill Yuchen Lin", 
      "Ziyi Wu", 
      "Yichi Yang", 
      "Dong-Ho Lee", 
      "Xiang Ren"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.269", 
    "title": "Retrieval Enhanced Model for Commonsense Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Commonsense generation is a challenging task of generating a plausible sentence describing an everyday scenario using provided concepts. Its requirement of reasoning over commonsense knowledge and compositional generalization ability even puzzles strong pre-trained language generation models. We propose a novel framework using retrieval methods to enhance both the pre-training and fine-tuning for commonsense generation. We retrieve prototype sentence candidates by concept matching and use them as auxiliary input. For finetuning, we further boost its performance with a trainable sentence retriever. We demonstrate experimentally on the large-scale CommonGen benchmark that our approach achieves new state-of-the-art results.1", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 32, 
    "authors": [
      "Han Wang", 
      "Yang Liu", 
      "Chenguang Zhu", 
      "Linjun Shou", 
      "Ming Gong", 
      "Yichong Xu", 
      "Michael Zeng"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.571", 
    "title": "Identifying Exaggerated Language", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 26, 
    "authors": [
      "Li Kong", 
      "Chuanyi Li", 
      "Jidong Ge", 
      "Bin Luo", 
      "Vincent Ng"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.repl4nlp-1.18", 
    "title": "NPVec1: Word Embeddings for Nepali - Construction and Evaluation", 
    "fields_of_study": null, 
    "abstract": "Word Embedding maps words to vectors of real numbers. It is derived from a large corpus and is known to capture semantic knowledge from the corpus. Word Embedding is a critical component of many state-of-the-art Deep Learning techniques. However, generating good Word Embeddings is a special challenge for low-resource languages such as Nepali due to the unavailability of large text corpus. In this paper, we present NPVec1 which consists of 25 state-of-art Word Embeddings for Nepali that we have derived from a large corpus using Glove, Word2Vec, FastText, and BERT. We further provide intrinsic and extrinsic evaluations of these Embeddings using well established metrics and methods. These models are trained using 279 million word tokens and are the largest Embeddings ever trained for Nepali language. Furthermore, we have made these Embeddings publicly available to accelerate the development of Natural Language Processing (NLP) applications in Nepali.", 
    "year": 2021, 
    "venue": "REPL4NLP", 
    "references": 34, 
    "authors": [
      "Pravesh Koirala", 
      "Nobal B. Niraula"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-0901", 
    "title": "Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of alignments results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach.", 
    "year": 2017, 
    "venue": "LSDSem@EACL", 
    "references": 35, 
    "authors": [
      "Lilian D. A. Wanzare", 
      "Alessandra Zarcone", 
      "Stefan Thater", 
      "Manfred Pinkal"
    ], 
    "topics": [
      "Crowdsourcing", 
      "Description", 
      "Cluster analysis", 
      "Semiconductor industry", 
      "statistical cluster", 
      "Semi-supervised learning", 
      "Scalability", 
      "Muscle Rigidity", 
      "Receiver operating characteristic", 
      "receptor operated channel"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3213", 
    "title": "Multi-Reference Evaluation for Dialectal Speech Recognition System: A Study for Egyptian ASR", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Dialectal Arabic has no standard orthographic representation. This creates a challenge when evaluating an Automatic Speech Recognition (ASR) system for dialect. Since the reference transcription text can vary widely from one user to another, we propose an innovative approach for evaluating dialectal speech recognition using Multi-References. For each recognized speech segments, we ask five different users to transcribe the speech. We combine the alignment for the multiple references, and use the combined alignment to report a modified version of Word Error Rate (WER). This approach is in favor of accepting a recognized word if any of the references typed it in the same form. Our method proved to be more effective in capturing many correctly recognized words that have multiple acceptable spellings. The initial WER according to each of the five references individually ranged between 76.4% to 80.9%. When considering all references combined, the Multi-References MR-WER was found to be 53%.", 
    "year": 2015, 
    "venue": "ANLP@ACL", 
    "references": 15, 
    "authors": [
      "Ahmed M. Ali", 
      "Walid Magdy", 
      "S. Renals"
    ], 
    "topics": [
      "Speech recognition", 
      "Word error rate", 
      "Orthographic projection", 
      "Arabic chat alphabet", 
      "Transcription (software)", 
      "Speech synthesis", 
      "Automated system recovery"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2002", 
    "title": "HLT@SUDA at SemEval-2019 Task 1: UCCA Graph Parsing as Constituent Tree Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a simple UCCA semantic graph parsing approach. The key idea is to convert a UCCA semantic graph into a constituent tree, in which extra labels are deliberately designed to mark remote edges and discontinuous nodes for future recovery. In this way, we can make use of existing syntactic parsing techniques. Based on the data statistics, we recover discontinuous nodes directly according to the output labels of the constituent parser and use a biaffine classification model to recover the more complex remote edges. The classification model and the constituent parser are simultaneously trained under the multi-task learning framework. We use the multilingual BERT as extra features in the open tracks. Our system ranks the first place in the six English/German closed/open tracks among seven participating systems. For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 11, 
    "authors": [
      "Wei Jiang", 
      "Yu Zhang", 
      "Zhenghua Li", 
      "Min Zhang"
    ], 
    "topics": [
      "Parsing", 
      "SemEval", 
      "Multi-task learning", 
      "Computer multitasking", 
      "Entity\u2013relationship model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5902", 
    "title": "Improving Interaction Quality Estimation with BiLSTMs and the Impact on Dialogue Policy Learning", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Learning suitable and well-performing dialogue behaviour in statistical spoken dialogue systems has been in the focus of research for many years. While most work which is based on reinforcement learning employs an objective measure like task success for modelling the reward signal, we use a reward based on user satisfaction estimation. We propose a novel estimator and show that it outperforms all previous estimators while learning temporal dependencies implicitly. Furthermore, we apply this novel user satisfaction estimation model live in simulated experiments where the satisfaction estimation model is trained on one domain and applied in many other domains which cover a similar task. We show that applying this model results in higher estimated satisfaction, similar task success rates and a higher robustness to noise.", 
    "year": 2019, 
    "venue": "SIGdial", 
    "references": 45, 
    "authors": [
      "Stefan Ultes"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Computer user satisfaction", 
      "Robustness (computer science)", 
      "Dialog system", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Laptop"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2020.acl-main.291", 
    "title": "Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets. In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets. Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags. Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell. Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 25, 
    "authors": [
      "B. Zhang", 
      "Min Yang", 
      "Xutao Li", 
      "Yunming Ye", 
      "Xiaofei Xu", 
      "Kuai Dai"
    ], 
    "topics": [
      "Lexicon", 
      "Long short-term memory", 
      "Graph (abstract data type)", 
      "Commonsense knowledge (artificial intelligence)", 
      "Experiment", 
      "Baseline (configuration management)", 
      "YANG", 
      "Graphics Core Next", 
      "Bridging (networking)", 
      "Min-max heap"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.ACL-MAIN.48", 
    "title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 36, 
    "authors": [
      "Yi-Ju Lu", 
      "Cheng-te Li"
    ], 
    "topics": [
      "Social media", 
      "Artificial neural network", 
      "Document classification", 
      "Experiment", 
      "Semantic network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2107", 
    "title": "SentiME++ at SemEval-2017 Task 4: Stacking State-of-the-Art Classifiers to Enhance Sentiment Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we describe the participation of the SentiME++ system to the SemEval 2017 Task 4A \u201cSentiment Analysis in Twitter\u201d that aims to classify whether English tweets are of positive, neutral or negative sentiment. SentiME++ is an ensemble approach to sentiment analysis that leverages stacked generalization to automatically combine the predictions of five state-of-the-art sentiment classifiers. SentiME++ achieved officially 61.30% F1-score, ranking 12th out of 38 participants.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 16, 
    "authors": [
      "Raphael Troncy", 
      "E. Palumbo", 
      "Efstratios Sygkounas", 
      "Giuseppe Rizzo"
    ], 
    "topics": [
      "SemEval", 
      "Sentiment analysis", 
      "Bootstrapping (statistics)", 
      "Stacking", 
      "F1 score", 
      "Ensemble learning", 
      "Sampling (signal processing)", 
      "Generalization (Psychology)", 
      "Learning classifier system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-0525", 
    "title": "Combined Tree Kernel-based classifiers for Assessing Quality of Scientific Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This document describes Tree Kernel-SVM based methods for identifying sentences that could be improved in scientific text. This has the goal of contributing to the body of knowledge that attempt to build assistive tools to aid scientist improve the quality of their writings. Our methods consist of a combination of the output from multiple support vector machines which use Tree Kernel computations. Therefore, features for individual sentences are trees that reflect their grammatical structure. For the AESW 2016 Shared Task we built systems that provide probabilistic and binary outputs by using these models for trees comparisons.", 
    "year": 2016, 
    "venue": "BEA@NAACL-HLT", 
    "references": 15, 
    "authors": [
      "Liliana Mamani S\u00e1nchez", 
      "Hector-Hugo Franco-Penya"
    ], 
    "topics": [
      "Computation", 
      "Support vector machine", 
      "Entropy (information theory)", 
      "Machine learning", 
      "Scalability", 
      "Time complexity", 
      "Requirement", 
      "Assistive technology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w15-31", 
    "title": "Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing, SIGHAN@IJCNLP 2015, Beijing, China, July 30-31, 2015", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2015, 
    "venue": "SIGHAN@IJCNLP", 
    "references": 0, 
    "authors": [
      "L. Yu", 
      "Zhifang Sui", 
      "Y. Zhang", 
      "Vincent Ng"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119033", 
    "title": "SRA Participation in TIPSTER Phase II", 
    "fields_of_study": [
      "Engineering", 
      "Computer Science"
    ], 
    "abstract": "SRA, although not a research contractor under Tipster Phase II, nonetheless actively participated in the program in a variety of ways. Some of the activities include:\u2022 Preparation and distribution of the Named Entity Tagging and Discourse Tagging Tools for the Sixth Message Understanding Conference (MUC6) and Multilingual Entity Task (MET)\u2022 Implementation / Award of two Tipster application prototype systems: Bluesky and USACOM\u2022 Sole contractor on two Tipster-affiliated systems, the Intelligence Analysts Associate (IAA) under funding from Rome Laboratories for NAIC, and the Overture program for OIR.\u2022 Participation in the Contractor Architecture Working Group (CAWG)\u2022 Participation in the Common Pattern Specification Language subgroup", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 0, 
    "authors": [
      "Lisa Rau"
    ], 
    "topics": [
      "Sequence Read Archive"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974358.974411", 
    "title": "Guided Sentences Composition for Disabled People", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the advantages of guided sentences composition for communicating in natural language with computers. We show how guidance can be achieved by means of the partial synthesis of sentences and describe our approach consisting in separating knowledge that comes under different levels of well-formedness and coroutining their treatment. This approach has led us to develop in Prolog a software, ILLICO, allowing for the conception of natural language interfaces with guided sentences composition. We present an application we have developed by means of ILLICO : KOMBE, a communication aid system for handicapped persons.", 
    "year": 1994, 
    "venue": "ANLP", 
    "references": 8, 
    "authors": [
      "R. Pasero", 
      "Nathalie Richardet", 
      "P. Sabatier"
    ], 
    "topics": [
      "Natural language", 
      "Prolog", 
      "Context-free grammar", 
      "Programming language", 
      "Context-free language", 
      "Formal language", 
      "Database", 
      "Lexicon", 
      "Decision problem", 
      "Computer", 
      "Formal grammar", 
      "Dead code elimination"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1571", 
    "title": "Modeling Semantic Compositionality with Sememe Knowledge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 42, 
    "authors": [
      "Fanchao Qi", 
      "Junjie Huang", 
      "Chenghao Yang", 
      "Zhiyuan Liu", 
      "Xiao Chen", 
      "Qun Liu", 
      "Maosong Sun"
    ], 
    "topics": [
      "Knowledge base", 
      "Experiment", 
      "Numerical analysis", 
      "Baseline (configuration management)", 
      "HTTPS", 
      "Expression (computer science)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1107", 
    "title": "Prefix Lexicalization of Synchronous CFGs using Synchronous TAG", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar\u2019s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Logan Born", 
      "Anoop Sarkar"
    ], 
    "topics": [
      "Greibach normal form", 
      "Natural language processing", 
      "Machine translation", 
      "Tree-adjoining grammar", 
      "OLAP cube", 
      "Synchronous context-free grammar", 
      "Context-free language", 
      "Parsing", 
      "Formal grammar", 
      "Binary prefix", 
      "PL/I", 
      "Stochastic context-free grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.647", 
    "title": "MLSUM: The Multilingual Summarization Corpus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 78, 
    "authors": [
      "Thomas Scialom", 
      "Paul-Alexis Dray", 
      "Sylvain Lamprier", 
      "Benjamin Piwowarski", 
      "Jacopo Staiano"
    ], 
    "topics": [
      "Performance", 
      "Experiment", 
      "Automatic summarization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-5005", 
    "title": "Language Learning and Processing in People and Machines", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The goal of this tutorial is to bring the fields of computational linguistics and computational cognitive science closer: we will introduce different stages of language acquisition and their parallel problems in NLP. As an example, one of the early challenges children face is mapping the meaning of word labels (such as \u201ccat\u201d) to their referents (the furry animal in the living room). Word learning is similar to the word alignment problem in machine translation. We explain the current computational models of language acquisition, their limitations, and how the insights from these models can be incorporated into NLP applications. Moreover, we discuss how we can take advantage of the cognitive science of language in computational linguistics: for example, by designing cognitively-motivated evaluations task or buildings language-learning inductive biases into our models.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 2, 
    "authors": [
      "Aida Nematzadeh", 
      "Richard Futrell", 
      "R. Levy"
    ], 
    "topics": [
      "Natural language processing", 
      "Computational linguistics", 
      "Semantic role labeling", 
      "Reverse engineering", 
      "Computer multitasking", 
      "Computation", 
      "Word lists by frequency", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1379", 
    "title": "Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 42, 
    "authors": [
      "Renfen Hu", 
      "Shen Li", 
      "Shichen Liang"
    ], 
    "topics": [
      "Word embedding", 
      "Sensor", 
      "Word sense", 
      "Dictionary", 
      "Experiment", 
      "Angular defect"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-2089", 
    "title": "Sentence Embedding for Neural Machine Translation Domain Adaptation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT\u2019s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 27, 
    "authors": [
      "Rui Wang", 
      "A. Finch", 
      "M. Utiyama", 
      "E. Sumita"
    ], 
    "topics": [
      "Domain adaptation", 
      "Neural machine translation", 
      "BLEU", 
      "Performance", 
      "Baseline (configuration management)", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980691.980791", 
    "title": "Using Leading Text for News Summaries: Evaluation Results and Implications for Commercial Summarization Applications", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Leading text extracts created to support some online Boolean retrieval goals are evaluated for their acceptability as news document summaries. Results are presented and discussed from the perspective of commercial summarization technology needs.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 9, 
    "authors": [
      "Mark Wasson"
    ], 
    "topics": [
      "Standard Boolean model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.580", 
    "title": "Generating Fact Checking Briefs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Fact checking at scale is difficult -- while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset which we collected via crowdsourcing. We show that fact checking with briefs -- in particular QABriefs -- increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "Angela Fan", 
      "Aleksandra Piktus", 
      "Fabio Petroni", 
      "Guillaume Wenzek", 
      "Marzieh Saeidi", 
      "Andreas Vlachos", 
      "Antoine Bordes", 
      "Sebastian Riedel"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974147.974179", 
    "title": "Language Independent Morphological Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes a framework of language independent morphological analysis and mainly concentrate on tokenization, the first process of morphological analysis. Although tokenization is usually not regarded as a difficult task in most segmented languages such as English, there are a number of problems in achieving precise treatment of lexical entries. We first introduce the concept of morpho-fragments, which are intermediate units between characters and lexical entries. We describe our approach to resolve problems arising in tokenization so as to attain a language independent morphological analyzer.", 
    "year": 2000, 
    "venue": "ANLP", 
    "references": 32, 
    "authors": [
      "Tatsuo Yamashita", 
      "Yuji Matsumoto"
    ], 
    "topics": [
      "Tokenization (data security)", 
      "Alloy Analyzer"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-6610", 
    "title": "Improving Evidence Detection by Leveraging Warrants", 
    "fields_of_study": null, 
    "abstract": "Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired warrants. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our method can still improve the performance of evidence detection.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 19, 
    "authors": [
      "Keshav Singh", 
      "Paul Reisert", 
      "Naoya Inoue", 
      "Pride Kavumba", 
      "Kentaro Inui"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.517", 
    "title": "Cross-Lingual Emotion Lexicon Induction using Representation Alignment in Low-Resource Settings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Emotion lexicons provide information about associations between words and emotions. They have proven useful in analyses of reviews, literary texts, and posts on social media, among other things. We evaluate the feasibility of deriving emotion lexicons cross-lingually, especially for low-resource languages, from existing emotion lexicons in resource-rich languages. For this, we start out from very small corpora to induce cross-lingually aligned vector spaces. Our study empirically analyses the effectiveness of the induced emotion lexicons by measuring translation precision and correlations with existing emotion lexicons, along with measurements on a downstream task of sentence emotion prediction.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 31, 
    "authors": [
      "Arun Ramachandran", 
      "Gerard de Melo"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.28", 
    "title": "Enhance Robustness of Sequence Labelling with Masked Adversarial Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification. However, it lacks sufficient contextual information enhancement and thus is less useful for sequence labelling tasks such as chunking and named entity recognition (NER). To address this limitation, we propose masked adversarial training (MAT) to improve robustness from contextual information in sequence labelling. MAT masks or replaces some words in the sentence when computing adversarial loss from perturbed inputs and consequently enhances model robustness using more context-level information. In our experiments, our method shows significant improvements on accuracy and robustness of sequence labelling. By further incorporating with ELMo embeddings, our model achieves better or comparable results to state-of-the-art on CoNLL 2000 and 2003 benchmarks using much less parameters.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 18, 
    "authors": [
      "Luoxin Chen", 
      "Xinyue Liu", 
      "Weitong Ruan", 
      "J. Lu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00263", 
    "title": "RELPRON: A Relative Clause Evaluation Data Set for Compositional Distributional Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article introduces RELPRON, a large data set of subject and object relative clauses, for the evaluation of methods in compositional distributional semantics. RELPRON targets an intermediate level of grammatical complexity between content-word pairs and full sentences. The task involves matching terms, such as \u201cwisdom,\u201d with representative properties, such as \u201cquality that experience teaches.\u201d A unique feature of RELPRON is that it is built from attested properties, but without the need for them to appear in relative clause format in the source corpus. The article also presents some initial experiments on RELPRON, using a variety of composition methods including simple baselines, arithmetic operators on vectors, and finally, more complex methods in which argument-taking words are represented as tensors. The latter methods are based on the Categorial framework, which is described in detail. The results show that vector addition is difficult to beat\u2014in line with the existing literature\u2014but that an implementation of the Categorial framework based on the Practical Lexical Function model is able to match the performance of vector addition. The article finishes with an in-depth analysis of RELPRON, showing how results vary across subject and object relative clauses, across different head nouns, and how the methods perform on the subtasks necessary for capturing relative clause semantics, as well as providing a qualitative analysis highlighting some of the more common errors. Our hope is that the competitive results presented here, in which the best systems are on average ranking one out of every two properties correctly for a given term, will inspire new approaches to the RELPRON ranking task and other tasks based on linguistically interesting constructions.", 
    "year": 2016, 
    "venue": "CL", 
    "references": 97, 
    "authors": [
      "Laura Rimell", 
      "Jean Maillard", 
      "T. Polajnar", 
      "S. Clark"
    ], 
    "topics": [
      "Distributional semantics", 
      "Function model", 
      "Lexical function", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.583", 
    "title": "Is Graph Structure Necessary for Multi-hop Question Answering?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for textual multi-hop reasoning. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 20, 
    "authors": [
      "Nan Shao", 
      "Yiming Cui", 
      "Ting Liu", 
      "Shijin Wang", 
      "Guoping Hu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981311.981335", 
    "title": "Syntactic Constraints and Efficient Parsability", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A central goal of linguistic theory is to explain why natural languages are the way they are. It has often been supposed that computational considerations ought to play a role in this characterization, but rigorous arguments along these lines have been difficult to come by. In this paper we show how a key \"axiom\" of certain theories of grammar, Subjacency, can be explained by appealing to general restrictions on on-line parsing plus natural constraints on the rule-writing vocabulary of grammars. The explanation avoids the problems with Marcus' [1980] attempt to account for the same constraint. The argument is robust with respect to machine implementation, and thus avoids the problems that often arise when making detailed claims about parsing efficiency. It has the added virtue of unifying in the functional domain of parsing certain grammatically disparate phenomena, as well as making a strong claim about the way in which the grammar is actually embedded into an on-line sentence processor.", 
    "year": 1983, 
    "venue": "ACL", 
    "references": 13, 
    "authors": [
      "R. Berwick", 
      "A. Weinberg"
    ], 
    "topics": [
      "Parsing", 
      "Natural language", 
      "Online and offline", 
      "Vocabulary", 
      "Embedded system", 
      "Theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K16-1007", 
    "title": "Learning to Jointly Predict Ellipsis and Comparison Structures", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Domain-independent meaning representation of text has received a renewed interest in the NLP community. Comparison plays a crucial role in shaping objective and subjective opinion and measurement in natural language, and is often expressed in complex constructions including ellipsis. In this paper, we introduce a novel framework for jointly capturing the semantic structure of comparison and ellipsis constructions. Our framework models ellipsis and comparison as interconnected predicate-argument structures, which enables automatic ellipsis resolution. We show that a structured prediction model trained on our dataset of 2,800 gold annotated review sentences yields promising results. Together with this paper we release the dataset and an annotation tool which enables two-stage expert annotation on top of tree structures.", 
    "year": 2016, 
    "venue": "CoNLL", 
    "references": 35, 
    "authors": [
      "Omid Bakhshandeh", 
      "Alexis Wellwood", 
      "James Allen"
    ], 
    "topics": [
      "Structured prediction", 
      "Comparison sort", 
      "Noise shaping", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-0539", 
    "title": "Deep Learning Architecture for Complex Word Identification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe a system for the CWI-task that includes information on 5 aspects of the (complex) lexical item, namely distributional information of the item itself, morphological structure, psychological measures, corpus-counts and topical information. We constructed a deep learning architecture that combines those features and apply it to the probabilistic and binary classification task for all English sets and Spanish. We achieved reasonable performance on all sets with best performances seen on the probabilistic task, particularly on the English news set (MAE 0.054 and F1-score of 0.872). An analysis of the results shows that reasonable performance can be achieved with a single architecture without any domain-specific tweaking of the parameter settings and that distributional features capture almost all of the information also found in hand-crafted features.", 
    "year": 2018, 
    "venue": "BEA@NAACL-HLT", 
    "references": 26, 
    "authors": [
      "Dirk De Hertog", 
      "Ana\u00efs Tack"
    ], 
    "topics": [
      "Deep learning", 
      "Binary classification", 
      "F1 score", 
      "Performance", 
      "Tweaking"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119058", 
    "title": "Integration of Document Detection and Information Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We have conducted a number of experiments to evaluate various modes of building an integrated detection/extraction system. The experiments were performed using SMART system as baseline. The goal was to determine if advanced information extraction methods can improve recall and precision of document detection. We identified the following two modes of integration:I. Extraction to Detection: broad-coverage extraction1. Extraction step: identify concepts for indexing2. Detection step 1: low recall, high initial precision3. Detection step 2: automatic relevance feedback using top N retrieved documents to regain recall.II. Detection to Extraction: query-specific extraction1. Detection step 1: high recall, low precision run2. Extraction step: learn concept(s) from query and retrieved subcollection3. Detection step 2: re-rank the subcollection to increase precisionOur integration effort concentrated on mode I, and the following issues:1. use of shallow but fast NLP for phrase extractions and disambiguation in place of a full syntactic parser2. use existing MUC-6 extraction capabilities to index a retrieval collection3. mixed Boolean/soft match retrieval model4. create a Universal Spotter algorithm for learning arbitrary concepts", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 5, 
    "authors": [
      "Louise Guthrie", 
      "T. Strzalkowski", 
      "Jin Wang", 
      "Fang Lin"
    ], 
    "topics": [
      "Information extraction", 
      "Relevance feedback", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Pattern language", 
      "Information retrieval", 
      "Deterministic finite automaton", 
      "Word-sense disambiguation", 
      "Pattern matching", 
      "Precision and recall", 
      "Nondeterministic finite automaton", 
      "Algorithm", 
      "Finite-state machine", 
      "Compiler", 
      "Lexico", 
      "Know-how trading", 
      "Natural language processing", 
      "Automata theory", 
      "Parsing", 
      "Smart system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4739", 
    "title": "The University of Edinburgh\u2019s Neural MT Systems for WMT17", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques.", 
    "year": 2017, 
    "venue": "WMT", 
    "references": 21, 
    "authors": [
      "Rico Sennrich", 
      "Alexandra Birch", 
      "Anna Currey", 
      "Ulrich Germann", 
      "B. Haddow", 
      "Kenneth Heafield", 
      "Antonio Valerio Miceli Barone", 
      "P. Williams"
    ], 
    "topics": [
      "Neural machine translation", 
      "Experiment", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4729", 
    "title": "University of Rochester WMT 2017 NMT System Submission", 
    "fields_of_study": [
      "Computer Science", 
      "Engineering"
    ], 
    "abstract": "We describe the neural machine translation system submitted by the University of Rochester to the Chinese-English language pair for the WMT 2017 news translation task. We applied unsupervised word and subword segmentation techniques and deep learning in order to address (i) the word segmentation problem caused by the lack of delimiters between words and phrases in Chinese and (ii) the morphological and syntactic differences between Chinese and English. We integrated promising recent developments in NMT, including back-translations, language model reranking, subword splitting and minimum risk tuning.", 
    "year": 2017, 
    "venue": "WMT", 
    "references": 19, 
    "authors": [
      "Chester Holtz", 
      "Chuyang Ke", 
      "D. Gildea"
    ], 
    "topics": [
      "Neural machine translation", 
      "Language model", 
      "Deep learning", 
      "Substring", 
      "MT Framework", 
      "Text segmentation", 
      "Computer multitasking", 
      "Tokenization (data security)", 
      "Delimiter", 
      "Unsupervised learning"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W15-51", 
    "title": "Proceedings of the 6th Workshop on Speech and Language Processing for Assistive Technologies, SLPAT@Interspeech 2015, Dresden, Germany, September 11, 2015", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2015, 
    "venue": "SLPAT@Interspeech", 
    "references": 0, 
    "authors": [], 
    "topics": [
      "Assistive technology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.484", 
    "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 35, 
    "authors": [
      "Yaobo Liang", 
      "Nan Duan", 
      "Yeyun Gong", 
      "Ning Wu", 
      "Fenfei Guo", 
      "Weizhen Qi", 
      "Ming Gong", 
      "Linjun Shou", 
      "Daxin Jiang", 
      "Guihong Cao", 
      "Xiaodong Fan", 
      "Bruce Zhang", 
      "Rahul Agrawal", 
      "Edward Cui", 
      "Sining Wei", 
      "Taroon Bharti", 
      "Ying Qiao", 
      "Jiun-Hung Chen", 
      "Winnie Wu", 
      "Shuguang Liu", 
      "Fan Yang", 
      "Daniel Fernando Campos", 
      "Rangan Majumder", 
      "M. Zhou"
    ], 
    "topics": [
      "Natural language understanding", 
      "Benchmark (computing)", 
      "Text corpus", 
      "Natural language processing", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00036", 
    "title": "Neural Lattice Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions \u2014 including polysemy and the existence of multiword lexical items \u2014 into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.", 
    "year": 2018, 
    "venue": "TACL", 
    "references": 42, 
    "authors": [
      "J. Buckman", 
      "Graham Neubig"
    ], 
    "topics": [
      "Language model", 
      "Perplexity", 
      "Baseline (configuration management)", 
      "Programming paradigm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1144", 
    "title": "AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distant supervision has been widely used in current systems of fine-grained entity typing to automatically assign categories (entity types) to entity mentions. However, the types so obtained from knowledge bases are often incorrect for the entity mention\u2019s local context. This paper proposes a novel embedding method to separately model \u201cclean\u201d and \u201cnoisy\u201d mentions, and incorporates the given type hierarchy to induce loss functions. We formulate a joint optimization problem to learn embeddings for mentions and typepaths, and develop an iterative algorithm to solve the problem. Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement in accuracy over the next best compared method1.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 37, 
    "authors": [
      "Xiang Ren", 
      "W. He", 
      "Meng Qu", 
      "Lifu Huang", 
      "Heng Ji", 
      "Jiawei Han"
    ], 
    "topics": [
      "Class hierarchy", 
      "Loss function", 
      "Typing", 
      "Relationship extraction", 
      "Algorithm", 
      "Optimization problem", 
      "Iterative method", 
      "Mathematical optimization", 
      "Knowledge base", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-1606", 
    "title": "Semantic Annotation of Japanese Functional Expressions and its Impact on Factuality Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recognizing the meaning of functional expressions is essential for natural language understanding. This is a difficult task, owing to the lack of a sufficient corpus for machine learning and evaluation. In this study, we design a new annotation scheme and construct a corpus containing 2,327 Japanese sentences and 8,775 functional expressions. Our scheme achieves high inter-annotator agreement with kappa score of 0.85. In the experiments, we confirmed that machine learning-based functional expression analysis contributes to factuality analysis.", 
    "year": 2015, 
    "venue": "LAW@NAACL-HLT", 
    "references": 19, 
    "authors": [
      "Yudai Kamioka", 
      "Kazuya Narita", 
      "Junta Mizuno", 
      "M. Kanno", 
      "Kentaro Inui"
    ], 
    "topics": [
      "Text corpus", 
      "Machine learning", 
      "Natural language understanding", 
      "Inter-rater reliability", 
      "Shin Megami Tensei: Persona 3", 
      "Experiment", 
      "Requirements analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-0611", 
    "title": "Interpreting Questions with a Log-Linear Ranking Model in a Virtual Patient Dialogue System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a log-linear ranking model for interpreting questions in a virtual patient dialogue system and demonstrate that it substantially outperforms a more typical multiclass classifier model using the same information. The full model makes use of weighted and concept-based matching features that together yield a 15% error reduction over a strong lexical overlap baseline. The accuracy of the ranking model approaches that of an extensively handcrafted pattern matching system, promising to reduce the authoring burden and make it possible to use confidence estimation in choosing dialogue acts; at the same time, the effectiveness of the concept-based features indicates that manual development resources can be productively employed with the approach in developing concept hierarchies.", 
    "year": 2015, 
    "venue": "BEA@NAACL-HLT", 
    "references": 25, 
    "authors": [
      "Evan Jaffe", 
      "Michael White", 
      "William Schuler", 
      "E. Fosler-Lussier", 
      "Alex Rosenfeld", 
      "Douglas R. Danforth"
    ], 
    "topics": [
      "Dialog system", 
      "Pattern matching", 
      "Multiclass classification", 
      "Anaphora (linguistics)", 
      "Speech recognition", 
      "WordNet", 
      "Log-linear model", 
      "Baseline (configuration management)", 
      "Meteor", 
      "Synonym ring", 
      "Cognitive dimensions of notations", 
      "Error analysis (mathematics)", 
      "Dialog tree", 
      "Virtual Storage Platform"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1136", 
    "title": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 46, 
    "authors": [
      "Andrea Madotto", 
      "Chien-Sheng Wu", 
      "Pascale Fung"
    ], 
    "topics": [
      "Dialog system", 
      "Knowledge base", 
      "Generative model", 
      "Pointer (computer programming)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981251.981278", 
    "title": "Themes from 1972", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although 1972 was the year that Winograd published his now classic natural language Study of the blocks world, that fact had not yet penetrated to the ACL. At that time people with AI computational interests were strictly in a minority in the association and it was a radical move to appoint Roger Schank as program chairman for the year's meeting. That was also the year that we didn't have a presidential banquet, and my \"speech\" was a few informal remarks at the roadhouse restaurant somewhere in North Carolina reassuring a doubtful few members that computational understanding of natural language was certainly progressing and that applied natural language systems were distinctly feasible.", 
    "year": 1982, 
    "venue": "ACL", 
    "references": 3, 
    "authors": [
      "R. F. Simmons"
    ], 
    "topics": [
      "Natural language", 
      "Blocks world", 
      "Theme (computing)", 
      "Coppersmith\u2013Winograd algorithm", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1100", 
    "title": "Correlation Coefficients and Semantic Textual Similarity", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "A large body of research into semantic textual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks. By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases. In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use. We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure. Importantly, we show that Pearson correlation is appropriate for some word vectors but not others. When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance. We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks. On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 56, 
    "authors": [
      "V. Zhelezniak", 
      "Aleksandar Savkov", 
      "April Shen", 
      "N. Hammerla"
    ], 
    "topics": [
      "Cosine similarity", 
      "Coefficient", 
      "Word embedding", 
      "Semantic similarity", 
      "Benchmark (computing)", 
      "Similarity measure"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-0207", 
    "title": "Bilingual Chronological Classification of Hafez\u2019s Poems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel task: the chronological classification of Hafez\u2019s poems (ghazals). We compiled a bilingual corpus in digital form, with consistent idiosyncratic properties. We have used Hooman\u2019s labeled ghazals in order to train automatic classifiers to classify the remaining ghazals. Our classification framework uses a Support Vector Machine (SVM) classifier with similarity features based on Latent Dirichlet Allocation (LDA). In our analysis of the results we use the LDA topics\u2019 main terms that are passed on to a Principal Component Analysis (PCA) module.", 
    "year": 2016, 
    "venue": "CLfL@NAACL-HLT", 
    "references": 32, 
    "authors": [
      "A. Rahgozar", 
      "D. Inkpen"
    ], 
    "topics": [
      "Latent Dirichlet allocation", 
      "Support vector machine", 
      "Principal component analysis", 
      "Word embedding", 
      "Text corpus", 
      "Test set", 
      "Compiler", 
      "Tracing (software)", 
      "Book", 
      "Statistical classification", 
      "Apex (geometry)", 
      "Microsoft Word for Mac"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1220175.1220295", 
    "title": "Accurate Collocation Extraction Using a Multilingual Parser", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction. A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English, French, Spanish and Italian corpora. The advantage of relying on full parsing over using a traditional window method (which ignores the syntactic information) is first theoretically motivated, then empirically validated by a comparative evaluation experiment.", 
    "year": 2006, 
    "venue": "ACL", 
    "references": 34, 
    "authors": [
      "Violeta Seretan", 
      "E. Wehrli"
    ], 
    "topics": [
      "Collocation extraction", 
      "Parser", 
      "Hybrid system", 
      "Sensor", 
      "Text corpus", 
      "Shallow parsing", 
      "Window function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00330", 
    "title": "Unsupervised Quality Estimation for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Abstract Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation, and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By utilizing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivaling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE.", 
    "year": 2020, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 139, 
    "authors": [
      "M. Fomicheva", 
      "Shuo Sun", 
      "Lisa Yankovskaya", 
      "F. Blain", 
      "Francisco Guzm\u00e1n", 
      "M. Fishel", 
      "Nikolaos Aletras", 
      "Vishrav Chaudhary", 
      "Lucia Specia"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Black box", 
      "Uncertainty quantification", 
      "Neural machine translation", 
      "Computation", 
      "Query expansion"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.273", 
    "title": "MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to \"carryover\" the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\\% training data, and 3) Lev greatly improves the inference efficiency.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 58, 
    "authors": [
      "Zhaojiang Lin", 
      "Andrea Madotto", 
      "Genta Indra Winata", 
      "Pascale Fung"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.sdp-1.24", 
    "title": "Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.", 
    "year": 2020, 
    "venue": "SDP", 
    "references": 45, 
    "authors": [
      "Muthu Kumar Chandrasekaran", 
      "Guy Feigenblat", 
      "E. Hovy", 
      "Abhilasha Ravichander", 
      "Michal Shmueli-Scheuer", 
      "Anita de Waard"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2040", 
    "title": "Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents. Topic words are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not. We propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (Blei et al., 2003) to these documents. For the results of LDA, we identified topic words by using Moving Average Convergence Divergence. In order to evaluate the method, we applied the results of topic detection to extractive multi-document summarization. The results showed that the method was effective for sentence selection in summarization.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 35, 
    "authors": [
      "Yoshimi Suzuki", 
      "Fumiyo Fukumoto"
    ], 
    "topics": [
      "Automatic summarization", 
      "Latent Dirichlet allocation", 
      "Multi-document summarization", 
      "Sensor", 
      "Local-density approximation", 
      "Series and parallel circuits", 
      "Conditional random field"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119082", 
    "title": "CRL's Approach to MET", 
    "fields_of_study": [
      "Engineering", 
      "Computer Science"
    ], 
    "abstract": "From February to April CRL carried out investigations into the modification of our English name recognition software developed for MUC-6 [1] to Chinese and Spanish. In addition a Japanese system, developed under Tipster Phase I [2], was modified to comply with the MET task. Finally learning methods developed for MUC-6 were adapted to handle Chinese. All systems performed with good levels of accuracy and it is clear that further tuning and refinement, for which there was no time or resources, would lead to even higher levels of performance.", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 2, 
    "authors": [
      "J. Cowie"
    ], 
    "topics": [
      "Refinement (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.iwslt-1.27", 
    "title": "Re-translation versus Streaming for Simultaneous Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation\u2019s ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.", 
    "year": 2020, 
    "venue": "IWSLT", 
    "references": 23, 
    "authors": [
      "N. Arivazhagan", 
      "Colin Cherry", 
      "Wolfgang Macherey", 
      "George F. Foster"
    ], 
    "topics": [
      "Machine translation", 
      "Baseline (configuration management)", 
      "Computational complexity theory", 
      "Programming paradigm", 
      "Logo", 
      "RTX, RTX64"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1124", 
    "title": "A Neural Transition-based Model for Nested Mention Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "It is common that entity mentions can contain other mentions recursively. This paper introduces a scalable transition-based method to model the nested structure of mentions. We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest. Our shift-reduce based system then learns to construct the forest structure in a bottom-up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length. Based on Stack-LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letter-level patterns. Our model gets the state-of-the-art performances in ACE datasets, showing its effectiveness in detecting nested mentions.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 40, 
    "authors": [
      "Bailin Wang", 
      "Wei Lu", 
      "Yu Wang", 
      "Hongxia Jin"
    ], 
    "topics": [
      "Recursion", 
      "The Forest", 
      "Text-based (computing)", 
      "Sensor", 
      "Scalability", 
      "Maximal set", 
      "Long short-term memory", 
      "Bottom-up parsing", 
      "ACE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1124", 
    "title": "Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Traditional generative dialogue models generate responses solely from input queries. Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways. Recently, researchers have attempted to fill the information gap by exploiting information retrieval techniques. For a given query, similar dialogues are retrieved from the entire training data and considered as an additional knowledge source. While the use of retrieval may harvest extensive information, the generative models could be overwhelmed, leading to unsatisfactory performance. In this paper, we propose a new framework which exploits retrieval results via a skeleton-to-response paradigm. At first, a skeleton is extracted from the retrieved dialogues. Then, both the generated skeleton and the original query are used for response generation via a novel response generator. Experimental results show that our approach significantly improves the informativeness of the generated responses", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 36, 
    "authors": [
      "Deng Cai", 
      "Yan Wang", 
      "Victoria Bi", 
      "Zhaopeng Tu", 
      "Xiaojiang Liu", 
      "Wai Lam", 
      "Shuming Shi"
    ], 
    "topics": [
      "Generative model", 
      "Information retrieval", 
      "Programming paradigm", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5440", 
    "title": "Probing sentence embeddings for structure-dependent tense", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Learning universal sentence representations which accurately model sentential semantic content is a current goal of natural language processing research. A prominent and successful approach is to train recurrent neural networks (RNNs) to encode sentences into fixed length vectors. Many core linguistic phenomena that one would like to model in universal sentence representations depend on syntactic structure. Despite the fact that RNNs do not have explicit syntactic structural representations, there is some evidence that RNNs can approximate such structure-dependent phenomena under certain conditions, in addition to their widespread success in practical tasks. In this work, we assess RNNs\u2019 ability to learn the structure-dependent phenomenon of main clause tense.", 
    "year": 2018, 
    "venue": "BlackboxNLP@EMNLP", 
    "references": 21, 
    "authors": [
      "Geoff Bacon", 
      "T. Regier"
    ], 
    "topics": [
      "Tension", 
      "Recurrent neural network", 
      "Natural language processing", 
      "Baseline (configuration management)", 
      "Test set", 
      "sentence", 
      "Feeling tense", 
      "Long short-term memory", 
      "Preprocessor", 
      "Word embedding", 
      "Computation", 
      "Experiment", 
      "Approximation algorithm", 
      "Linguistics", 
      "Alternating direction implicit method", 
      "Categories", 
      "Encoder Device Component", 
      "Random neural network", 
      "Consistency model", 
      "Word lists by frequency", 
      "Artificial neural network", 
      "ENCODE", 
      "Heuristics", 
      "Neural Network Simulation", 
      "Won", 
      "Architecture as Topic", 
      "Decoder Device Component", 
      "Data Collection"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.152", 
    "title": "Fusing Label Embedding into BERT: An Efficient Improvement for Text Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With pre-trained models, such as BERT, gaining more and more attention, plenty of research has been done to further promote their capabilities, from enhancing the experimental procedures (Sun et al., 2019) to improving the mathematical principles. In this paper, we propose a concise method for improving BERT\u2019s performance in text classification by utilizing a label embedding technique while keeping almost the same computational cost. Experimental results on six text classification benchmark datasets demonstrate its effectiveness.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 48, 
    "authors": [
      "Yijin Xiong", 
      "Yukun Feng", 
      "Hao Wu", 
      "Hidetaka Kamigaito", 
      "M. Okumura"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1103", 
    "title": "Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Xiangyang Zhou", 
      "Lu Li", 
      "Daxiang Dong", 
      "Y. Liu", 
      "Ying Chen", 
      "Wayne Xin Zhao", 
      "Dianhai Yu", 
      "Hua Wu"
    ], 
    "topics": [
      "Software agent", 
      "Machine translation", 
      "Functional dependency", 
      "Transformer", 
      "Artificial neural network", 
      "Memory segmentation", 
      "Experiment", 
      "Impedance matching"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075096.1075112", 
    "title": "Synonymous Collocation Extraction Using Translation Information", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatically acquiring synonymous collocation pairs such as and from corpora is a challenging task. For this task, we can, in general, have a large monolingual corpus and/or a very limited bilingual corpus. Methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low precision or low coverage. In this paper, we propose a method that uses both these resources to get an optimal compromise of precision and coverage. This method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus, and then selects the appropriate pairs from the candidates using their translations in a second language. The translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus. The translation information is proved as effective to select synonymous collocation pairs. Experimental results indicate that the average precision and recall of our approach are 74% and 64% respectively, which outperform those methods that only use monolingual corpora and those that only use bilingual corpora.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "Hua Wu", 
      "M. Zhou"
    ], 
    "topics": [
      "Precision and recall", 
      "Text corpus", 
      "Collocation extraction", 
      "Information retrieval", 
      "Statistical machine translation", 
      "Synonym ring", 
      "Thesaurus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981436.981445", 
    "title": "Should Computers Write Spoken Language?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently there has developed a great deal of interest in the differences between written and spoken language. I joined this trend a little more than a year ago, and have been exploring not only what the specific differences are, but also the reasons why they might exist. The approach I have taken has been to look for differences between the situations and processes involved in speaking on the one hand and writing on the other, and to speculate on how those differences might be responsible for the observable differences in the output, ~at happens when we write and what happens when we speak are different things, both psychologically and socially, and I have been trying to see how what we do in the two situations leads to the specific things that we find in writing and speaking.", 
    "year": 1980, 
    "venue": "ACL", 
    "references": 0, 
    "authors": [
      "W. Chafe"
    ], 
    "topics": [
      "Observable"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119055", 
    "title": "A Simple Probabilistic Approach to Classification And Routing", 
    "fields_of_study": [
      "Mathematics", 
      "Computer Science"
    ], 
    "abstract": "Several classification and routing methods were implemented and compared. The experiments used FBIS documents from four categories, and the measures used were the tf.idf and Cosine similarity measures, and a maximum likelihood estimate based on assuming a Multinomial Distribution for the various topics (populations). In addition, the SMART program was run with 'lnc.ltc' weighting and compared to the others.Decisions for both our classification scheme (documents are put into any number of disjoint categories) and our routing scheme (documents are assigned a 'score' and ranked relative to each category) are based on the highest probability for correct classification or routing. All of the techniques described here are fully automatic, and use a training set of relevant documents to produce lists of distinguishing terms and weights. All methods (ours and the ones we compared to) gave excellent results for the classification task, while the one based on the Multinomial Distribution produced the best results on the routing task.", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 3, 
    "authors": [
      "Louise Guthrie", 
      "J. Leistensnider"
    ], 
    "topics": [
      "Routing", 
      "Cosine similarity", 
      "Tf\u2013idf", 
      "SMART", 
      "Population", 
      "Experiment", 
      "Multinomial logistic regression"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-6103", 
    "title": "A logical-based corpus for cross-lingual evaluation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "At present, different deep learning models are presenting high accuracy on popular inference datasets such as SNLI, MNLI, and SciTail. However, there are different indicators that those datasets can be exploited by using some simple linguistic patterns. This fact poses difficulties to our understanding of the actual capacity of machine learning models to solve the complex task of textual inference. We propose a new set of syntactic tasks focused on contradiction detection that require specific capacities over linguistic logical forms such as: Boolean coordination, quantifiers, definite description, and counting operators. We evaluate two kinds of deep learning models that implicitly exploit language structure: recurrent models and the Transformer network BERT. We show that although BERT is clearly more efficient to generalize over most logical forms, there is space for improvement when dealing with counting operators. Since the syntactic tasks can be implemented in different languages, we show a successful case of cross-lingual transfer learning between English and Portuguese.", 
    "year": 2019, 
    "venue": "DeepLo@EMNLP-IJCNLP", 
    "references": 31, 
    "authors": [
      "Felipe Salvatore", 
      "M. Finger", 
      "R. Hirata"
    ], 
    "topics": [
      "Deep learning", 
      "Machine learning", 
      "Transformer", 
      "Quantifier (logic)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1070", 
    "title": "Effective Use of Transformer Networks for Entity Tracking", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities\u2019 interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still unknown. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperforms even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the model to focus on a particular entity. Second, we assess the degree to which the transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-of-the-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate process state.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 28, 
    "authors": [
      "Aditya Gupta", 
      "Greg Durrett"
    ], 
    "topics": [
      "Transformer", 
      "Entity", 
      "Natural language understanding", 
      "Procedural programming", 
      "GUID Partition Table", 
      "Baseline (configuration management)", 
      "Process state", 
      "Interaction", 
      "Transformers", 
      "Encoder", 
      "Oblique projection", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6438", 
    "title": "LIUM-CVC Submissions for WMT18 Multimodal Translation Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English\u2192French and second for English\u2192German language pairs among the constrained submissions according to the automatic evaluation metric METEOR.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 26, 
    "authors": [
      "Ozan Caglayan", 
      "Walid Aransa", 
      "Adrien Bardet", 
      "Mercedes Garc\u00eda-Mart\u00ednez", 
      "Fethi Bougares", 
      "Lo\u00efc Barrault", 
      "Marc Masana", 
      "Luis Herranz", 
      "Joost van de Weijer"
    ], 
    "topics": [
      "Multimodal interaction", 
      "METEOR", 
      "Neural machine translation", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.NLPCOVID19-2.17", 
    "title": "Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. In this paper, we analyse Dutch public sentiment on governmental COVID-19 measures from text data collected across three online media sources (Twitter, Reddit and Nu.nl) from February to July 2020. We apply sentiment analysis methods to analyse polarity over time, as well as to identify stance towards two specific pandemic policies regarding social distancing and wearing face masks. The presented preliminary results provide valuable insights into the narratives shown in vast social media text data, which help understand the influence of COVID-19 measures on the general public.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 29, 
    "authors": [
      "Shihan Wang", 
      "M. Schraagen", 
      "E. T. K. Sang", 
      "M. Dastani"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.550", 
    "title": "Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A. One big challenge for addressing this task is the lack of training data. Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers. We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task. Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation. Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model. This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 27, 
    "authors": [
      "Xinyu Xing", 
      "Xiaosheng Fan", 
      "Xiaojun Wan"
    ], 
    "topics": [
      "Natural language generation", 
      "Multi-source", 
      "Pointer (computer programming)", 
      "Scientific literature"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1303", 
    "title": "SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With the recent rise of #MeToo, an increasing number of personal stories about sexual harassment and sexual abuse have been shared online. In order to push forward the fight against such harassment and abuse, we present the task of automatically categorizing and analyzing various forms of sexual harassment, based on stories shared on the online forum SafeCity. For the labels of groping, ogling, and commenting, our single-label CNN-RNN model achieves an accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%. Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this helps extract features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and \u2018pin the creeps\u2019.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 32, 
    "authors": [
      "S. Karlekar", 
      "Mohit Bansal"
    ], 
    "topics": [
      "Physiological Sexual Disorders", 
      "Multi-label classification", 
      "Sexual abuse", 
      "Multiclass classification", 
      "Categorization", 
      "Plausibility structure", 
      "The Fight: Lights Out", 
      "Cluster analysis", 
      "Description", 
      "Numerous", 
      "Drug abuse", 
      "Activation function", 
      "Fill Out Form", 
      "Embedding", 
      "Window function", 
      "Imagery", 
      "Random neural network", 
      "statistical cluster", 
      "Calcium oxide"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1811", 
    "title": "Syllable and language model based features for detecting non-scorable tests in spoken language proficiency assessment applications", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This work introduces new methods for detecting non-scorable tests, i.e., tests that cannot be accurately scored automatically, in educational applications of spoken language proficiency assessment. Those include cases of unreliable automatic speech recognition (ASR), often because of noisy, off-topic, foreign or unintelligible speech. We examine features that estimate signalderived syllable information and compare it with ASR results in order to detect responses with problematic recognition. Further, we explore the usefulness of language model based features, both for language models that are highly constrained to the spoken task, and for task independent phoneme language models. We validate our methods on a challenging dataset of young English language learners (ELLs) interacting with an automatic spoken assessment system. Our proposed methods achieve comparable performance compared to existing non-scorable detection approaches, and lead to a 21% relative performance increase when combined with existing approaches.", 
    "year": 2014, 
    "venue": "BEA@ACL", 
    "references": 24, 
    "authors": [
      "A. Metallinou", 
      "Jian Cheng"
    ], 
    "topics": [
      "Language model", 
      "Syllable", 
      "Sensor", 
      "Speech recognition", 
      "Random forest", 
      "Interaction", 
      "Off topic", 
      "Bigram", 
      "Information", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Formal language", 
      "Feature model", 
      "Automated system recovery"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2077", 
    "title": "UWAV at SemEval-2017 Task 7: Automated feature-based system for locating puns", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe our system created for SemEval-2017 Task 7: Detection and Interpretation of English Puns(Miller et al., 2017). We tackle subtask 1, pun detection, by leveraging features selected from sentences to design a classifier that can disambiguate between the presence or absence of a pun. We address subtask 2, pun location, by utilizing a decision flow structure that uses presence or absence of certain features to decide the next action. The results obtained by our system are encouraging, considering the simplicity of the system. We consider this system as a precursor for deeper exploration on efficient feature selection for pun detection.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 7, 
    "authors": [
      "Ankit Vadehra"
    ], 
    "topics": [
      "Feature selection", 
      "Language model", 
      "Statistical classification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6708", 
    "title": "Supporting Content Design with an Eye Tracker: The Case of Weather-based Recommendations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Designing content output for weatheraware services based on domain experts can sometimes be arduous due to their limited availability and the amount and complexity of information considered in explaining their recommendations. As an initial step in our work towards generating recommendations that are acceptable and readable, our methodology involving an eye tracker attempts to simplify and capture more valuable data in early design stages. Our pilot study explored which information in weather-based recommendations seemed to be more useful to support users decision making. The results suggest that interactive content could be deployed based on the relevance of informational items and both graphical points of interest and legends could help in delivering content more efficiently.", 
    "year": 2018, 
    "venue": "", 
    "references": 16, 
    "authors": [
      "Alejandro Catal\u00e1", 
      "J. M. Alonso", 
      "Alberto Bugar\u00edn"
    ], 
    "topics": [
      "Eye tracking", 
      "Recommender system", 
      "Point of interest", 
      "Limited availability", 
      "Relevance", 
      "Human-readable medium", 
      "Graphical user interface"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-5506", 
    "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.", 
    "year": 2017, 
    "venue": "SIGDIAL Conference", 
    "references": 35, 
    "authors": [
      "Mihail Eric", 
      "Lakshmi. Krishnan", 
      "F. Charette", 
      "Christopher D. Manning"
    ], 
    "topics": [
      "Knowledge base", 
      "Commonsense knowledge (artificial intelligence)", 
      "End-to-end principle", 
      "Smoothing", 
      "Heuristic", 
      "Attribute\u2013value pair", 
      "Information retrieval", 
      "Point of interest", 
      "Dialog system", 
      "Human reliability", 
      "Scheduling (computing)", 
      "Automatic sounding", 
      "Closing (morphology)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/P19-1028", 
    "title": "Augmenting Neural Networks with First-order Logic", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 49, 
    "authors": [
      "Tao Li", 
      "Vivek Srikumar"
    ], 
    "topics": [
      "Artificial neural network", 
      "Shallow parsing", 
      "First-order logic", 
      "Baseline (configuration management)", 
      "Natural language", 
      "Commonsense knowledge (artificial intelligence)", 
      "End-to-end principle", 
      "Computation", 
      "Experiment", 
      "Programming paradigm", 
      "First-order predicate", 
      "Mathematical model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974358.974405", 
    "title": "A Practical Evaluation of an Integrated Translation Tool during a Large Scale Localisation Project", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper reports on the assessment of computer assisted translation tools for a large localisation company and the practical evaluation of one such tool.", 
    "year": 1994, 
    "venue": "ANLP", 
    "references": 7, 
    "authors": [
      "Reinhard Sch\u00e4ler"
    ], 
    "topics": [
      "Computer-assisted translation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5438", 
    "title": "Language Models Learn POS First", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.", 
    "year": 2018, 
    "venue": "BlackboxNLP@EMNLP", 
    "references": 7, 
    "authors": [
      "Naomi Saphra", 
      "Adam Lopez"
    ], 
    "topics": [
      "Language model", 
      "Black box", 
      "Long short-term memory", 
      "Part-of-speech tagging", 
      "Natural language", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-3011", 
    "title": "Computational Linguistics for Enhancing Scientific Reproducibility and Reducing Healthcare Inequities", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Computational linguistics holds promise for improving scientific integrity in clinical psychology, and for reducing longstanding inequities in healthcare access and quality. This paper describes how computational linguistics approaches could address the \u201creproducibility crisis\u201d facing social science, particularly with regards to reliable diagnosis of neurodevelopmental and psychiatric conditions including autism spectrum disorder (ASD). It is argued that these improvements in scientific integrity are poised to naturally reduce persistent healthcare inequities in neglected subpopulations, such as verbally fluent girls and women with ASD, but that concerted attention to this issue is necessary to avoid reproducing biases built into training data. Finally, it is suggested that computational linguistics is just one component of an emergent digital phenotyping toolkit that could ultimately be used for clinical decision support, to improve clinical care via precision medicine (i.e., personalized intervention planning), granular treatment response monitoring (including remotely), and for gene-brain-behavior studies aiming to pinpoint the underlying biological etiology of otherwise behaviorally-defined conditions like ASD.", 
    "year": 2019, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "J. Parish-Morris"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation", 
      "Precision medicine", 
      "Clinical decision support system", 
      "Personalization", 
      "Emergence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119384.1119389", 
    "title": "NE Recognition Without Training Data on a Language You Don't Speak", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe an experiment to adapt a named entity recognition system from English to Cebuano as part of the TIDES surprise language program. With 4 person-days of effort, and with no previous knowledge of which language would be involved, no knowledge of the language in question once it was announced, and no training data available, we adapted the ANNIE system for Cebuano and achieved an F-measure of 77.5%.", 
    "year": 2003, 
    "venue": "NER@ACL", 
    "references": 6, 
    "authors": [
      "D. Maynard", 
      "V. Tablan", 
      "H. Cunningham"
    ], 
    "topics": [
      "Named-entity recognition"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.wnut-1.21", 
    "title": "Impact of ASR on Alzheimer\u2019s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.", 
    "year": 2020, 
    "venue": "WNUT", 
    "references": 37, 
    "authors": [
      "Aparna Balagopalan", 
      "Ksenia Shkaruta", 
      "Jekaterina Novikova"
    ], 
    "topics": [
      "Alzheimer's Disease", 
      "Automatic speech recognition", 
      "Mathematical optimization", 
      "Automated system recovery", 
      "Automatic system recovery", 
      "Deletion Mutation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1034", 
    "title": "A Boundary-aware Neural Model for Nested Named Entity Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In natural language processing, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our model can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our model utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce multitask learning to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on GENIA dataset and the experimental results demonstrate that our model outperforms other state-of-the-art methods.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 28, 
    "authors": [
      "Changmeng Zheng", 
      "Y. Cai", 
      "J. Xu", 
      "Ho-fung Leung", 
      "Guandong Xu"
    ], 
    "topics": [
      "Named-entity recognition"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1125", 
    "title": "The UWNLP system at SemEval-2018 Task 7: Neural Relation Extraction Model with Selectively Incorporated Concept Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our submission for SemEval 2018 Task 7 shared task on semantic relation extraction and classification in scientific papers. Our model is based on the end-to-end relation extraction model of (Miwa and Bansal, 2016) with several enhancements such as character-level encoding attention mechanism on selecting pretrained concept candidate embeddings. Our official submission ranked the second in relation classification task (Subtask 1.1 and Subtask 2 Senerio 2), and the first in the relation extraction task (Subtask 2 Scenario 1).", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 22, 
    "authors": [
      "Yi Luan", 
      "Mari Ostendorf", 
      "Hannaneh Hajishirzi"
    ], 
    "topics": [
      "Relationship extraction", 
      "SemEval", 
      "Information extraction", 
      "Ontology components", 
      "Scientific literature", 
      "End-to-end principle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.ngt-1.4", 
    "title": "Compressing Neural Machine Translation Models with 4-bit Precision", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural Machine Translation (NMT) is resource-intensive. We design a quantization procedure to compress fit NMT models better for devices with limited hardware capability. We use logarithmic quantization, instead of the more commonly used fixed-point quantization, based on the empirical fact that parameters distribution is not uniform. We find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. We also propose to use an error-feedback mechanism during retraining, to preserve the compressed model as a stale gradient. We empirically show that NMT models based on Transformer or RNN architecture can be compressed up to 4-bit precision without any noticeable quality degradation. Models can be compressed up to binary precision, albeit with lower quality. RNN architecture seems to be more robust towards compression, compared to the Transformer.", 
    "year": 2020, 
    "venue": "NGT", 
    "references": 25, 
    "authors": [
      "A. F. Aji", 
      "Kenneth Heafield"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1377", 
    "title": "Top-Down Structurally-Constrained Neural Response Generation with Lexicalized Probabilistic Context-Free Grammar", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We consider neural language generation under a novel problem setting: generating the words of a sentence according to the order of their first appearance in its lexicalized PCFG parse tree, in a depth-first, left-to-right manner. Unlike previous tree-based language generation methods, our approach is both (i) top-down and (ii) explicitly generating syntactic structure at the same time. In addition, our method combines neural model with symbolic approach: word choice at each step is constrained by its predicted syntactic function. We applied our model to the task of dialog response generation, and found it significantly improves over sequence-to-sequence baseline, in terms of diversity and relevance. We also investigated the effect of lexicalization on language generation, and found that lexicalization schemes that give priority to content words have certain advantages over those focusing on dependency relations.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 28, 
    "authors": [
      "Wenchao Du", 
      "A. Black"
    ], 
    "topics": [
      "Natural language generation", 
      "Parse tree", 
      "Stochastic context-free grammar", 
      "Depth-first search", 
      "Vocabulary", 
      "Relevance", 
      "Baseline (configuration management)", 
      "Parsing", 
      "dialog", 
      "Error analysis (mathematics)", 
      "Categorial grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/P19-1496", 
    "title": "TWEETQA: A Social Media Focused Question Answering Dataset", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the fine-tuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 29, 
    "authors": [
      "Wenhan Xiong", 
      "Jiawei Wu", 
      "Hong Wang", 
      "Vivek Kulkarni", 
      "Mo Yu", 
      "Shiyu Chang", 
      "Xiaoxiao Guo", 
      "William Yang Wang"
    ], 
    "topics": [
      "Social media", 
      "Question answering", 
      "Baseline (configuration management)", 
      "Crowdsourcing", 
      "Hashtag", 
      "Named entity", 
      "Real-time locating system", 
      "Wikipedia", 
      "Natural language", 
      "Google Questions and Answers", 
      "Human reliability", 
      "Categorization", 
      "User (computing)", 
      "Institute for Operations Research and the Management Sciences", 
      "Abstract data type", 
      "Convolutional neural network", 
      "Profiling (computer programming)", 
      "Squad"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3606", 
    "title": "TwittDict: Extracting Social Oriented Keyphrase Semantics from Twitter", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Social media not only carries information that is up-to-date, but also bears the wisdom of the crowd. In social media, new words are developed everyday, including slangs, combinations of existing terms, entity names, etc. These terms are initially used in small communities, which can later grow popular and become new standards. The ability to early recognize the existence and understand the meanings of these terms can prove to be crucial, especially to emergence detection applications. We present an ongoing research work that investigates the use of topical analysis to extract semantic of terms in social media. In particular, the proposed method extracts semantically related words associated with a target word from a corpus of tweets. We provide preliminary, anecdotal results comprising the semantic extraction of five different keywords.", 
    "year": 2015, 
    "venue": "ACL 2015", 
    "references": 25, 
    "authors": [
      "Suppawong Tuarob", 
      "Wanghuan Chu", 
      "D. Chen", 
      "Conrad S. Tucker"
    ], 
    "topics": [
      "Social media", 
      "Emergence", 
      "Wisdom of the crowd", 
      "Natural language processing", 
      "Temporal logic", 
      "User experience", 
      "Test data", 
      "Protologism", 
      "Baseline (configuration management)", 
      "Word lists by frequency", 
      "Evaluation", 
      "Name", 
      "Behavior", 
      "standards characteristics", 
      "Information extraction", 
      "Body of uterus", 
      "Community"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.457", 
    "title": "A Study on Efficiency, Accuracy and Document Structure for Answer Sentence Selection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An essential task of most Question Answering (QA) systems is to re-rank the set of answer candidates, i.e., Answer Sentence Selection (AS2). These candidates are typically sentences either extracted from one or more documents preserving their natural order or retrieved by a search engine. Most state-of-the-art approaches to the task use huge neural models, such as BERT, or complex attentive architectures. In this paper, we argue that by exploiting the intrinsic structure of the original rank together with an effective word-relatedness encoder, we achieve the highest accuracy among the cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the 18 minutes required by a standard BERT-base fine-tuning.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 45, 
    "authors": [
      "Daniele Bonadiman", 
      "Alessandro Moschitti"
    ], 
    "topics": [
      "Question answering", 
      "Encoder", 
      "Transformer", 
      "Web search engine", 
      "Analysis of algorithms", 
      "Language model", 
      "Random neural network", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/issn.2683-0078.2019_011", 
    "title": "Translation Quality Assessment Tools and Processes in Relation to CAT Tools", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines \u201cquality\u201d. Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.", 
    "year": 2019, 
    "venue": "Proceedings of the Second Workshop Human-Informed Translation and Interpreting Technology associated with RANLP 2019", 
    "references": 5, 
    "authors": [
      "Viktoriya Petrova"
    ], 
    "topics": [
      "Computer-assisted translation", 
      "Language industry", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1037", 
    "title": "A Graph-based Approach for Contextual Text Normalization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools. Text normalization, which corresponds to restoring the non-standard words to their canonical forms, provides a solution to this challenge. We introduce an unsupervised text normalization approach that utilizes not only lexical, but also contextual and grammatical features of social text. The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus. The graph encodes the relative positions of the words with respect to each other, as well as their part-ofspeech tags. The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words, and the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the non-standard words in the input text. Our results show that it achieves state-ofthe-art F-score performance on standard datasets. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 32, 
    "authors": [
      "Cagil Sonmez", 
      "Arzucan \u00d6zg\u00fcr"
    ], 
    "topics": [
      "Text normalization", 
      "Metaphone", 
      "Natural language processing", 
      "Social media", 
      "Longest common subsequence problem", 
      "Unsupervised learning", 
      "Text corpus", 
      "Algorithm", 
      "Edit distance", 
      "Dictionary", 
      "Rendering (computer graphics)", 
      "F1 score", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980845.980860", 
    "title": "The Berkeley FrameNet Project", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 8, 
    "authors": [
      "Collin F. Baker", 
      "C. Fillmore", 
      "J. Lowe"
    ], 
    "topics": [
      "FrameNet", 
      "Text corpus", 
      "Frame language", 
      "Emoticon", 
      "Lexicon", 
      "Lexicography", 
      "Component-based software engineering", 
      "HTML element", 
      "Exemplification", 
      "IBM Notes"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-demo.42", 
    "title": "Stretch-VST: Getting Flexible With Visual Stories", 
    "fields_of_study": null, 
    "abstract": "In visual storytelling, a short story is generated based on a given image sequence. Despite years of work, most visual storytelling models remain limited in terms of the generated stories\u2019 fixed length: most models produce stories with exactly five sentences because five sentence stories dominate the training data. The fix-length stories carry limited details and provide ambiguous textual information to the readers. Therefore, we propose to \u201cstretch\u201d the stories, which create the potential to present in-depth visual details. This paper presents Stretch-VST, a visual storytelling framework that enables the generation of prolonged stories by adding appropriate knowledge, which is selected by the proposed scoring function. We propose a length-controlled Transformer to generate long stories. This model introduces novel positional encoding methods to maintain story quality with lengthy inputs. Experiments confirm that long stories are generated without deteriorating the quality. The human evaluation further shows that Stretch-VST can provide better focus and detail when stories are prolonged compared to the state of the art. The demo video is available on Youtube1, and the live demo can be found on website2.", 
    "year": 2021, 
    "venue": "", 
    "references": 27, 
    "authors": [
      "Chi-Yang Hsu", 
      "Yun-Wei Chu", 
      "Tsai-Lun Yang", 
      "Ting-Hao Huang", 
      "Lun-Wei Ku"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.318", 
    "title": "Probing Image-Language Transformers for Verb Understanding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multimodal image\u2013language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations \u2013 in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image\u2013sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image\u2013language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging. 1 Evaluating Verb Understanding The success of image\u2013language models in realworld applications relies on their ability to relate different aspects of language (such as verbs or objects) to images, which we refer to as multimodal understanding. For example, an image-retrieval model needs to distinguish between \u201ceating an apple\u201d and \u201ccutting an apple\u201d and a captioning model must accurately describe the actions in a scene. Previous work shows that image\u2013language benchmarks do not always fully measure such multimodal understanding: object retrieval models fail to account for linguistic structure (Akula et al., 2020), visual question answering (VQA) models overly rely on language priors (Goyal et al., 2017; Agrawal et al., 2018), and captioning metrics do not always measure if captions \u201challucinate\u201d objects in an image (Rohrbach et al., 2018). Inspired by this, prior work introduced tasks to specifically examine whether models can relate objects to images (Shekhar et al., 2017) or classify frequent interactions associated with objects (Chao et al., 2015). However, both these datasets are limited to the 80 objects in the MSCOCO detection challenge (Lin et al., 2014). To address this gap, we design a benchmark focused on verbs called SVO-Probes for examining subject, verb, object triplets; more specifically, we collect a set of image\u2013sentence pairs (in English) where each pair is annotated with whether the sentence corresponds to the image or not. As shown in Fig. 1, for a given sentence, in addition to a positive image that matches the sentence, our dataset includes controlled negative images that do not correspond to specific aspects of the sentence (i.e., subject, verb, and object). These controlled examples enable us to probe models for their understanding of verbs as well as subjects and objects. Our dataset consists of 421 verbs and includes over 48, 000 image\u2013sentence pairs. We use our benchmark to evaluate the recent family of multimodal (image\u2013language) transformers that have shown impressive results on benchmarks like VQA and image retrieval (Lu et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Li et al., 2020b,a; Huang et al., 2020). Our goal is to investigate if the good performance of these models is due to learned representations that successfully relate different aspects of language to images. More specifically, we evaluate a few architectural variations of these models in a zero-shot way by using the pretrained models to classify if image\u2013sentence pairs from SVO-Probes match. Our results show that the performance of all evaluated models is worst on verbs, with subjects being easier than verbs but harder than objects. We find that this observation does not depend on the frequency of test examples in pretraining data. Moreover, it is considerably harder for all models to correctly classify image\u2013sentence pairs that do not ar X iv :2 10 6. 09 14 1v 1 [ cs .C L ] 1 6 Ju n 20 21 A woman jogs on the beach. A man is jumping into the sea. A person sings at a concert. A man jumping into a river. A animal lays in the grass. Children cross the street. child, cross, street lady, cross, street animal, lay, grass woman, lay, grass person, sing, concert person, dance, concert man, jump, river man, kayak, river man, jump, sea man, jump, mountain woman, jog, beach woman, jog, forest Pos Neg", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 31, 
    "authors": [
      "Lisa Anne Hendricks", 
      "Aida Nematzadeh"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1106", 
    "title": "A Structured Syntax-Semantics Interface for English-AMR Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested. To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our annotation explains how 97% of AMR edges are evoked by words or syntax. Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%. While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMR-to-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 39, 
    "authors": [
      "Ida Szubert", 
      "Adam Lopez", 
      "Nathan Schneider"
    ], 
    "topics": [
      "Adaptive Multi-Rate audio codec", 
      "Align (company)", 
      "Meaning\u2013text theory", 
      "Parsing", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-2406", 
    "title": "Identifying Sensible Lexical Relations in Generated Stories", 
    "fields_of_study": [
      "History"
    ], 
    "abstract": "As with many text generation tasks, the focus of recent progress on story generation has been in producing texts that are perceived to \u201cmake sense\u201d as a whole. There are few automated metrics that address this dimension of story quality even on a shallow lexical level. To initiate investigation into such metrics, we apply a simple approach to identifying word relations that contribute to the \u2018narrative sense\u2019 of a story. We use this approach to comparatively analyze the output of a few notable story generation systems in terms of these relations. We characterize differences in the distributions of relations according to their strength within each story.", 
    "year": 2019, 
    "venue": "", 
    "references": 22, 
    "authors": [
      "Melissa Roemmele"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5406", 
    "title": "Unbabel\u2019s Participation in the WMT19 Translation Quality Estimation Shared Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: We combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.", 
    "year": 2019, 
    "venue": "WMT", 
    "references": 20, 
    "authors": [
      "Fabio Kepler", 
      "Jonay Tr\u00e9nous", 
      "Marcos Vin\u00edcius Treviso", 
      "M. Vera", 
      "Ant\u00f3nio G\u00f3is", 
      "M. Amin Farajian", 
      "Ant\u00f3nio Vilarinho Lopes", 
      "Andr\u00e9 F. T. Martins"
    ], 
    "topics": [
      "Ensemble forecasting", 
      "Kerrison Predictor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-3012", 
    "title": "Multimodal Machine Translation with Embedding Prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand visual objects and their relations with natural languages. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 F-score for rare word translation.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 21, 
    "authors": [
      "Tosho Hirasawa", 
      "Hayahide Yamagishi", 
      "Yukio Matsumura", 
      "Mamoru Komachi"
    ], 
    "topics": [
      "Multimodal interaction", 
      "Neural machine translation", 
      "BLEU", 
      "Visual Objects", 
      "Natural language", 
      "Meteor", 
      "Computer"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.tlt-1.9", 
    "title": "Estimating POS Annotation Consistency of Different Treebanks in a Language", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a new symmetric measure (called \u03b8pos) that utilises the non-symmetric KLcpos3 measure (Rosa and \u017dabokrtsk\u00fd, 2015) to allow us to compare the annotation consistency between different treebanks of a given language, annotated under the same guidelines. We can set a threshold for this new measure so that a pair of treebanks can be considered harmonious in their annotation if \u03b8pos does not surpass the threshold. For the calculation of the threshold, we estimate the effects of (i) the size variation, and (ii) the genre variation in the considered pair of treebanks. The estimations are based on data from treebanks of distinct language families, making the threshold less dependent on the properties of individual languages. We demonstrate the utility of the proposed measure by listing the treebanks in Universal Dependencies version 2.5 (UDv2.5) (Zeman et al., 2019) data that are annotated consistently with other treebanks of the same language. However, the measure could be used to assess inter-treebank annotation consistency under other (non-UD) annotation guidelines as well.", 
    "year": 2020, 
    "venue": "TLT", 
    "references": 21, 
    "authors": [
      "Akshay Aggarwal", 
      "Daniel Zeman"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-056-4_140", 
    "title": "Cross-Lingual Word Embeddings for Morphologically Rich Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2% in the nearest neighbour ranking.", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 15, 
    "authors": [
      "A. \u00dcst\u00fcn", 
      "G. Bouma", 
      "Gertjan van Noord"
    ], 
    "topics": [
      "Word embedding", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_093", 
    "title": "Unsupervised Learning of Morphology with Graph Sampling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a language-independent, graph-based probabilistic model of morphology, which uses transformation rules operating on whole words instead of the traditional morphological segmentation. The morphological analysis of a set of words is expressed through a graph having words as vertices and structural relationships between words as edges. We define a probability distribution over such graphs and develop a sampler based on the Metropolis-Hastings algorithm. The sampling is applied in order to determine the strength of morphological relationships between words, filter out accidental similarities and reduce the set of rules necessary to explain the data. The model is evaluated on the task of finding pairs of morphologically similar words, as well as generating new words. The results are compared to a state-of-the-art segmentation-based approach.", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 34, 
    "authors": [
      "Maciej Sumalvico"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Sampling (signal processing)", 
      "Markov chain Monte Carlo", 
      "Statistical model", 
      "Galaxy morphological classification", 
      "Mathematical morphology", 
      "Metropolis\u2013Hastings algorithm", 
      "Brown Corpus", 
      "Word embedding", 
      "Point of sale", 
      "Language model", 
      "Metropolis", 
      "Language-independent specification", 
      "Approximation algorithm", 
      "Protologism", 
      "Error-tolerant design", 
      "Monte Carlo method", 
      "Filter (signal processing)", 
      "Semantics (computer science)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1161", 
    "title": "Denoising Distantly Supervised Open-Domain Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "Yankai Lin", 
      "Haozhe Ji", 
      "Zhiyuan Liu", 
      "Maosong Sun"
    ], 
    "topics": [
      "Question answering", 
      "Dublin Core", 
      "Noise reduction"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00097", 
    "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence\u2019s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https://github.com/yinwenpeng/Answer_Selection.", 
    "year": 2016, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 67, 
    "authors": [
      "Wenpeng Yin", 
      "Hinrich Sch\u00fctze", 
      "Bing Xiang", 
      "Bowen Zhou"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Textual entailment", 
      "Convolution", 
      "Deep learning", 
      "Interdependence", 
      "Test engineer", 
      "Best practice", 
      "Artificial neural network", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2112", 
    "title": "Towards segment-based recognition of argumentation structure in short texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Despite recent advances in discourse parsing and causality detection, the automatic recognition of argumentation structure of authentic texts is still a very challenging task. To approach this problem, we collected a small corpus of German microtexts in a text generation experiment, resulting in texts that are authentic but of controlled linguistic and rhetoric complexity. We show that trained annotators can determine the argumentation structure on these microtexts reliably. We experiment with different machine learning approaches for automatic argumentation structure recognition on various levels of granularity of the scheme. Given the complex nature of such a discourse understanding tasks, the first results presented here are promising, but invite for further investigation.", 
    "year": 2014, 
    "venue": "ArgMining@ACL", 
    "references": 45, 
    "authors": [
      "A. Peldszus"
    ], 
    "topics": [
      "Machine learning", 
      "Microsoft Outlook for Mac", 
      "Text corpus", 
      "Causality", 
      "Out of the box (feature)", 
      "Natural language generation", 
      "Crowdsourcing", 
      "Parsing", 
      "Top-down and bottom-up design"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00203", 
    "title": "Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy. In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency-based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7% and 80.8% respectively.", 
    "year": 2014, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 57, 
    "authors": [
      "Yonatan Belinkov", 
      "Tao Lei", 
      "R. Barzilay", 
      "A. Globerson"
    ], 
    "topics": [
      "Word embedding", 
      "Computational linguistics", 
      "Discriminative model", 
      "Support vector machine", 
      "Parsing", 
      "Word lists by frequency", 
      "Word-sense disambiguation", 
      "Nonlinear system", 
      "Relevance", 
      "Sparse matrix", 
      "Computation", 
      "Realms of the Haunting", 
      "Wolfgang Kr\u00f6ger", 
      "Linear algebra", 
      "eric", 
      "Attachments"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075096.1075121", 
    "title": "Compounding and Derivational Morphology in a Finite-State Setting", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word formation for a language like German. A refinement of an RTN-based approximation algorithm is proposed, which extends the state space of the automaton by selectively adding distinctions based on the parsing history at the point of entering a context-free rule. The selection of history items exploits the specific linguistic nature of word formation. As experiments show, this algorithm avoids an explosion of the size of the automaton in the approximation construction.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 18, 
    "authors": [
      "Jonas Kuhn"
    ], 
    "topics": [
      "Approximation algorithm", 
      "Automaton", 
      "State space", 
      "Mathematical morphology", 
      "Context-free language", 
      "Parsing", 
      "Experiment", 
      "Refinement (computing)", 
      "Unification (computer science)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3005", 
    "title": "ParFDA for Fast Deployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems. ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deploy", 
    "year": 2015, 
    "venue": "WMT@EMNLP", 
    "references": 12, 
    "authors": [
      "Ergun Bi\u00e7ici", 
      "Qun Liu", 
      "Andy Way"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Moses", 
      "Software deployment", 
      "BLEU", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.36", 
    "title": "Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI: Ongoing Applied Research", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter, there are issues with their interpretability. We propose a novel use of learned feature importance which improves upon the performance of prior state-of-the-art text classification techniques, while producing more easily interpretable decisions. We also discuss both technical and practical challenges that remain for this task.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 16, 
    "authors": [
      "D. Hardage", 
      "Peyman Najafirad"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.conll-1.36", 
    "title": "Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.", 
    "year": 2020, 
    "venue": "CONLL", 
    "references": 73, 
    "authors": [
      "Steven Derby", 
      "Paul Miller", 
      "Barry Devereux"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1059", 
    "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 72, 
    "authors": [
      "Wenya Wang", 
      "Sinno Jialin Pan", 
      "Daniel Dahlmeier", 
      "Xiaokui Xiao"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Conditional random field", 
      "Information extraction", 
      "SemEval", 
      "Terminology extraction", 
      "Feature engineering", 
      "User-generated content", 
      "High- and low-level", 
      "Brown Corpus", 
      "Recursion (computer science)", 
      "Unified Framework", 
      "Nonlinear system", 
      "Logic programming", 
      "Baseline (configuration management)", 
      "Recursive neural network", 
      "Network interface device", 
      "Artificial neural network", 
      "Software propagation", 
      "Benchmark (computing)", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5549", 
    "title": "Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distinguishing between singular and plural \"you\" in English is a challenging task which has potential for downstream applications, such as machine translation or coreference resolution. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as \"yall\"), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a model to distinguish between the single/plural you, finding that although in-domain training achieves reasonable accuracy (over 77%), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.", 
    "year": 2019, 
    "venue": "W-NUT@EMNLP", 
    "references": 16, 
    "authors": [
      "Gabriel Stanovsky", 
      "Ronen Tamari"
    ], 
    "topics": [
      "Machine translation", 
      "Downstream (software development)", 
      "Supervised learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6451", 
    "title": "Findings of the WMT 2018 Shared Task on Quality Estimation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granularity levels: word, phrase, sentence and document. This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems. Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 37, 
    "authors": [
      "Lucia Specia", 
      "F. Blain", 
      "V. Logacheva", 
      "Ram\u00f3n Fern\u00e1ndez Astudillo", 
      "Andr\u00e9 F. T. Martins"
    ], 
    "topics": [
      "Neural machine translation", 
      "Postediting", 
      "HTTPS", 
      "Formal language", 
      "Query expansion"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4809", 
    "title": "A comparative User Evaluation of Terminology Management Tools for Interpreters", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "When facing new fields, interpreters need to perform extensive searches for specialised knowledge and terminology. They require this information prior to an interpretation and have it accessible during the interpreting service. Fortunately, there are currently several terminology management tools capable of assisting interpreters before and during an interpretation service. Although these tools appear to be quite similar, they provide different kind of features and as a result they exhibit different degrees of usefulness. This paper aims at describing current terminology management tools with a view to establishing a set of features to assess the extent to which terminology tools meet the specific needs of the interpreters. Subsequently, a comparative analysis is performed to evaluate these tools based on the list of features previously identified.", 
    "year": 2014, 
    "venue": "", 
    "references": 18, 
    "authors": [
      "H. Costa", 
      "Gloria Corpas Pastor", 
      "Isabel Dur\u00e1n Mu\u00f1oz"
    ], 
    "topics": [
      "Computer-assisted translation", 
      "Interpreter (computing)", 
      "Glossary", 
      "Qualitative comparative analysis", 
      "Lingo (programming language)", 
      "Simple DirectMedia Layer"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-short.69", 
    "title": "When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 0, 
    "authors": [
      "Jiahuan Li", 
      "Yutong Shen", 
      "Shujian Huang", 
      "Xinyu Dai", 
      "Jiajun Chen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-1508", 
    "title": "A Deep Architecture for Non-Projective Dependency Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Graph-based dependency parsing algorithms commonly employ features up to third order in an attempt to capture richer syntactic relations. However, each level and each feature combination must be defined manually. Besides that, input features are usually represented as huge, sparse binary vectors, offering limited generalization. In this work, we present a deep architecture for dependency parsing based on a convolutional neural network. It can examine the whole sentence structure before scoring each head/modifier candidate pair, and uses dense embeddings as input. Our model is still under ongoing work, achieving 91.6% unlabeled attachment score in the Penn Treebank.", 
    "year": 2015, 
    "venue": "VS@HLT-NAACL", 
    "references": 22, 
    "authors": [
      "E. Fonseca", 
      "Sandra M. Alu\u00edsio"
    ], 
    "topics": [
      "Parsing", 
      "Convolutional neural network", 
      "Treebank", 
      "HTTPS", 
      "Artificial neural network", 
      "Modifier key", 
      "Sparse matrix", 
      "Algorithm", 
      "Attachments"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.90", 
    "title": "ENT-DESC: Entity Description Generation by Exploring Knowledge Graph", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 31, 
    "authors": [
      "L. Cheng", 
      "Dekun Wu", 
      "Lidong Bing", 
      "Yan Zhang", 
      "Zhanming Jie", 
      "Wei Lu", 
      "Luo Si"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1089", 
    "title": "Multilingual Neural Machine Translation with Language Clustering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 43, 
    "authors": [
      "Xu Tan", 
      "Jiale Chen", 
      "Di He", 
      "Yingce Xia", 
      "Tao Qin", 
      "Tie-Yan Liu"
    ], 
    "topics": [
      "Neural machine translation", 
      "Cluster analysis", 
      "ISO 639", 
      "Experiment", 
      "Test set", 
      "BLEU", 
      "Test data", 
      "Language code", 
      "Many-to-many", 
      "Scalability", 
      "Baseline (configuration management)", 
      "Randomness", 
      "One-to-many (data model)", 
      "Horner's method", 
      "Holographic principle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1088", 
    "title": "What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 31, 
    "authors": [
      "Ver\u00f3nica P\u00e9rez-Rosas", 
      "Xinyi Wu", 
      "K. Resnicow", 
      "Rada Mihalcea"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1186", 
    "title": "Deep Multitask Learning for Semantic Dependency Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches---one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at this https URL.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 81, 
    "authors": [
      "Hao Peng", 
      "Sam Thomson", 
      "Noah A. Smith"
    ], 
    "topics": [
      "Parsing", 
      "Computer multitasking", 
      "Multilayer perceptron", 
      "Open-source software", 
      "HTTPS", 
      "Long short-term memory"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.94", 
    "title": "hub at SemEval-2021 Task 2: Word Meaning Similarity Prediction Model Based on RoBERTa and Word Frequency", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper introduces the system description of the hub team, which explains the related work and experimental results of our team\u2019s participation in SemEval 2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). The data of this shared task is mainly some cross-language or multi-language sentence pair corpus. The languages covered in the corpus include English, Chinese, French, Russian, and Arabic. The task goal is to judge whether the same words in these sentence pairs have the same meaning in the sentence. This can be seen as a task of binary classification of sentence pairs. What we need to do is to use our method to determine as accurately as possible the meaning of the words in a sentence pair are the same or different. The model used by our team is mainly composed of RoBERTa and Tf-Idf algorithms. The result evaluation index of task submission is the F1 score. We only participated in the English language task. The final score of the test set prediction results submitted by our team was 84.60.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 18, 
    "authors": [
      "Bo Huang", 
      "Yang Bai", 
      "Xiaobing Zhou"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4006", 
    "title": "Incremental Adaptation Strategies for Neural Network Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without overfitting the small adaptation data.", 
    "year": 2014, 
    "venue": "ArXiv", 
    "references": 39, 
    "authors": [
      "A. Ter-Sarkisov", 
      "Holger Schwenk", 
      "Fethi Bougares", 
      "Lo\u00efc Barrault"
    ], 
    "topics": [
      "Language model", 
      "Artificial neural network", 
      "Statistical machine translation", 
      "Speech recognition", 
      "BLEU", 
      "Perplexity", 
      "Overfitting", 
      "Simultaneous multithreading", 
      "Backoff"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-3628", 
    "title": "Socially-Aware Animated Intelligent Personal Assistant Agent", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "SARA (Socially-Aware Robot Assistant) is an embodied intelligent personal assistant that analyses the user\u2019s visual (head and face movement), vocal (acoustic features) and verbal (conversational strategies) behaviours to estimate its rapport level with the user, and uses its own appropriate visual, vocal and verbal behaviors to achieve task and social goals. The presented agent aids conference attendees by eliciting their preferences through building rapport, and then making informed personalized recommendations about sessions to attend and people to meet.", 
    "year": 2016, 
    "venue": "SIGDIAL Conference", 
    "references": 12, 
    "authors": [
      "Yoichi Matsuyama", 
      "Arjun Bhardwaj", 
      "Ran Zhao", 
      "Oscar Romeo", 
      "Sushma A. Akoju", 
      "J. Cassell"
    ], 
    "topics": [
      "Personal digital assistant", 
      "Norm (social)", 
      "End-to-end principle", 
      "Personalization", 
      "Natural language generation", 
      "Acoustic cryptanalysis", 
      "Office Assistant"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1382", 
    "title": "On Knowledge distillation from complex networks for response prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent advances in Question Answering have lead to the development of very complex models which compute rich representations for query and documents by capturing all pairwise interactions between query and document words. This makes these models expensive in space and time, and in practice one has to restrict the length of the documents that can be fed to these models. Such models have also been recently employed for the task of predicting dialog responses from available background documents (e.g., Holl-E dataset). However, here the documents are longer, thereby rendering these complex models infeasible except in select restricted settings. In order to overcome this, we use standard simple models which do not capture all pairwise interactions, but learn to emulate certain characteristics of a complex teacher network. Specifically, we first investigate the conicity of representations learned by a complex model and observe that it is significantly lower than that of simpler models. Based on this insight, we modify the simple architecture to mimic this characteristic. We go further by using knowledge distillation approaches, where the simple model acts as a student and learns to match the output from the complex teacher network. We experiment with the Holl-E dialog data set and show that by mimicking characteristics and matching outputs from a teacher, even a simple network can give improved performance.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 29, 
    "authors": [
      "Siddharth Arora", 
      "Mitesh M. Khapra", 
      "H. G. Ramaswamy"
    ], 
    "topics": [
      "Complex network", 
      "Question answering", 
      "Interaction", 
      "Artificial intelligence", 
      "dialog", 
      "Analysis of algorithms", 
      "Requirement", 
      "Document", 
      "Role-based collaboration", 
      "Computer science", 
      "Integrated information theory", 
      "Paint Tool SAI"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/S19-2151", 
    "title": "SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the pilot SemEval task on Suggestion Mining. The task consists of subtasks A and B, where we created labeled data from feedback forum and hotel reviews respectively. Subtask A provides training and test data from the same domain, while Subtask B evaluates the system on a test dataset from a different domain than the available training data. 33 teams participated in the shared task, with a total of 50 members. We summarize the problem definition, benchmark dataset preparation, and methods used by the participating teams, providing details of the methods used by the top ranked systems. The dataset is made freely available to help advance the research in suggestion mining, and reproduce the systems submitted under this task", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 38, 
    "authors": [
      "S. Negi", 
      "Tobias Daudert", 
      "P. Buitelaar"
    ], 
    "topics": [
      "SemEval", 
      "Benchmark (computing)", 
      "Algorithm", 
      "Artificial neural network", 
      "Feedback", 
      "Shroud of the Avatar:", 
      "Document classification", 
      "Test data", 
      "High-level programming language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-2303", 
    "title": "How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Until recently, summarization evaluations compared systems that produce summaries of the same target length. Neural approaches to summarization however have done away with length requirements. Here we present detailed experiments demonstrating that summaries of different length produced by the same system have a clear non-linear pattern of quality as measured by ROUGE F1 scores: initially steeply improving with summary length, then starting to gradually decline. Neural models produce summaries of different length, possibly confounding improvements of summarization techniques with potentially spurious learning of optimal summary length. We propose a new evaluation method where ROUGE scores are normalized by those of a random system producing summaries of the same length. We reanalyze a number of recently reported results and show that some negative results are in fact reports of system improvement once differences in length are taken into account. Finally, we present a small-scale human evaluation showing a similar trend of perceived quality increase with summary length, calling for the need of similar normalization in reporting human scores.", 
    "year": 2019, 
    "venue": "", 
    "references": 33, 
    "authors": [
      "Simeng Sun", 
      "Ori Shapira", 
      "Ido Dagan", 
      "A. Nenkova"
    ], 
    "topics": [
      "Stochastic process", 
      "Nonlinear system", 
      "Automatic summarization", 
      "Requirement", 
      "Experiment", 
      "ROUGE (metric)", 
      "T-norm", 
      "Rand index"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/N19-1025", 
    "title": "Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as KL vanishing, where the posterior easily collapses to the prior and model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Variational Wasserstein Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a latent variable into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the non-informative prior. The Wasserstein objective minimizes the distance between marginal distribution and the prior directly and therefore does not force the posterior to match the prior. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performance in tasks such as language modeling, likelihood approximation, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 36, 
    "authors": [
      "Prince Zizhuang Wang", 
      "William Yang Wang"
    ], 
    "topics": [
      "Autoencoder", 
      "Language model", 
      "Kullback\u2013Leibler divergence", 
      "Latent variable", 
      "Experiment", 
      "Natural language generation", 
      "Variational principle", 
      "Optimization problem", 
      "Prince", 
      "Approximation", 
      "Performance", 
      "Information", 
      "Marginal model", 
      "Mathematical optimization", 
      "Code", 
      "Entity\u2013relationship model", 
      "Li-Chen Wang"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2088", 
    "title": "Context-Dependent Translation Selection Using Convolutional Neural Network", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 38, 
    "authors": [
      "Baotian Hu", 
      "Zhaopeng Tu", 
      "Zhengdong Lu", 
      "Hang Li", 
      "Q. Chen"
    ], 
    "topics": [
      "Convolutional neural network", 
      "BLEU", 
      "Context-sensitive language", 
      "Statistical machine translation", 
      "Artificial neural network", 
      "Semantic similarity", 
      "Test data", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1177", 
    "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 19, 
    "authors": [
      "Yong Jiang", 
      "Wenjuan Han", 
      "Kewei Tu"
    ], 
    "topics": [
      "Lagrangian relaxation", 
      "Parsing", 
      "Discriminative model", 
      "Generative model", 
      "Unsupervised learning", 
      "Cluster analysis", 
      "Treebank", 
      "Expectation\u2013maximization algorithm", 
      "Urban Dictionary"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3217", 
    "title": "QCMUQ$@$QALB-2015 Shared Task: Combining Character level MT and Error-tolerant Finite-State Recognition for Arabic Spelling Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe the CMU-Q and QCRI\u2019s joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task. Our system is based on a hybrid pipeline that combines rule-based linguistic techniques with statistical methods using language modeling and machine translation, as well as an error-tolerant finite-state automata method. We trained and tested our spelling corrector using the dataset provided by the shared task organizers. Our system outperforms the baseline system and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask.", 
    "year": 2015, 
    "venue": "ANLP@ACL", 
    "references": 26, 
    "authors": [
      "Houda Bouamor", 
      "Hassan Sajjad", 
      "Nadir Durrani", 
      "Kemal Oflazer"
    ], 
    "topics": [
      "Baseline (configuration management)", 
      "Finite-state machine", 
      "Logic programming", 
      "Statistical machine translation", 
      "Language model", 
      "Error-tolerant design", 
      "Automata theory", 
      "Spell checker"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.68", 
    "title": "Modeling language evolution and feature dynamics in a realistic geographic environment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent, innovative efforts to understand the uneven distribution of languages and linguistic feature values in time and space attest to both the challenge these issues pose and the value in solving them. In this paper, we introduce a model for simulating languages and their features over time in a realistic geographic environment. At its core is a model of language phylogeny and migration whose parameters are chosen to reproduce known language family sizes and geographic dispersions. This foundation in turn is used to explore the dynamics of linguistic features. Languages are assigned feature values that can change randomly or under the influence of nearby languages according to predetermined probabilities. We assess the effects of these settings on resulting geographic and genealogical patterns using homogeneity measures defined in the literature. The resulting model is both flexible and realistic, and it can be employed to answer a wide range of related questions.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 24, 
    "authors": [
      "R. Kapur", 
      "P. Rogers"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-DEMOS.15", 
    "title": "Arabic Curriculum Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Developing a platform that analyzes the content of curricula can help identify their shortcomings and whether they are tailored to specific desired outcomes. In this paper, we present a system to analyze Arabic curricula and provide insights into their content. It allows users to explore word presence, surface-forms used, as well as contrasting statistics between different countries from which the curricula were selected. Also, it provides a facility to grade text in reference to given grade-level and gives users feedback about the complexity or difficulty of words used in a text.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 16, 
    "authors": [
      "Hamdy Mubarak", 
      "Shimaa A. Amer", 
      "Ahmed Abdelali", 
      "Kareem Darwish"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3610", 
    "title": "Discourse Segmentation for Building a RST Chinese Treebank", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd", 
    "year": 2017, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Shuyuan Cao", 
      "Nianwen Xue", 
      "I. D. Cunha", 
      "Mikel Iruskieta", 
      "Chuan Wang"
    ], 
    "topics": [
      "Treebank", 
      "Intel Matrix RAID"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1052", 
    "title": "Finding the Middle Ground - A Model for Planning Satisficing Answers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To establish sophisticated dialogue systems, text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues. Little attention has been given to this topic in text planning in contrast to dialogues that are fully aligned with anticipated user interests. When considering dialogues with congruent and incongruent interlocutor interests, dialogue partners are facing the constant challenge of finding a balance between cooperation and competition. We introduce the concept of fairness that operationalize an equal and adequate, i.e. equitable satisfaction of all interlocutors\u2019 interests. Focusing on Question-Answering (QA) settings, we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests. Due to the fact that fairness is subjective per se, we present promising results from an empirical study (N=107) in which human subjects interacted with a QA system implementing the proposed approach.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 49, 
    "authors": [
      "S. Janzen", 
      "W. Maass", 
      "T. Kowatsch"
    ], 
    "topics": [
      "Question answering", 
      "Fairness measure", 
      "Dialog system", 
      "Commonsense knowledge (artificial intelligence)", 
      "Embedded system", 
      "Automated planning and scheduling", 
      "Domain-specific language", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1033", 
    "title": "Learning Better Embeddings for Rare Words Using Distributional Representations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Irina Sergienya", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "Distributional semantics"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4634", 
    "title": "JHU System Description for the MADAR Arabic Dialect Identification Shared Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning.", 
    "year": 2019, 
    "venue": "WANLP@ACL 2019", 
    "references": 7, 
    "authors": [
      "Thomas Lippincott", 
      "Pamela Shapiro", 
      "Kevin Duh", 
      "P. McNamee"
    ], 
    "topics": [
      "Language model", 
      "Prediction by partial matching", 
      "Word embedding", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1142", 
    "title": "Improved Language Modeling by Decoding the Past", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 38, 
    "authors": [
      "Siddhartha Brahma"
    ], 
    "topics": [
      "Language model", 
      "Treebank"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.25", 
    "title": "DATE: Detecting Anomalies in Text via Self-Supervision of Transformers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others).", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 44, 
    "authors": [
      "A. Manolache", 
      "Florin Brad", 
      "E. Burceanu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-demos.8", 
    "title": "EVIDENCEMINER: Textual Evidence Discovery for Life Sciences", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation. It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization. EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis. The system of EVIDENCEMINER is available at https://evidenceminer.firebaseapp.com/.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Xuan Wang", 
      "Yingjun Guan", 
      "Weili Liu", 
      "Aabhas Chauhan", 
      "Enyi Jiang", 
      "Qi Li", 
      "D. Liem", 
      "D. Sigdel", 
      "J. Caufield", 
      "P. Ping", 
      "Jiawei Han"
    ], 
    "topics": [
      "Information extraction", 
      "PubMed", 
      "Named-entity recognition", 
      "Natural language", 
      "Document retrieval", 
      "Web search engine", 
      "Precomputation", 
      "Web application", 
      "Entity", 
      "Text corpus", 
      "Online and offline", 
      "Indexed grammar"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.SDP-1.12", 
    "title": "On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce two small ELECTRA based model named Bio-ELECTRA and Bio-ELECTRA++ that are eight times smaller than BERT Base and Bio-BERT and achieves comparable or better performance on biomedical question answering, yes/no question answer classification, question answer candidate ranking and relation extraction tasks. Bio-ELECTRA is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. Bio-ELECTRA++ is the further pre-trained version of Bio-ELECTRA trained on a corpus of open access full papers from PubMed Central. While, for biomedical named entity recognition, large BERT Base model outperforms Bio-ELECTRA++, Bio-ELECTRA and ELECTRA-Small++, with hyperparameter tuning Bio-ELECTRA++ achieves results comparable to BERT.", 
    "year": 2020, 
    "venue": "", 
    "references": 17, 
    "authors": [
      "I. B. Ozyurt"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.563", 
    "title": "Mention Extraction and Linking for SQL Query Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 33, 
    "authors": [
      "Jianqiang Ma", 
      "Zeyu Yan", 
      "Shuai Pang", 
      "Y. Zhang", 
      "Jianping Shen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1650", 
    "title": "Informative Image Captioning with External Sources of Information", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important \u201cinformativeness\u201d dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "Sanqiang Zhao", 
      "Piyush Sharma", 
      "Tomer Levinboim", 
      "Radu Soricut"
    ], 
    "topics": [
      "Information", 
      "Transformer", 
      "Entity", 
      "Multimodal interaction", 
      "Upstream (software development)", 
      "Precomputation", 
      "Encoder", 
      "Ground truth"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.424", 
    "title": "IGT2P: From Interlinear Glossed Texts to Paradigms", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language\u2019s inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Sarah Moeller", 
      "L. Liu", 
      "Changbing Yang", 
      "Katharina Kann", 
      "Mans Hulden"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1364", 
    "title": "Budgeted Policy Learning for Task-Oriented Dialogue Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 31, 
    "authors": [
      "Zhirui Zhang", 
      "Xiujun Li", 
      "Jianfeng Gao", 
      "Enhong Chen"
    ], 
    "topics": [
      "Scheduling (computing)", 
      "Interaction", 
      "Baseline (configuration management)", 
      "Algorithm", 
      "Simulation", 
      "Experiment", 
      "Sampling (signal processing)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.153", 
    "title": "ES-JUST at SemEval-2021 Task 7: Detecting and Rating Humor and Offensive Text Using Deep Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2021, 
    "venue": "SemEval@ACL/IJCNLP", 
    "references": 0, 
    "authors": [
      "Emran Al-Bashabsheh", 
      "S. A. Alasal"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.130", 
    "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and wellstructured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 42, 
    "authors": [
      "Hao Sun", 
      "Zhenru Lin", 
      "Chujie Zheng", 
      "Siyang Liu", 
      "Minlie Huang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/p15-2", 
    "title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2015, 
    "venue": "IJCNLP 2015", 
    "references": 0, 
    "authors": [
      "Chengqing Zong", 
      "M. Strube"
    ], 
    "topics": [
      "Natural language processing", 
      "Computational linguistics", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073012.1073074", 
    "title": "Incremental Construction of Compact Acyclic NFAs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents and analyzes an incremental algorithm for the construction of Acyclic Non-deterministic Finite-state Automata (NFA). Automata of this type are quite useful in computational linguistics, especially for storing lexicons. The proposed algorithm produces compact NFAs, i.e. NFAs that do not contain equivalent states. Unlike Deterministic Finite-state Automata (DFA), this property is not sufficient to ensure minimality, but still the resulting NFAs are considerably smaller than the minimal DFAs for the same languages.", 
    "year": 2001, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "K. Sgarbas", 
      "N. Fakotakis", 
      "G. Kokkinakis"
    ], 
    "topics": [
      "Directed acyclic graph", 
      "Algorithm", 
      "Nondeterministic finite automaton", 
      "Lexicon", 
      "DFA minimization", 
      "Computational linguistics", 
      "Finite-state machine", 
      "Automata theory", 
      "Experiment", 
      "Computation", 
      "Online and offline", 
      "Linux", 
      "Insertion sort", 
      "Deterministic finite automaton"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1006", 
    "title": "Syntax-based Rewriting for Simultaneous Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Divergent word order between languages causes delay in simultaneous machine translation. We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff. We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees. We apply the rules to reference translations to make their word order closer to the source language word order. On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "He He", 
      "Alvin Grissom II", 
      "John Morgan", 
      "Jordan L. Boyd-Graber", 
      "Hal Daum\u00e9"
    ], 
    "topics": [
      "Machine translation", 
      "Rewriting", 
      "Parallel text", 
      "Sparse voxel octree", 
      "Text corpus", 
      "Parse tree", 
      "Existential quantification", 
      "Baseline (configuration management)", 
      "Parsing", 
      "Architecture tradeoff analysis method"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-3009", 
    "title": "German and French Neural Supertagging Experiments for LTAG Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present ongoing work on data-driven parsing of German and French with Lexicalized Tree Adjoining Grammars. We use a supertagging approach combined with deep learning. We show the challenges of extracting LTAG supertags from the French Treebank, introduce the use of left- and right-sister-adjunction, present a neural architecture for the supertagger, and report experiments of n-best supertagging for French and German.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "Tatiana Bladier", 
      "Andreas van Cranenburgh", 
      "Younes Samih", 
      "Laura Kallmeyer"
    ], 
    "topics": [
      "Parsing", 
      "Experiment", 
      "Semantic role labeling", 
      "Deep learning", 
      "Treebank", 
      "Algorithm", 
      "Neural coding", 
      "Tree-adjoining grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3330", 
    "title": "LIMSI @ WMT\u201914 Medical Translation Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes LIMSI\u2019s submission to the first medical translation task at WMT\u201914. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of NCODE (n-gram-based) and MOSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building MOSES\u2019 phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by NCODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006).", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 39, 
    "authors": [
      "N. P\u00e9cheux", 
      "Li Gong", 
      "Quoc-Khanh Do", 
      "Benjamin Marie", 
      "Yulia Ivanishcheva", 
      "A. Allauzen", 
      "T. Lavergne", 
      "J. Niehues", 
      "A. Max", 
      "Fran\u00e7ois Yvon"
    ], 
    "topics": [
      "N-gram", 
      "Language model", 
      "Domain adaptation", 
      "Moses", 
      "Entity\u2013relationship model", 
      "Video post-processing", 
      "BLEU", 
      "Error analysis (mathematics)", 
      "Sampling (signal processing)", 
      "Biological anthropology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2140", 
    "title": "IBA-Sys at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the details of our system IBA-Sys that participated in SemEval Task: Fine-grained sentiment analysis on Financial Microblogs and News. Our system participated in both tracks. For microblogs track, a supervised learning approach was adopted and the regressor was trained using XgBoost regression algorithm on lexicon features. For news headlines track, an ensemble of regressors was used to predict sentiment score. One regressor was trained using TF-IDF features and another was trained using the n-gram features. The source code is available at Github 1", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 10, 
    "authors": [
      "Zarmeen Nasim"
    ], 
    "topics": [
      "Sentiment analysis", 
      "SemEval", 
      "Supervised learning", 
      "Parsing", 
      "Tf\u2013idf", 
      "InfiniBand", 
      "XGBoost", 
      "Algorithm", 
      "N-gram", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1329", 
    "title": "Exploring Numeracy in Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions. In this work, we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers, which is important for language understanding. Numbers are ubiquitous and frequently appear in text. Inspired by cognitive studies on how humans perceive numbers, we develop an analysis framework to test how well word embeddings capture two essential properties of numbers: magnitude (e.g. 3<4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 31, 
    "authors": [
      "Aakanksha Naik", 
      "Abhilasha Ravichander", 
      "C. Ros\u00e9", 
      "E. Hovy"
    ], 
    "topics": [
      "Word embedding", 
      "Cognitive science", 
      "Natural language processing", 
      "Essence", 
      "Natural language understanding", 
      "Experiment", 
      "Approximation algorithm", 
      "Pervasive informatics"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1107", 
    "title": "Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant Supervision", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Distant supervision (DS) is an important paradigm for automatically extracting relations. It utilizes existing knowledge base to collect examples for the relation we intend to extract, and then uses these examples to automatically generate the training data. However, the examples collected can be very noisy, and pose significant challenge for obtaining high quality labels. Previous work has made remarkable progress in predicting the relation from distant supervision, but typically ignores the temporal relations among those supervising instances. This paper formulates the problem of relation extraction with temporal reasoning and proposes a solution to predict whether two given entities participate in a relation at a given time spot. For this purpose, we construct a dataset called WIKI-TIME which additionally includes the valid period of a certain relation of two entities in the knowledge base. We propose a novel neural model to incorporate both the temporal information encoding and sequential reasoning. The experimental results show that, compared with the best of existing models, our model achieves better performance in both WIKI-TIME dataset and the well-studied NYT-10 dataset.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 28, 
    "authors": [
      "Jianhao Yan", 
      "Lin He", 
      "Ruqin Huang", 
      "Jian Li", 
      "Y. Liu"
    ], 
    "topics": [
      "Relationship extraction", 
      "Knowledge base", 
      "Entity", 
      "Code", 
      "Display resolution", 
      "Whole Earth 'Lectronic Link", 
      "Programming paradigm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1194", 
    "title": "Non-uniform Language Detection in Technical Writing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Technical writing in professional environments, such as user manual authoring, requires the use of uniform language. Nonuniform language detection is a novel task, which aims to guarantee the consistency for technical writing by detecting sentences in a document that are intended to have the same meaning within a similar context but use different words or writing style. This paper proposes an approach that utilizes text similarity algorithms at lexical, syntactic, semantic and pragmatic levels. Different features are extracted and integrated by applying a machine learning classification method. We tested our method using smart phone user manuals, and compared its performance against the state-ofthe-art methods in a related area. The experiments demonstrate that our approach achieves the upper bound performance for this task.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 61, 
    "authors": [
      "Weibo Wang", 
      "A. Mohammad", 
      "Aminul Islam", 
      "Axel J. Soto", 
      "E. Milios"
    ], 
    "topics": [
      "Language identification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.iwslt-1.8", 
    "title": "End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020", 
    "fields_of_study": [
      "Computer Science", 
      "Engineering"
    ], 
    "abstract": "This paper describes FBK\u2019s participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems\u2019 ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whether to work on custom segmentation or not. We used the provided segmentation. Our system is an end-to-end model based on an adaptation of the Transformer for speech data. Its training process is the main focus of this paper and it is based on: i) transfer learning (ASR pretraining and knowledge distillation), ii) data augmentation (SpecAugment, time stretch and synthetic data), iii)combining synthetic and real data marked as different domains, and iv) multi-task learning using the CTC loss. Finally, after the training with word-level knowledge distillation is complete, our ST models are fine-tuned using label smoothed cross entropy. Our best model scored 29 BLEU on the MuST-CEn-De test set, which is an excellent result compared to recent papers, and 23.7 BLEU on the same data segmented with VAD, showing the need for researching solutions addressing this specific data condition.", 
    "year": 2020, 
    "venue": "IWSLT", 
    "references": 40, 
    "authors": [
      "Marco Gaido", 
      "Mattia Antonino Di Gangi", 
      "M. Negri", 
      "M. Turchi"
    ], 
    "topics": [
      "Cross entropy", 
      "Synthetic data", 
      "BLEU", 
      "Convolutional neural network", 
      "Transformer", 
      "Smoothing", 
      "Voice activity detection", 
      "Computer multitasking", 
      "Online and offline", 
      "Multiple encryption", 
      "Source-to-source compiler", 
      "End-to-end principle", 
      "Time stretch analog-to-digital converter", 
      "Experiment", 
      "Test set", 
      "Ansari X Prize", 
      "Ground truth"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1365", 
    "title": "Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Formality text style transfer plays an important role in various NLP applications, such as non-native speaker assistants and child education. Early studies normalize informal sentences with rules, before statistical and neural models become a prevailing method in the field. While a rule-based system is still a common preprocessing step for formality style transfer in the neural era, it could introduce noise if we use the rules in a naive way such as data preprocessing. To mitigate this problem, we study how to harness rules into a state-of-the-art neural network that is typically pretrained on massive corpora. We propose three fine-tuning methods in this paper and achieve a new state-of-the-art on benchmark datasets", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 18, 
    "authors": [
      "Yunli Wang", 
      "Yu Wu", 
      "Lili Mou", 
      "Zhoujun Li", 
      "Wen-Han Chao"
    ], 
    "topics": [
      "Neural Networks"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.685", 
    "title": "Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 39, 
    "authors": [
      "Angeliki Lazaridou", 
      "Anna Potapenko", 
      "O. Tieleman"
    ], 
    "topics": [
      "Natural language", 
      "Language model", 
      "Multi-agent system", 
      "Synergy"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.378", 
    "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model, Optimus. A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks. We hope that our first pre-trained big VAE language model itself and results can help the NLP community renew the interests of deep generative models in the era of large-scale pre-training, and make these principled methods more practical.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 69, 
    "authors": [
      "Chunyuan Li", 
      "Xiang Gao", 
      "Yuan Li", 
      "Xiujun Li", 
      "Baolin Peng", 
      "Yizhe Zhang", 
      "Jianfeng Gao"
    ], 
    "topics": [
      "Autoencoder", 
      "Generative model", 
      "Language model", 
      "Text corpus", 
      "Natural language generation", 
      "Benchmark (computing)", 
      "Feature learning", 
      "Natural language understanding", 
      "Natural language processing", 
      "Variational principle", 
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3106", 
    "title": "Sentence Classification for Investment Rules Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the last years, compliance requirements for the banking sector have greatly augmented, making the current compliance processes difficult to maintain. Any process that allows to accelerate the identification and implementation of compliance requirements can help address this issues. The contributions of the paper are twofold: we propose a new NLP task that is the investment rule detection, and a group of methods identify them. We show that the proposed methods are highly performing and fast, thus can be deployed in production.", 
    "year": 2018, 
    "venue": "ECONLP@ACL", 
    "references": 23, 
    "authors": [
      "Youness Mansar", 
      "Sira Ferradans"
    ], 
    "topics": [
      "Rule (guideline)", 
      "Convolutional neural network", 
      "Requirement", 
      "Statistical classification", 
      "Preprocessor", 
      "Run time (program lifecycle phase)", 
      "Neural Network Simulation", 
      "Artificial neural network", 
      "NINL gene", 
      "Natural language processing", 
      "sentence", 
      "Mandatory - HL7DefinedRoseProperty", 
      "Inference", 
      "Page (document)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-0508", 
    "title": "Towards a computational model of grammaticalization and lexical diversity", 
    "fields_of_study": [
      "Mathematics"
    ], 
    "abstract": "Languages use dierent lexical inventories to encode information, ranging from small sets of simplex words to large sets of morphologically complex words. Grammaticalization theories argue that this variation arises as the outcome of diachronic processes whereby co-occurring words merge to one word and build up complex morphology. To model these processes we present a) a quantitative measure of lexical diversity and b) a preliminary computational model of changes in lexical diversity over several generations of merging higly frequent collocates.", 
    "year": 2014, 
    "venue": "", 
    "references": 24, 
    "authors": [
      "Christian Bentz", 
      "P. Buttery"
    ], 
    "topics": [
      "Computational model", 
      "Computation", 
      "Synchronicity", 
      "Programming Languages", 
      "Languages", 
      "Inventory", 
      "Refinement (computing)", 
      "Linguistics", 
      "Mandelbrot set", 
      "Theory", 
      "Zipf's law", 
      "Emoticon", 
      "Galaxy morphological classification", 
      "ENCODE", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980491.980551", 
    "title": "Linguistically Motivated Descriptive Term Selection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A linguistically motivated approach to indexing, that is the provision of descriptive terms for texts of any kind, is presented and illustrated. The approach is designed to achieve good, i.e. accurate and flexible, indexing by identifying index term sources in the meaning representations built by a powerful general purpose analyser, and providing a range of text expressions constituting semantic and syntactic variants for each term concept. Indexing is seen as a legitimate form of shallow text processing, but one requiring serious semantically based language processing, particularly to obtain well-founded complex terms, which is the main objective of the project described. The type of indexing strategy described is further seen as having utility in a range of applications environments.", 
    "year": 1984, 
    "venue": "COLING", 
    "references": 2, 
    "authors": [
      "K. Jones", 
      "J. Tait"
    ], 
    "topics": [
      "Mental representation", 
      "Information retrieval", 
      "Vii", 
      "Domain-specific language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119384.1119393", 
    "title": "Chinese Named Entity Recognition Combining Statistical Model wih Human Knowledge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on. Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation. In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well. In order to avoid data sparseness problem, we employ a back-off model and [Abstract contained text which could not be captured.], a Chinese thesaurus, to smooth the parameters in the model. The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively.", 
    "year": 2003, 
    "venue": "NER@ACL", 
    "references": 17, 
    "authors": [
      "Youzheng Wu", 
      "Jun Zhao", 
      "Bo Xu"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Statistical model", 
      "Named entity", 
      "Hybrid algorithm", 
      "Natural language processing", 
      "Question answering", 
      "Information retrieval", 
      "Text segmentation", 
      "Information processing", 
      "Thesaurus", 
      "Neural coding", 
      "Test data", 
      "Super Robot Monkey Team Hyperforce Go!"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1501", 
    "title": "Abstractive Text Summarization Based on Deep Learning and Semantic Content Generalization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. Initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. Subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. The overall approach is evaluated on two popular datasets with encouraging results.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 42, 
    "authors": [
      "Panagiotis Kouris", 
      "Georgios Alexandridis", 
      "A. Stafylopatis"
    ], 
    "topics": [
      "Deep learning", 
      "Natural language processing", 
      "Automatic summarization", 
      "Human-readable medium", 
      "Mathematical optimization", 
      "Vocabulary", 
      "Theory", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-0409", 
    "title": "Temporal and Aspectual Entailment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Inferences regarding \u201cJane\u2019s arrival in London\u201d from predications such as \u201cJane is going to London\u201d or \u201cJane has gone to London\u201d depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of \u201cgoing to London\u201d is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern embedding models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform inference on predications across aspectual types and tenses. We show that they encode a substantial amount of information relating to tense and aspect, but fail to consistently model inferences that require reasoning with these semantic properties.", 
    "year": 2019, 
    "venue": "IWCS", 
    "references": 71, 
    "authors": [
      "Thomas Kober", 
      "Sander Bijl de Vroe", 
      "Mark Steedman"
    ], 
    "topics": [
      "Mark Steedman", 
      "Logical connective", 
      "HTTPS", 
      "Wikipedia", 
      "Dropout (neural networks)", 
      "Word2vec", 
      "Direct numerical control", 
      "Feedback", 
      "Temporal logic", 
      "Activation function", 
      "Jane (software)", 
      "Substring", 
      "Logic programming", 
      "Convolutional neural network", 
      "Baseline (configuration management)", 
      "Embedded system", 
      "Bloomberg Terminal", 
      "Tea", 
      "Google News", 
      "Archive", 
      "Long short-term memory", 
      "Natural language processing", 
      "ENCODE", 
      "Software release life cycle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1310", 
    "title": "Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich table\u2019s representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimension\u2019s representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 23, 
    "authors": [
      "Heng Gong", 
      "X. Feng", 
      "Bing Qin", 
      "Ting Liu"
    ], 
    "topics": [
      "Encoder", 
      "Natural language generation", 
      "Table cell", 
      "Time series", 
      "BLEU", 
      "Baseline (configuration management)", 
      "Column (database)", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.334", 
    "title": "Compositional Demographic Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 55, 
    "authors": [
      "Charles F Welch", 
      "Jonathan K. Kummerfeld", 
      "Ver\u00f3nica P\u00e9rez-Rosas", 
      "Rada Mihalcea"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075222", 
    "title": "Translation with Cascaded Finite State Transducers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we discuss the use of cascaded finite state transducers for machine translation. A number of small, dedicated transducers is applied to convert sentence pairs from a bilingual corpus into generalized translation patterns. These patterns, together with the transducers are then used as a hierarchical translation memory for fully automatic translation. Results on the German--English VERBMOBIL corpus are given.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 10, 
    "authors": [
      "S. Vogel", 
      "H. Ney"
    ], 
    "topics": [
      "Finite-state transducer"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1094", 
    "title": "Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An interesting aspect of structured prediction is the evaluation of an output structure against the gold standard. Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions. In this paper, we trade off exact computation for enabling the use and study of more complex loss functions for coreference resolution. Most interestingly, we show that such functions can be (i) automatically learned also from controversial but commonly accepted coreference measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL-2012 setting shows the benefit of more expressive loss functions.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 22, 
    "authors": [
      "Iryna Haponchyk", 
      "Alessandro Moschitti"
    ], 
    "topics": [
      "Structured prediction", 
      "Loss function", 
      "Algorithm", 
      "RL (complexity)", 
      "Machine learning", 
      "Model selection", 
      "Approximation", 
      "Linear separability", 
      "Computation", 
      "Emoticon", 
      "OLGA (technology)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1126", 
    "title": "Context Sensitive Neural Lemmatization with Lematus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of context actually helps in these cases. We introduce Lematus, a lemmatizer based on a standard encoder-decoder architecture, which incorporates character-level sentence context. We evaluate its lemmatization accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. In both settings, we show that including context significantly improves results against a context-free version of the model. Context helps more for ambiguous words than for unseen words, though the latter has a greater effect on overall performance differences between languages. We also compare to three previous context-sensitive lemmatization systems, which all use pre-extracted edit trees as well as hand-selected features and/or additional sources of information such as tagged training data. Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the fulldata setting, and performs on par in the lowerresource setting.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 29, 
    "authors": [
      "Toms Bergmanis", 
      "S. Goldwater"
    ], 
    "topics": [
      "Lemmatisation", 
      "Context-sensitive grammar", 
      "Brown Corpus", 
      "Context-free language", 
      "Encoder", 
      "Word embedding", 
      "Kerrison Predictor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-2502", 
    "title": "Clustering-Based Article Identification in Historical Newspapers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents.", 
    "year": 2019, 
    "venue": "LaTeCH@NAACL-HLT", 
    "references": 24, 
    "authors": [
      "Martin Riedl", 
      "Daniela Betz", 
      "Sebastian Pad\u00f3"
    ], 
    "topics": [
      "Cluster analysis", 
      "Optical character recognition", 
      "Text segmentation", 
      "N-gram", 
      "Word embedding", 
      "Similarity measure", 
      "Grams", 
      "Emoticon"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.50", 
    "title": "Bayes-enhanced Lifelong Attention Networks for Sentiment Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The classic deep learning paradigm learns a model from the training data of a single task and the learned model is also tested on the same task. This paper studies the problem of learning a sequence of tasks (sentiment classification tasks in our case). After each sentiment classification task is learned, its knowledge is retained to help future task learning. Following this setting, we explore attention neural networks and propose a Bayes-enhanced Lifelong Attention Network (BLAN). The key idea is to exploit the generative parameters of naive Bayes to learn attention knowledge. The learned knowledge from each task is stored in a knowledge base and later used to build lifelong attentions. The constructed lifelong attentions are then used to enhance the attention of the network to help new task learning. Experimental results on product reviews from Amazon.com show the effectiveness of the proposed model.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 51, 
    "authors": [
      "Hao Wang", 
      "Shuai Wang", 
      "S. Mazumder", 
      "B. Liu", 
      "Yan Yang", 
      "Tianrui Li"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.nlp4posimpact-1.5", 
    "title": "Theano: A Greek-speaking conversational agent for COVID-19", 
    "fields_of_study": [
      "Medicine"
    ], 
    "abstract": "Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. CAs can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines. Additionally, Theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. The relevant, localized information that Theano provides, makes it a valuable tool for combating COVID-19 in Greece. Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.", 
    "year": 2021, 
    "venue": "NLP4POSIMPACT", 
    "references": 40, 
    "authors": [
      "Nikoletta Ventoura", 
      "Kosmas Palios", 
      "Yannis Vasilakis", 
      "G. Paraskevopoulos", 
      "Nassos Katsamanis", 
      "Vassilis Katsouros"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1446", 
    "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 70, 
    "authors": [
      "Logan Lebanoff", 
      "Kaiqiang Song", 
      "Fei Liu"
    ], 
    "topics": [
      "Encoder", 
      "Automatic summarization", 
      "Sentence extraction", 
      "Multi-document summarization", 
      "Algorithm", 
      "Relevance", 
      "Marginal model", 
      "Maximal set", 
      "World Wide Web", 
      "Multi-master replication"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.sdp-1.32", 
    "title": "Team MLU@CL-SciSumm20: Methods for Computational Linguistics Scientific Citation Linkage", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity scores to identify spans of the reference text for the given citance. In Task 1b, we use a logistic regression to classifying the discourse facets.", 
    "year": 2020, 
    "venue": "SDP", 
    "references": 9, 
    "authors": [
      "Rong Huang", 
      "Kseniia Krylova"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.86", 
    "title": "BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with Assembly Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a system submitted by team BigGreen to LCP 2021 for predicting the lexical complexity of English words in a given context. We assemble a feature engineering-based model with a deep neural network model founded on BERT. While BERT itself performs competitively, our feature engineering-based model helps in extreme cases, eg. separating instances of easy and neutral difficulty. Our handcrafted features comprise a breadth of lexical, semantic, syntactic, and novel phonological measures. Visualizations of BERT attention maps offer insight into potential features that Transformers models may learn when fine-tuned for lexical complexity prediction. Our ensembled predictions score reasonably well for the single word subtask, and we demonstrate how they can be harnessed to perform well on the multi word expression subtask too.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 41, 
    "authors": [
      "Aadil Islam", 
      "Weicheng Ma", 
      "Soroush Vosoughi"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3801", 
    "title": "Complex Event Extraction using DRUM", 
    "fields_of_study": [
      "Engineering", 
      "Computer Science"
    ], 
    "abstract": "Complex mechanisms, such as cell-signaling pathways, consist of many highly interconnected components, yet they are often described in disconnected fragmentary ways. The goal of DRUM (Deep Reader for Understanding Mechanisms) is to develop a system that can read papers and combine results of individual studies into a comprehensive explanatory model. A first step is to automatically extract relevant events and event relationships from the literature. This paper describes initial steps in extending an existing general deep language understanding system, TRIPS, to read biomedical papers. In a preliminary evaluation, our system was the best performing system among the participants, achieving results close to human expert performance. These results suggested that our system is viable for complex event extraction and, ultimately, understanding complex systems and mechanisms.", 
    "year": 2015, 
    "venue": "BioNLP@IJCNLP", 
    "references": 47, 
    "authors": [
      "James F. Allen", 
      "W. Beaumont", 
      "Lucian Galescu", 
      "C. Teng"
    ], 
    "topics": [
      "Drum memory", 
      "Complex systems", 
      "Natural language understanding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-2010", 
    "title": "MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents MED, the main system of the LMU team for the SIGMORPHON 2016 Shared Task on Morphological Reinflection as well as an extended analysis of how different design choices contribute to the final performance. We model the task of morphological reinflection using neural encoder-decoder models together with an encoding of the input as a single sequence of the morphological tags of the source and target form as well as the sequence of letters of the source form. The Shared Task consists of three subtasks, three different tracks and covers 10 different languages to encourage the use of language-independent approaches. MED was the system with the overall best performance, demonstrating our method generalizes well for the low-resource setting of the SIGMORPHON 2016 Shared Task.", 
    "year": 2016, 
    "venue": "SIGMORPHON", 
    "references": 25, 
    "authors": [
      "Katharina Kann", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "Encoder", 
      "Language-independent specification", 
      "Text-based (computing)", 
      "Programming Languages", 
      "Decoder Device Component", 
      "Encoder Device Component", 
      "MEDLINE", 
      "Track (course)", 
      "Random neural network", 
      "Choice Behavior"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1145", 
    "title": "ConnotationWordNet: Learning Connotation over the Word+Sense Network", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses. We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields, and present a loopy belief propagation algorithm for inference. The key aspect of our method is that it is the first unified approach that assigns the polarity of both wordand sense-level connotations, exploiting the innate bipartite graph structure encoded in WordNet. We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 39, 
    "authors": [
      "Jun Seok Kang", 
      "Song Feng", 
      "L. Akoglu", 
      "Yejin Choi"
    ], 
    "topics": [
      "WordNet", 
      "Lexicon", 
      "Belief propagation", 
      "Markov random field", 
      "Algorithm", 
      "Word sense", 
      "Casio Loopy", 
      "Markov chain", 
      "Software propagation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4335", 
    "title": "MVA: The Multimodal Virtual Assistant", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Multimodal Virtual Assistant (MVA) is an application that enables users to plan an outing through an interactive multimodal dialog with a mobile device. MVA demonstrates how a cloud-based multimodal language processing infrastructure can support mobile multimodal interaction. This demonstration will highlight incremental recognition, multimodal speech and gesture input, contextually-aware language understanding, and the targeted clarification of potentially incorrect segments within user input.", 
    "year": 2014, 
    "venue": "SIGDIAL Conference", 
    "references": 6, 
    "authors": [
      "Michael Johnston", 
      "J. Chen", 
      "Patrick Ehlen", 
      "Hyuckchul Jung", 
      "Jay Lieske", 
      "A. Reddy", 
      "Ethan Selfridge", 
      "Svetlana Stoyanchev", 
      "Brant Vasilieff", 
      "J. Wilpon"
    ], 
    "topics": [
      "Multimodal interaction", 
      "Model\u2013view\u2013adapter", 
      "Cloud computing", 
      "Mobile device", 
      "Natural language understanding", 
      "dialog"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-7903", 
    "title": "Which annotation scheme is more expedient to measure syntactic difficulty and cognitive demand?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper investigates which annotation scheme of dependency treebank is more congruent for the measurement of syntactic complexity and cognitive constraint of language materials. Two representatives of semanticand syntactic-oriented annotation schemes, the Universal Dependencies (UD) and the Surface-Syntactic Universal Dependencies (SUD), are under discussion. The results show that, on the one hand, natural languages based on both annotation schemes follow the universal linguistic law of Dependency Distance Minimization (DDM); on the other hand, according to the metric of Mean Dependency Distances (MDDs), the SUD annotation scheme that accords with traditional dependency syntaxes are more expedient to measure syntactic difficulty and cognitive demand. 1 Background and Motivation Dependency grammar deals with the syntactically related words, i.e. the governor and the dependent, within sentence structure (Heringer, 1993; Hudson, 1995; Liu, 2009). It can be dated back to the seminal work of El\u00e9ments de Syntaxe Structurale by Tesni\u00e8re (1959), and developed through different theories, including Word Grammar, Meaning-Text-Theory, Lexicase, etc. (e.g. Hudson,1984; Mel\u2019\u010duk, 1988; Starosta, 1988; Eroms, 2000). Thus far, there are many representations of dependency grammar. Figure 1 displays two typical dependency representations of one sample sentence We walked along the lake. Figure 1. Dependency Representations of One English Sentence We walked along the lake Based on UD and SUD Annotation Schemes. The dependency representation based on the Universal Dependencies (UD), as shown in Figure 1 (a), is one of the most eminent models by now under the framework of dependency grammar. It attempts at establishing a multilingual morphosyntactic scheme to annotate various languages in a consistent manner (Nivre, 2015; Osborne and Gerdes, 2019). Thus, the UD annotation scheme holds a semantic over See also http://universaldependencies.org/. syntactic criteria to put priorities to content words to maximize \u201ccrosslinguistic parallelism\u201d (Nivre, 2015; de Marneffe and Nivre, 2019). On the contrary, the Surface-Syntactic Universal Dependencies (SUD) annotation scheme, as shown in Figure 1 (b), follows the syntactic criteria to define not only the dependency labels but also the dependency links. It aims to make the annotation scheme close to the dependency traditions, like Meaning-Text-Theory (MTT) (Mel\u2019\u010duk, 1988), Word Grammar (Hudson, 1984), etc. Hence, the SUD annotation scheme is a syntactic-oriented dependency representation that seeks to promote the syntactic motivations (Gerdes et al., 2018; Osborne and Gerdes, 2019). Therefore, the UD and SUD annotation schemes signify two typical preferences of dependency grammar, one is semantic-oriented, and the other is syntactic-oriented. As shown in Figure 1, the linear sentence in both representations can be divided into several words; and the labelled arcs, directed from the governors to the dependents, represent different dependency types indicating the syntactic relations between elements within the sentence. Hence, the dependency representations indicate both the functional role of each word as well as the syntactic relations between different elements. More importantly, based on dependency representations, linguists have proposed several measurements for linguistic analysis. For one thing, dependency distance is defined as the linear distance of the governor and the dependent (Hudson, 1995). For another, the linear order of the governor and the dependent of each dependency type is referred to as dependency direction (Liu, 2010). When a governor appears before a dependent, the dependency direction is governor-initial or negative. Otherwise, it is governor-final or positive. For instance, in Figure 1 (a), the arc above the dependent we and the governor walked forms a governor-final relation; and the dependency distance between these two elements is 2 \u2013 1 = 1 (the number 2 and 1 in the subtraction represent the linear order of the governor and dependent, respectively). Detailed calculating method will be shown in Section 2. Therefore, the dependency representations and the measures of dependency relations are both explicit and clear-cut. This explains the reason why dependency treebanks, i.e. corpora with annotations (Abeill\u00e9, 2003), are widespread among linguists in big-data era. As a result, the variations and universals of human languages are explored and unveiled through statistical and mathematical tools (Hudson, 1995; Liu et al., 2017). What is noteworthy is that previous studies have shown that dependency distance is an important indicator in demonstrating the notion of syntactic complexity and cognitive demand (Hudson, 1995; Gibson, 2000; Liu, 2008). Under the framework of dependency grammar, Hudson (1995) characterized the definition of dependency distance based on the theories of memory decaying and short-term memory (e.g. Brown, 1958; Levy et al., 2013). The notion of syntactic difficulty and cognitive demand have been subsequently related to the linear distance between the governors and the dependents in cognitive science (Gibson, 1998; Hawkins, 2004). Based on a Romanian dependency treebank, Ferrer-i-Cancho (2004) hypothesized and proved that the mean distance of a sentence is minimized and constrained. These paved the way for Liu\u2019s (2008) empirical study on dependency distance which provides a viable treebank-based approach towards the metric of syntactic complexity and cognitive constraint. Afterwards, series of studies exploring the relationship between dependency distance and syntactic and cognitive benchmarks have been conducted (e.g. Jiang and Liu, 2015; Wang and Liu, 2017; Liu et al., 2017). These studies share some similarities. First, it is well-corroborated that the frequency of dependency distance decreases with the increase of the dependency distance, viz., the distribution of dependency distance follows the linguistic law of the Least Effort Principle (LEP) or Dependency Distance Minimization (DDM) (Zipf, 1965; Liu et al., 2017). Second, it is believed that the greater the dependency distance is, the more difficult the sentence structure (Gibson, 1998; Hiranuma, 1999; Liu et al., 2017). Thus, the arithmetic average of all dependency distances of one sentence or a treebank or the mean dependency distances (MDDs) (Liu, 2008) has been an important index of memory burden, demonstrating the syntactic complexity and cognitive demand of the language concerned (Hudson, 1995; Liu et al., 2017). Previous studies have shown that there are several factors that have effects on the measurement of dependency distance of a sentence, including sentence length, genre, chunking, language type, grammar, annotation scheme and so forth (e.g. Jiang and Liu, 2015; Wang and Liu, 2017; Lu et al., 2016; Hiranuma, 1999; Liu and Xu, 2012; Gildea and Temperley, 2010). Most of these factors have been wellinvestigated, however, the factor of annotation scheme has rarely been studied. Liu et al. (2009), for instance, investigated Chinese syntactic and typological properties based on five different Chinese See also https://gitlab.inria.fr/grew/SUD. treebanks with different genres and annotation schemes, yet the treebanks adopted with different annotation schemes were used to avoid the corpus influences to ensure a reliable conclusion. Hence, the question as to the effects of annotation scheme on the distribution of dependency distance and MDD remains open. Moreover, investigations into the benchmark of syntactic complexity and cognitive demand introduced above were primarily based on traditionally syntactic-oriented dependency models, for instance, the Stanford Typed Dependencies annotation scheme (de Marneffe and Manning, 2008) or other annotation schemes that specifically designed for each individual language. Thus, there is no consistency among different treebanks. In addition, although there are some qualitative investigations on the distinctions between the UD annotation scheme and various traditional syntactic-oriented annotation schemes (e.g. Osborne and Maxwell, 2015), and the existing studies also include some empirical studies focusing primarily on the consistently annotated UD scheme (e.g. Chen and Gerdes, 2017; 2018), it is still of our interest that, compared with those based on consistently annotated traditionally syntactic-oriented schemes, whether linguistic analysis based on the UD annotation scheme can still function as a metric of syntactic difficulty and cognitive demand, and if it can, what are the reasons for these distinctions? Therefore, the deficiency of investigations into annotation scheme of treebanks leads to the inquiry of current study. We attempt at making comparisons of dependency distances based on two different annotation schemes, UD and SUD. Aimed to address the issues mentioned above, the following questions are under discussion based on UD and SUD treebanks: (1) Will the probability distribution of dependency distances of natural texts change when they are based on different annotation schemes? Do they still follow the linguistic law of DDM? (2) Based on MDDs, which annotation scheme is more congruent for the measurement of syntactic complexity and cognitive demand? (3) Which dependency types account most for the distinctions between UD and SUD annotation schemes? 2 Materials and Methods Taking English language as an example, we adopt the Georgetown University Multilayer Corpus (GUM) (Zeldes, 2017) in UD 2.2 and SUD 2.2 projects. Both versions of the treebank are consisted of seven genres, viz. academic writing, biographies, fiction, interviews, news stories, travel guides and how-to guides. Since the treebanks are balanced in term of genres, it would better demonstrate the general features of the probability distribution of dependency distance when we adopt different annotation schemes. To measure the effectiveness of MDDs as a metric of syntactic dif", 
    "year": 2019, 
    "venue": "", 
    "references": 45, 
    "authors": [
      "Jianwei Yan", 
      "Haitao Liu"
    ], 
    "topics": [
      "Treebank", 
      "Dependency grammar", 
      "Hudson", 
      "Urban Dictionary", 
      "Word grammar", 
      "Lexicase", 
      "Cognitive science", 
      "Viz: The Computer Game", 
      "Big data", 
      "Natural language", 
      "Parallel computing", 
      "Angular defect", 
      "Whole Earth 'Lectronic Link", 
      "Text corpus", 
      "Zipf's law", 
      "Emoticon", 
      "Model-driven engineering", 
      "Granular computing", 
      "Entity\u2013relationship model", 
      "Word lists by frequency", 
      "Benchmark (computing)", 
      "Shallow parsing", 
      "LU decomposition", 
      "Maxwell (microarchitecture)", 
      "Meaning\u2013text theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.404", 
    "title": "We Can Detect Your Bias: Predicting the Political Ideology of News Articles", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology -left, center, or right-, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 37, 
    "authors": [
      "R. Baly", 
      "Giovanni Da San Martino", 
      "James R. Glass", 
      "Preslav Nakov"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1289", 
    "title": "What Gets Echoed? Understanding the \"Pointers\" in Explanations of Persuasive Arguments", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 57, 
    "authors": [
      "D. Atkinson", 
      "Kumar Bhargav Srinivasan", 
      "Chenhao Tan"
    ], 
    "topics": [
      "Pointer (computer programming)", 
      "Persuasive technology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-0816", 
    "title": "An Approach to Take Multi-Word Expressions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This research discusses preliminary efforts to expand the coverage of the PropBank lexicon to multi-word and idiomatic expressions, such as take one for the team. Given overwhelming numbers of such expressions, an efficient way for increasing coverage is needed. This research discusses an approach to adding multiword expressions to the PropBank lexicon in an effective yet semantically rich fashion. The pilot discussed here uses double annotation of take multi-word expressions, where annotations provide information on the best strategy for adding the multi-word expression to the lexicon. This work represents an important step for enriching the semantic information included in the PropBank corpus, which is a valuable and comprehensive resource for the field of Natural Language Processing.", 
    "year": 2014, 
    "venue": "MWE@EACL", 
    "references": 13, 
    "authors": [
      "Claire Bonial", 
      "Meredith Green", 
      "Jenette Preciado", 
      "Martha Palmer"
    ], 
    "topics": [
      "PropBank", 
      "Lexicon", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1065", 
    "title": "AMR Parsing with an Incremental Joint Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identification and relation identification simultaneously. To this end, we first develop a novel componentwise beam search algorithm for relation identification in an incremental fashion, and then incorporate the decoder into a unified framework based on multiple-beam search, which allows for the bi-directional information flow between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Junsheng Zhou", 
      "F. Xu", 
      "H. Uszkoreit", 
      "Weiguang Qu", 
      "Ran Li", 
      "Y. Gu"
    ], 
    "topics": [
      "Parsing", 
      "Adaptive Multi-Rate audio codec", 
      "Beam search", 
      "Semantic role labeling", 
      "Word-sense disambiguation", 
      "Propagation of uncertainty", 
      "Word sense", 
      "Search algorithm", 
      "Unified Framework", 
      "Software propagation", 
      "Information flow (information theory)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5035", 
    "title": "Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Chemical patents are an important resource for chemical information. However, few chemical Named Entity Recognition (NER) systems have been evaluated on patent documents, due in part to their structural and linguistic complexity. In this paper, we explore the NER performance of a BiLSTM-CRF model utilising pre-trained word embeddings, character-level word representations and contextualized ELMo word representations for chemical patents. We compare word embeddings pre-trained on biomedical and chemical patent corpora. The effect of tokenizers optimized for the chemical domain on NER performance in chemical patents is also explored. The results on two patent corpora show that contextualized word representations generated from ELMo substantially improve chemical NER performance w.r.t. the current state-of-the-art. We also show that domain-specific resources such as word embeddings trained on chemical patents and chemical-specific tokenizers, have a positive impact on NER performance.", 
    "year": 2019, 
    "venue": "BioNLP@ACL", 
    "references": 46, 
    "authors": [
      "Zenan Zhai", 
      "Dat Quoc Nguyen", 
      "S. Akhondi", 
      "Camilo Thorne", 
      "Christian Druckenbrodt", 
      "Trevor Cohn", 
      "M. Gregory", 
      "Karin M. Verspoor"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Word embedding", 
      "Named entity", 
      "Text corpus", 
      "F1 score", 
      "Cheminformatics", 
      "Word-sense disambiguation", 
      "Information", 
      "Downstream (software development)", 
      "Chemical database", 
      "STING", 
      "Error analysis (mathematics)", 
      "Software patent", 
      "Conditional random field", 
      "GUID Partition Table", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1112", 
    "title": "Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 23, 
    "authors": [
      "M. Tkatchenko", 
      "Chong Cher Chia", 
      "Hady W. Lauw"
    ], 
    "topics": [
      "Word embedding", 
      "Natural language processing", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1034678.1034727", 
    "title": "Two Accounts of Scope Availability and Semantic Underspecification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a formal system for representing the available readings of sentences displaying quantifier scope ambiguity, in which partial scopes may be expressed. We show that using a theory of scope availability based upon the function-argument structure of a sentence allows a deterministic, polynomial time test for the availability of a reading, while solving the same problem within theories based on the well-formedness of sentences in the meaning language has been shown to be NP-hard.", 
    "year": 1999, 
    "venue": "ACL", 
    "references": 15, 
    "authors": [
      "A. Willis", 
      "S. Manandhar"
    ], 
    "topics": [
      "Theory", 
      "Formal system", 
      "Polynomial", 
      "Quantifier (logic)", 
      "Whole Earth 'Lectronic Link", 
      "Time complexity"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.428", 
    "title": "Quantifying Intimacy in Language", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson's r=0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 135, 
    "authors": [
      "Jiaxin Pei", 
      "David Jurgens"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2504", 
    "title": "Document-Level Machine Translation Evaluation with Gist Consistency and Text Cohesion", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Current Statistical Machine Translation (SMT) is significantly affected by Machine Translation (MT) evaluation metric. Nowadays the emergence of document-level MT research increases the demand for corresponding evaluation metric. This paper proposes two superior yet low-cost quantitative objective methods to enhance traditional MT metric by modeling document-level phenomena from the perspectives of gist consistency and text cohesion. The experimental results show the proposed metrics can obtain better correlation with human judgments than traditional metrics on evaluating document-level translation quality.", 
    "year": 2015, 
    "venue": "DiscoMT@EMNLP", 
    "references": 36, 
    "authors": [
      "Zhengxian Gong", 
      "M. Zhang", 
      "Guodong Zhou"
    ], 
    "topics": [
      "Topic model", 
      "Statistical machine translation", 
      "GiST", 
      "Meteor", 
      "BLEU", 
      "Emergence", 
      "Off topic", 
      "Markov chain", 
      "Hybrid kernel"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1036", 
    "title": "INESC-ID at SemEval-2016 Task 4-A: Reducing the Problem of Out-of-Embedding Words", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the INESC-ID system for the 2016 edition of SemEval Twitter Sentiment Analysis shared task (subtask 4-A). The system was based on the Non-Linear Sub-space Embedding (NLSE) model developed for last year\u2019s competition. This model trains a projection of pre-trained embeddings into a small subspace using the supervised data available. Despite its simplicity, the system attained performances comparable to the best systems of last edition with no need for feature engineering. One limitation of this model was the assumption that a pre-trained embedding was available for every word. In this paper, we investigated different strategies to overcome this limitation by exploiting character-level embeddings and learning representations for out-ofembedding vocabulary words. The resulting approach outperforms our previous model by a relatively small margin, while still attaining strong results and a consistent good performance across all the evaluation datasets.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 9, 
    "authors": [
      "Silvio Amir", 
      "Ram\u00f3n Fern\u00e1ndez Astudillo", 
      "Wang Ling", 
      "M\u00e1rio J. Silva", 
      "I. Trancoso"
    ], 
    "topics": [
      "SemEval", 
      "Overfitting", 
      "Sentiment analysis", 
      "Feature engineering", 
      "Supervised learning", 
      "Performance", 
      "Vocabulary", 
      "ID-WSF", 
      "Loss function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.290", 
    "title": "Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on data quality and coverage. This is quite challenging since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and inconsistency of described facts. In this paper, we propose kens, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs.KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to com-bine prediction results from multiple language-specific embeddings, for which multiple en-semble techniques are investigated. Experiments on the basis of five real-world language-specific KGs show that, by effectively identifying and leveraging complementary knowledge, KEnS consistently improves state-of-the-art methods on KG completion.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 46, 
    "authors": [
      "X. Chen", 
      "Muhao Chen", 
      "Changjun Fan", 
      "Ankith Uppunda", 
      "Yizhou Sun", 
      "C. Zaniolo"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.78", 
    "title": "To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Part-of-Speech (POS) tags are routinely included as features in many NLP tasks. However, the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS. This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture. Each task is tested twice on identical data except for the presence/absence of POS tags, using published data in ten high- to low-resource languages or unpublished linguistic field data in five low-resource languages. We find that the presence or absence of POS tags does not have a significant bearing on performance. In joint segmentation and glossing, the largest average difference is an .09 improvement in F1-scores by removing POS tags. In reinflection, the greatest average difference is 1.2% in accuracy for published data and 5% for unpublished and noisy field data.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 60, 
    "authors": [
      "Sarah Moeller", 
      "Ling Liu", 
      "Mans Hulden"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1169", 
    "title": "Joint A* CCG Parsing and Semantic Role Labelling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Joint models of syntactic and semantic parsing have the potential to improve performance on both tasks\u2014but to date, the best results have been achieved with pipelines. We introduce a joint model using CCG, which is motivated by the close link between CCG syntax and semantics. Semantic roles are recovered by labelling the deep dependency structures produced by the grammar. Furthermore, because CCG is lexicalized, we show it is possible to factor the parsing model over words and introduce a new A parsing algorithm\u2014 which we demonstrate is faster and more accurate than adaptive supertagging. Our joint model is the first to substantially improve both syntactic and semantic accuracy over a comparable pipeline, and also achieves state-of-the-art results for a nonensemble semantic role labelling model.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 47, 
    "authors": [
      "M. Lewis", 
      "Luheng He", 
      "Luke Zettlemoyer"
    ], 
    "topics": [
      "Parsing", 
      "Semantic role labeling", 
      "Children's Cancer Group", 
      "Combinatory categorial grammar", 
      "Pipeline (computing)", 
      "emotional dependency", 
      "A* search algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1005", 
    "title": "Distinguishing Past, On-going, and Future Events: The EventStatus Corpus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Determining whether a major societal event has already happened, is still on-going, or may occur in the future is crucial for event prediction, timeline generation, and news summarization. We introduce a new task and a new corpus, EventStatus, which has 4500 English and Spanish articles about civil unrest events labeled as PAST, ON-GOING, or FUTURE. We show that the temporal status of these events is difficult to classify because local tense and aspect cues are often lacking, time expressions are insufficient, and the linguistic contexts have rich semantic compositionality. We explore two approaches for event status classification: (1) a feature-based SVM classifier augmented with a novel induced lexicon of future-oriented verbs, such as \u201cthreatened\u201d and \u201cplanned\u201d, and (2) a convolutional neural net. Both types of classifiers improve event status recognition over a state-of-the-art TempEval model, and our analysis offers linguistic insights into the semantic compositionality challenges for this new task.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 46, 
    "authors": [
      "Ruihong Huang", 
      "Ignacio Cases", 
      "Dan Jurafsky", 
      "C. Condoravdi", 
      "E. Riloff"
    ], 
    "topics": [
      "Information extraction", 
      "Artificial neural network", 
      "Timeline", 
      "Lexicon", 
      "Unrest", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-3019", 
    "title": "CroVeWA: Crosslingual Vector-Based Writing Assistance", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present an interactive web-based writing assistance system that is based on recent advances in crosslingual compositional distributed semantics. Given queries in Japanese or English, our system can retrieve semantically related sentences from high quality English corpora. By employing crosslingually constrained vector space models to represent phrases, our system naturally sidesteps several difficulties that would arise from direct word-to-text matching, and is able to provide novel functionality like the visualization of semantic relationships between phrases interlingually and intralingually.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 13, 
    "authors": [
      "Hubert Soyer", 
      "Goran Topic", 
      "Pontus Stenetorp", 
      "Akiko Aizawa"
    ], 
    "topics": [
      "Text corpus", 
      "Web application", 
      "Display resolution"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1160", 
    "title": "CoastalCPH at SemEval-2016 Task 11: The importance of designing your Neural Networks right", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present two methods for the automatic detection of complex words in context as perceived by non-native English readers, for the SemEval 2016 Task 11 on Complex Word Identification (Paetzold and Specia, 2016). The submitted systems exploit the same set of features, but are highly disparate in (i) their learning algorithm and (ii) their angle on the learning objective, where especially the latter presents an effort to account for the sparsity of positive instances in the data as well as the large disparity between the distributions of positive instances in the training and test data. We further present valuable insights that we gained during intensive and extensive posttask experiments. Those revealed that despite poor results in the task, our neural network approach is competitive with the systems achieving the best results. The central contribution of this paper is therefore a demonstration of the aptitude of deep neural networks for the task of identifying complex words.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 17, 
    "authors": [
      "Joachim Bingel", 
      "Natalie Schluter", 
      "H\u00e9ctor Mart\u00ednez Alonso"
    ], 
    "topics": [
      "SemEval", 
      "Deep learning", 
      "Jargon", 
      "Word embedding", 
      "Logistic regression", 
      "aptitude", 
      "Experiment", 
      "Test data", 
      "Sparse matrix", 
      "Algorithm", 
      "Neural Networks", 
      "Binocular disparity", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-6633", 
    "title": "Automatic Generation of Student Report Cards", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Learning Analytics Report Card (LARC) is a pilot system which takes time-series data from a student\u2019s course-related activity in a Virtual Learning Environment and generates automatic textual summaries in real time. Students are able to generate reports as often as they like, and to choose which aspects of their behaviour are included in each report. As well as rating a student\u2019s scores against set standards, the generated texts make comparisons with the individual student\u2019s previous behaviour from the same course, and with the average scores of their student cohort. In addition, we carry out sentiment analysis on the student\u2019s forum posts, and generate a summary using quantifiers. We report some student reactions to initial trials of the system.", 
    "year": 2016, 
    "venue": "INLG", 
    "references": 20, 
    "authors": [
      "Amy Isard", 
      "Jeremy Knox"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Time series", 
      "Report", 
      "UNIVAC LARC"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2105", 
    "title": "Automatic Keyword Extraction on Twitter", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 49, 
    "authors": [
      "Lu\u00eds Marujo", 
      "Wang Ling", 
      "I. Trancoso", 
      "Chris Dyer", 
      "A. Black", 
      "A. Gershman", 
      "David Martins de Matos", 
      "J. Neto", 
      "J. Carbonell"
    ], 
    "topics": [
      "Keyword extraction", 
      "Crowdsourcing", 
      "Feature extraction", 
      "Unsupervised learning", 
      "Baseline (configuration management)", 
      "Natural language processing", 
      "Controller (control theory)", 
      "Automatic summarization", 
      "International Symposium on Fundamentals of Computation Theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-056-4_027", 
    "title": "Porting Multilingual Morphological Resources to OntoLex-Lemon", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe work consisting in porting various morphological resources to the OntoLex-Lemon model. A main objective of this work is to offer a uniform representation of different morphological data sets in order to be able to compare and interlink multilingual resources and to cross-check and interlink or merge the content of morphological resources of one and the same language. The results of our work will be published on the Linguistic Linked Open Data cloud.", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 18, 
    "authors": [
      "Thierry Declerck", 
      "Stefania Racioppa"
    ], 
    "topics": [
      "Linguistic Linked Open Data", 
      "DBpedia", 
      "Wikidata", 
      "Linked data", 
      "Knowledge Graph", 
      "Tag cloud"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3714", 
    "title": "Integrating Query Performance Prediction in Term Scoring for Diachronic Thesaurus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A diachronic thesaurus is a lexical resource that aims to map between modern terms and their semantically related terms in earlier periods. In this paper, we investigate the task of collecting a list of relevant modern target terms for a domain-specific diachronic thesaurus. We propose a supervised learning scheme, which integrates features from two closely related fields: Terminology Extraction and Query Performance Prediction (QPP). Our method further expands modern candidate terms with ancient related terms, before assessing their corpus relevancy with QPP measures. We evaluate the empirical benefit of our method for a thesaurus for a diachronic Jewish corpus.", 
    "year": 2015, 
    "venue": "LaTeCH@ACL", 
    "references": 34, 
    "authors": [
      "Chaya Liebeskind", 
      "Ido Dagan"
    ], 
    "topics": [
      "Thesaurus", 
      "Performance prediction", 
      "Terminology extraction", 
      "Query expansion", 
      "Supervised learning", 
      "Relevance", 
      "Algorithm", 
      "Test engineer"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-2005", 
    "title": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 27, 
    "authors": [
      "Xiaojing Liu", 
      "F. Gao", 
      "Qiong Zhang", 
      "Huasha Zhao"
    ], 
    "topics": [
      "Convolution", 
      "Information extraction", 
      "Document classification", 
      "Named-entity recognition", 
      "Experiment", 
      "Conditional random field", 
      "Declaration (computer programming)", 
      "Multimodal interaction", 
      "ENCODE", 
      "Code segment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4306", 
    "title": "Adapting to Personality Over Time: Examining the Effectiveness of Dialogue Policy Progressions in Task-Oriented Interaction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper explores dialogue adaptation over repeated interactions within a taskoriented human tutorial dialogue corpus. We hypothesize that over the course of four tutorial dialogue sessions, tutors adapt their strategies based on the personality of the student, and in particular to student introversion or extraversion. We model changes in strategy over time and use them to predict how effectively the tutorial interactions support student learning. The results suggest that students leaning toward introversion learn more effectively with a minimal amount of interruption during task activity, but occasionally require a tutor prompt before voicing uncertainty; on the other hand, students tending toward extraversion benefit significantly from increased interaction, particularly through tutor prompts for reflection on task activity. This line of investigation will inform the development of future user-adaptive dialogue systems.", 
    "year": 2014, 
    "venue": "SIGDIAL Conference", 
    "references": 37, 
    "authors": [
      "A. Vail", 
      "K. Boyer"
    ], 
    "topics": [
      "Dialog system", 
      "Interaction", 
      "Interrupt", 
      "Openness", 
      "Next-generation network", 
      "Systems design", 
      "Cognitive tutor", 
      "Computer science"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.42", 
    "title": "SemEval-2021 Task 10: Source-Free Domain Adaptation for Semantic Processing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the Source-Free Domain Adaptation shared task held within SemEval-2021. The aim of the task was to explore adaptation of machine-learning models in the face of data sharing constraints. Specifically, we consider the scenario where annotations exist for a domain but cannot be shared. Instead, participants are provided with models trained on that (source) data. Participants also receive some labeled data from a new (development) domain on which to explore domain adaptation algorithms. Participants are then tested on data representing a new (target) domain. We explored this scenario with two different semantic tasks: negation detection (a text classification task) and time expression recognition (a sequence tagging task).", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 26, 
    "authors": [
      "Egoitz Laparra", 
      "X. Su", 
      "Yiyun Zhao", 
      "\u00d6zlem Uzuner", 
      "Timothy Miller", 
      "S. Bethard"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.40", 
    "title": "Multiscale Collaborative Deep Models for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 48, 
    "authors": [
      "Xiangpeng Wei", 
      "Heng Yu", 
      "Yue Hu", 
      "Yue Zhang", 
      "Rongxiang Weng", 
      "Weihua Luo"
    ], 
    "topics": [
      "Neural machine translation", 
      "BLEU", 
      "Encoder", 
      "Backpropagation", 
      "Gradient", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2027", 
    "title": "ConSSED at SemEval-2019 Task 3: Configurable Semantic and Sentiment Emotion Detector", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our system participating in the SemEval-2019 Task 3: EmoContext: Contextual Emotion Detection in Text. The goal was to for a given textual dialogue, i.e. a user utterance along with two turns of context, identify the emotion of user utterance as one of the emotion classes: Happy, Sad, Angry or Others. Our system: ConSSED is a configurable combination of semantic and sentiment neural models. The official task submission achieved a micro-average F1 score of 75.31 which placed us 16th out of 165 participating systems.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 17, 
    "authors": [
      "Rafal Poswiata"
    ], 
    "topics": [
      "F1 score"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2706", 
    "title": "Identification and Disambiguation of Lexical Cues of Rhetorical Relations across Different Text Genres", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Lexical cues are linguistic expressions that can signal the presence of a rhetorical relation. However, such cues can be ambiguous as they may signal more than one relation or may not always function as a relation indicator. In this study, we first conduct a corpus-based analysis to derive a set of n-grams as potential lexical cues. These cues are then utilized in graph-based probabilistic models to determine the syntactic context in which the cue is signaling the presence of a particular relation. Evaluation results are reported for various cues of the CIRCUMSTANCE relation, confirming the value of syntactic features for the task of cue disambiguation in the context of Rhetorical Structure Theory. Moreover, using a graph to encode syntactic information is shown to be a more generalizable and effective approach compared to the direct usage of syntactic features.", 
    "year": 2015, 
    "venue": "LSDSem@EMNLP", 
    "references": 35, 
    "authors": [
      "T. Khazaei", 
      "Lu Xiao", 
      "Robert E. Mercer"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Knowledge management", 
      "Graph (discrete mathematics)", 
      "Tf\u2013idf", 
      "Algorithm", 
      "Design rationale", 
      "N-gram", 
      "Lexicon", 
      "Corpus linguistics", 
      "Biconnected component", 
      "Experiment", 
      "Grams", 
      "Word lists by frequency", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.conll-1.30", 
    "title": "Word associations and the distance properties of context-aware word embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.", 
    "year": 2020, 
    "venue": "CONLL", 
    "references": 32, 
    "authors": [
      "M. A. Rodriguez", 
      "Paola Merlo"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-2020", 
    "title": "A Multilinear Approach to the Unsupervised Learning of Morphology", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel approach to the unsupervised learning of morphology. In particular, we use a Multiple Cause Mixture Model (MCMM), a type of autoencoder network consisting of two node layers\u2014hidden and surface\u2014and a matrix of weights connecting hidden nodes to surface nodes. We show that an MCMM shares crucial graphical properties with autosegmental morphology. We argue on the basis of this graphical similarity that our approach is theoretically sound. Experiment results on Hebrew data show that this theoretical soundness bears out in practice.", 
    "year": 2016, 
    "venue": "SIGMORPHON", 
    "references": 26, 
    "authors": [
      "A. Meyer", 
      "Markus Dickinson"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Galaxy morphological classification", 
      "Mathematical morphology", 
      "Mixture model", 
      "Autoencoder", 
      "Microsoft Outlook for Mac", 
      "Orthographic projection", 
      "Computational complexity theory", 
      "Parallel computing", 
      "Experiment", 
      "Morphological pattern", 
      "Rewriting", 
      "Map"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.214", 
    "title": "Cross-Lingual Word Embedding Refinement by \ud835\udcc11 Norm Optimisation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the l2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. l1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the l1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.", 
    "year": 2021, 
    "venue": "NAACL 2021", 
    "references": 34, 
    "authors": [
      "Xutan Peng", 
      "Chenghua Lin", 
      "Mark Stevenson"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3622", 
    "title": "The Columbia System in the QALB-2014 Shared Task on Arabic Error Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The QALB-2014 shared task focuses on correcting errors in texts written in Modern Standard Arabic. In this paper, we describe the Columbia University entry in the shared task. Our system consists of several components that rely on machinelearning techniques and linguistic knowledge. We submitted three versions of the system: these share several core elements but each version also includes additional components. We describe our underlying approach and the special aspects of the different versions of our submission. Our system ranked first out of nine participating teams.", 
    "year": 2014, 
    "venue": "ANLP@EMNLP", 
    "references": 22, 
    "authors": [
      "A. Rozovskaya", 
      "Nizar Habash", 
      "R. Eskander", 
      "N. Farra", 
      "W. Salloum"
    ], 
    "topics": [
      "Error detection and correction", 
      "Columbia (supercomputer)", 
      "QR code"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.184", 
    "title": "Simultaneous Machine Translation with Visual Context", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 50, 
    "authors": [
      "Ozan Caglayan", 
      "Julia Ive", 
      "Veneta Haralampieva", 
      "P. Madhyastha", 
      "Lo\u00efc Barrault", 
      "Lucia Specia"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.452", 
    "title": "Answer-driven Deep Question Generation based on Reinforcement Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Deep question generation (DQG) aims to generate complex questions through reasoning over multiple documents. The task is challenging and underexplored. Existing methods mainly focus on enhancing document representations, with little attention paid to the answer information, which may result in the generated question not matching the answer type and being answerirrelevant. In this paper, we propose an Answer-driven Deep Question Generation (ADDQG) model based on the encoder-decoder framework. The model makes better use of the target answer as a guidance to facilitate question generation. First, we propose an answer-aware initialization module with a gated connection layer which introduces both document and answer information to the decoder, thus helping to guide the choice of answer-focused question words. Then a semantic-rich fusion attention mechanism is designed to support the decoding process, which integrates the answer with the document representations to promote the proper handling of answer information during generation. Moreover, reinforcement learning is applied to integrate both syntactic and semantic metrics as the reward to enhance the training of the ADDQG. Extensive experiments on the HotpotQA dataset show that ADDQG outperforms state-of-the-art models in both automatic and human evaluations.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 45, 
    "authors": [
      "Liuyin Wang", 
      "Zi-Han Xu", 
      "Zibo Lin", 
      "Haitao Zheng", 
      "Ying Shen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.blackboxnlp-1.4", 
    "title": "What Happens To BERT Embeddings During Fine-tuning?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques\u2014supervised probing, unsupervised similarity analysis, and layer-based ablations\u2014we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.", 
    "year": 2020, 
    "venue": "BLACKBOXNLP", 
    "references": 78, 
    "authors": [
      "Amil Merchant", 
      "Elahe Rahimtoroghi", 
      "Ellie Pavlick", 
      "Ian Tenney"
    ], 
    "topics": [
      "Heuristic (computer science)", 
      "RSA (cryptosystem)", 
      "Catastrophic interference", 
      "Parsing", 
      "Downstream (software development)", 
      "Supervised learning", 
      "Linear classifier"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlposs-1.20", 
    "title": "User-centered & Robust NLP OSS: Lessons Learned from Developing & Maintaining RSMTool", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "For the last 5 years, we have developed and maintained RSMTool \u2013 an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its cross-disciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper.", 
    "year": 2020, 
    "venue": "NLPOSS", 
    "references": 27, 
    "authors": [
      "Nitin Madnani", 
      "Anastassia Loukina"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980691.980734", 
    "title": "Learning Intonation Rules for Concept to Speech Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we report on an effort to provide a general-purpose spoken language generation tool for Concept-to-Speech (CTS) applications by extending a widely used text generation package, FUF/SURGE, with an intonation generation component. As a first step, we applied machine learning and statistical models to learn intonation rules based on the semantic and syntactic information typically represented in FUF/SURGE at the sentence level. The results of this study are a set of intonation rules learned automatically which can be directly implemented in our intonation generation component. Through 5-fold cross-validation, we show that the learned rules achieve around 90% accuracy for break index, boundary tone and phrase accent and 80% accuracy for pitch accent. Our study is unique in its use of features produced by language generation to control intonation. The methodology adopted here can be employed directly when more discourse/pragmatic information is to be considered in the future.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 115, 
    "authors": [
      "Shimei Pan", 
      "K. McKeown"
    ], 
    "topics": [
      "Natural language generation", 
      "Speech corpus", 
      "Pragmatic theory of information", 
      "Machine learning", 
      "Experiment", 
      "Cross-validation (statistics)", 
      "NetWare File System", 
      "Statistical model", 
      "Hirschberg's algorithm", 
      "Learning to rank", 
      "Baseline (configuration management)", 
      "Carpal tunnel syndrome", 
      "General-purpose markup language", 
      "Columbia (supercomputer)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1242", 
    "title": "Think Visually: Question Answering through Virtual Imagery", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we study the problem of geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture designed for answering questions that admit latent visual representations. DSMN learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 35, 
    "authors": [
      "Ankit Goyal", 
      "J. Wang", 
      "Jia Deng"
    ], 
    "topics": [
      "Question answering", 
      "Synthetic data", 
      "Benchmark (computing)", 
      "Network architecture", 
      "Synthetic intelligence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.509", 
    "title": "Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In online debates, users express different levels of agreement/disagreement with one another\u2019s arguments and ideas. Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. Existing stance detection methods predict the polarity of a post\u2019s stance toward a topic or post, but don\u2019t consider the stance\u2019s degree of intensity. We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding. Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis. We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation. These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform. The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596. This work is the first to train models for predicting a post\u2019s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 59, 
    "authors": [
      "Joseph W. Sirrianni", 
      "Xiaoqing Frank Liu", 
      "Douglas Adams"
    ], 
    "topics": [
      "Sensor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1255", 
    "title": "Self-Discriminative Learning for Unsupervised Document Embedding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Unsupervised document representation learning is an important task providing pre-trained features for NLP applications. Unlike most previous work which learn the embedding based on self-prediction of the surface of text, we explicitly exploit the inter-document information and directly model the relations of documents in embedding space with a discriminative network and a novel objective. Extensive experiments on both small and large public datasets show the competitiveness of the proposed method. In evaluations on standard document classification, our model has errors that are 5 to 13% lower than state-of-the-art unsupervised embedding models. The reduction in error is even more pronounced in scarce label setting.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 37, 
    "authors": [
      "Hong-You Chen", 
      "Chin-Hua Hu", 
      "Leila Wehbe", 
      "Shou-de Lin"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Document classification", 
      "Semi-supervised learning", 
      "Competitive analysis (online algorithm)", 
      "Natural language processing", 
      "Microsoft Research", 
      "Feature learning", 
      "Supervised learning", 
      "Experiment", 
      "Lexicon", 
      "Semiconductor industry", 
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2705", 
    "title": "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Supervised domain adaptation\u2014where a large generic corpus and a smaller in-domain corpus are both available for training\u2014is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model\u2019s output word distribution and that of the out-of-domain model to prevent the model\u2019s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.", 
    "year": 2018, 
    "venue": "NMT@ACL", 
    "references": 25, 
    "authors": [
      "Huda Khayrallah", 
      "Brian Thompson", 
      "Kevin Duh", 
      "Philipp Koehn"
    ], 
    "topics": [
      "Neural machine translation", 
      "Domain adaptation", 
      "BLEU", 
      "Cross entropy", 
      "Domain model", 
      "Experiment", 
      "Vocabulary", 
      "Body of uterus", 
      "Baseline (configuration management)", 
      "Text corpus", 
      "Acclimatization", 
      "Mathematical model", 
      "Generic Drugs", 
      "NMT1 gene", 
      "Small", 
      "Matthews correlation coefficient", 
      "Description"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1191", 
    "title": "A Graph Degeneracy-based Approach to Keyword Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We operate a change of paradigm and hypothesize that keywords are more likely to be found among influential nodes of a graph-ofwords rather than among its nodes high on eigenvector-related centrality measures. To test this hypothesis, we introduce unsupervised techniques that capitalize on graph degeneracy. Our methods strongly and significantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents).", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 31, 
    "authors": [
      "A. Tixier", 
      "Fragkiskos D. Malliaros", 
      "M. Vazirgiannis"
    ], 
    "topics": [
      "Keyword extraction", 
      "Degeneracy (graph theory)", 
      "Centrality", 
      "Baseline (configuration management)", 
      "Unsupervised learning", 
      "Programming paradigm", 
      "Document", 
      "Word lists by frequency", 
      "Graph theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1165", 
    "title": "It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We address the problem of answering new questions in community forums, by selecting suitable answers to already asked questions. We approach the task as an answer ranking problem, adopting a pairwise neural network architecture that selects which of two competing answers is better. We focus on the utility of the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment, which we call relevance, relatedness, and appropriateness. Our proposed neural network models the interactions among all input components using syntactic and semantic embeddings, lexical matching, and domain-specific features. It achieves state-of-the-art results, showing that the three similarities are important and need to be modeled together. Our experiments demonstrate that all feature types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 50, 
    "authors": [
      "Preslav Nakov", 
      "Llu\u00eds M\u00e0rquez i Villodre", 
      "Francisco Guzm\u00e1n"
    ], 
    "topics": [
      "Question answering", 
      "Tango", 
      "Norm (social)", 
      "Relevance", 
      "Semantic similarity", 
      "Textual entailment", 
      "Network architecture", 
      "Feedforward neural network", 
      "Artificial neural network", 
      "Utility", 
      "Interaction", 
      "Experiment", 
      "Benchmark (computing)", 
      "Syntactic predicate", 
      "Domain-specific language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1224", 
    "title": "Show Your Work: Improved Reporting of Experimental Results", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 45, 
    "authors": [
      "Jesse Dodge", 
      "Suchin Gururangan", 
      "D. Card", 
      "Roy Schwartz", 
      "Noah A. Smith"
    ], 
    "topics": [
      "Computation", 
      "Natural language processing", 
      "Best practice", 
      "Test data"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2021", 
    "title": "Training Structured Prediction Energy Networks with Indirect Supervision", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 17, 
    "authors": [
      "Pedram Rooshenas", 
      "Aishwarya Kamath", 
      "A. McCallum"
    ], 
    "topics": [
      "Structured prediction", 
      "Gradient descent", 
      "Ground truth", 
      "Scoring functions for docking"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1110", 
    "title": "ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Effective text classification requires experts to annotate data with labels; these training data are time-consuming and expensive to obtain. If you know what labels you want, active learning can reduce the number of labeled documents needed. However, establishing the label set remains difficult. Annotators often lack the global knowledge needed to induce a label set. We introduce ALTO: Active Learning with Topic Overviews, an interactive system to help humans annotate documents: topic models provide a global overview of what labels to create and active learning directs them to the right documents to label. Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO\u2019s combination is best overall.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 56, 
    "authors": [
      "Forough Poursabzi-Sangdeh", 
      "Jordan L. Boyd-Graber", 
      "Leah Findlater", 
      "K. Seppi"
    ], 
    "topics": [
      "Topic model", 
      "Semantic role labeling", 
      "Active learning (machine learning)", 
      "Text corpus", 
      "Named-entity recognition", 
      "Document classification", 
      "Interactivity", 
      "Usability testing", 
      "Statistical classification", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K18-1022", 
    "title": "Simple Unsupervised Keyphrase Extraction using Sentence Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Keyphrase extraction is the task of automatically selecting a small set of phrases that best describe a given free text document. Supervised keyphrase extraction requires large amounts of labeled training data and generalizes very poorly outside the domain of the training data. At the same time, unsupervised systems have poor accuracy, and often do not generalize well, as they require the input document to belong to a larger corpus also given as input. Addressing these drawbacks, in this paper, we tackle keyphrase extraction from single documents with EmbedRank: a novel unsupervised method, that leverages sentence embeddings. EmbedRank achieves higher F-scores than graph-based state of the art systems on standard datasets and is suitable for real-time processing of large amounts of Web data. With EmbedRank, we also explicitly increase coverage and diversity among the selected keyphrases by introducing an embedding-based maximal marginal relevance (MMR) for new phrases. A user study including over 200 votes showed that, although reducing the phrases\u2019 semantic overlap leads to no gains in F-score, our high diversity selection is preferred by humans.", 
    "year": 2018, 
    "venue": "CoNLL", 
    "references": 27, 
    "authors": [
      "Kamil Bennani-Smires", 
      "C. Musat", 
      "Andreea Hossmann", 
      "Michael Baeriswyl", 
      "Martin Jaggi"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Information extraction", 
      "Usability testing", 
      "Semi-supervised learning", 
      "Sentence boundary disambiguation", 
      "Relevance", 
      "Scalability", 
      "Marginal model", 
      "Maximal set", 
      "Keyword extraction", 
      "Real-time clock", 
      "F1 score", 
      "ENCODE", 
      "Multi-master replication"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5049", 
    "title": "Sieg at MEDIQA 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively.", 
    "year": 2019, 
    "venue": "BioNLP@ACL", 
    "references": 29, 
    "authors": [
      "Sai Abishek Bhaskar", 
      "R. Rungta", 
      "James Route", 
      "Eric Nyberg", 
      "T. Mitamura"
    ], 
    "topics": [
      "Neural ensemble", 
      "Native-language identification", 
      "Knowledge integration", 
      "Natural language", 
      "Multi-task learning", 
      "Computer multitasking", 
      "Question answering"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.wnut-1.67", 
    "title": "EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.", 
    "year": 2020, 
    "venue": "WNUT", 
    "references": 26, 
    "authors": [
      "Nickil Maveli"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00215", 
    "title": "Modeling Child Divergences from Adult Grammar", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "During the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. Our corpus consists of child sentences with corrected adult forms. We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. Our method outperforms MT and ESL baselines, reducing child error by 20%. Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.", 
    "year": 2013, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 79, 
    "authors": [
      "Sam Sahakian", 
      "Benjamin Snyder"
    ], 
    "topics": [
      "Noisy channel model", 
      "Natural language processing", 
      "Plausibility structure", 
      "Experiment", 
      "Channel (communications)", 
      "Noisy-channel coding theorem", 
      "Discriminative model"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.ecnlp-1.18", 
    "title": "Combining semantic search and twin product classification for recognition of purchasable items in voice shopping", 
    "fields_of_study": null, 
    "abstract": "The accuracy of an online shopping system via voice commands is particularly important and may have a great impact on customer trust. This paper focuses on the problem of detecting if an utterance contains actual and purchasable products, thus referring to a shopping-related intent in a typical Spoken Language Understanding architecture consist- ing of an intent classifier and a slot detec- tor. Searching through billions of products to check if a detected slot is a purchasable item is prohibitively expensive. To overcome this problem, we present a framework that (1) uses a retrieval module that returns the most rele- vant products with respect to the detected slot, and (2) combines it with a twin network that decides if the detected slot is indeed a pur- chasable item or not. Through various exper- iments, we show that this architecture outper- forms a typical slot detector approach, with a gain of +81% in accuracy and +41% in F1 score.", 
    "year": 2021, 
    "venue": "ECNLP", 
    "references": 18, 
    "authors": [
      "Dieu-Thu Le", 
      "Verena Weber", 
      "Melanie Bradford"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1032", 
    "title": "AffecThor at SemEval-2018 Task 1: A cross-linguistic approach to sentiment intensity quantification in tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe our submission to SemEval-2018 Task 1: Affects in Tweets. The model which we present is an ensemble of various neural architectures and gradient boosted trees, and employs three different types of vectorial tweet representations. Furthermore, our system is language-independent and ranked first in 5 out of the 12 subtasks in which we participated, while achieving competitive results in the remaining ones. Comparatively remarkable performance is observed on both the Arabic and Spanish languages.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 40, 
    "authors": [
      "Mostafa Abdou", 
      "Artur Kulmizev", 
      "J. G. I. Ametll\u00e9"
    ], 
    "topics": [
      "Gradient boosting", 
      "Nonlinear system", 
      "Language-independent specification", 
      "End-to-end principle", 
      "Computer multitasking", 
      "Sigmoid function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-042-7_003", 
    "title": "Comparing Machine Translation and Human Translation: A Case Study", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "As machine translation technology improves comparisons to human performance are often made in quite general and exaggerated terms. Thus, it is important to be able to account for differences accurately. This paper reports a simple, descriptive scheme for comparing translations and applies it to two translations of a British opinion article published in March, 2017. One is a human translation (HT) into Swedish, and the other a machine translation (MT). While the comparison is limited to one text, the results are indicative of current limitations in MT.", 
    "year": 2017, 
    "venue": "", 
    "references": 31, 
    "authors": [
      "Lars Ahrenberg"
    ], 
    "topics": [
      "Machine translation", 
      "Language Translations", 
      "Postediting", 
      "Human reliability", 
      "Human-readable medium", 
      "Information flow (information theory)", 
      "Description", 
      "Exhibits as Topic", 
      "Hypertensive disease", 
      "Scientific Publication", 
      "Genetic Translation Process"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.266", 
    "title": "Assessing Robustness of Text Classification through Maximal Safe Radius Computation", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 63, 
    "authors": [
      "Emanuele La Malfa", 
      "M. Wu", 
      "L. Laurenti", 
      "Benjie Wang", 
      "A. Hartshorn", 
      "M. Kwiatkowska"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4714", 
    "title": "Generating R\u00e9cit from Sensor Data: Evaluation of a Task Model for Story Planning and Preliminary Experiments with GPS Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatic story generation is the subject of a growing research effort which has mainly focused on fictional stories. In this paper, we present some preliminary work to generate r\u00b4 (stories) from sensors data acquired during a ski sortie. In this approach, the story planning is performed using a task model that represents domain knowledge and sequential constraints between ski activities. To test the validity of the task model, a small-scale user evaluation was performed to compare the human perception of r\u00b4 ecit plans from hand writ...", 
    "year": 2015, 
    "venue": "ENLG", 
    "references": 14, 
    "authors": [
      "B. B. Miranda", 
      "S. Caffiau", 
      "C. Garbay", 
      "F. Portet"
    ], 
    "topics": [
      "Global Positioning System", 
      "Natural language generation", 
      "Sensor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2041", 
    "title": "Content Importance Models for Scoring Writing From Sources", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Selection of information from external sources is an important skill assessed in educational measurement. We address an integrative summarization task used in an assessment of English proficiency for nonnative speakers applying to higher education institutions in the USA. We evaluate a variety of content importance models that help predict which parts of the source material should be selected by the test-taker in order to succeed on this task.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 17, 
    "authors": [
      "Beata Beigman Klebanov", 
      "Nitin Madnani", 
      "J. Burstein", 
      "Swapna Somasundaran"
    ], 
    "topics": [
      "Trigram", 
      "Bigram", 
      "N-gram", 
      "Units of information", 
      "Space: Above and Beyond", 
      "Baseline (configuration management)", 
      "Grams", 
      "Complementarity (physics)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-1039", 
    "title": "I Can Has Cheezburger? A Nonparanormal Approach to Combining Textual and Visual Information for Predicting and Generating Popular Meme Descriptions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The advent of social media has brought Internet memes, a unique social phenomenon, to the front stage of the Web. Embodied in the form of images with text descriptions, little do we know about the \u201clanguage of memes\u201d. In this paper, we statistically study the correlations among popular memes and their wordings, and generate meme descriptions from raw images. To do this, we take a multimodal approach\u2014we propose a robust nonparanormal model to learn the stochastic dependencies among the image, the candidate descriptions, and the popular votes. In experiments, we show that combining text and vision helps identifying popular meme descriptions; that our nonparanormal model is able to learn dense and continuous vision features jointly with sparse and discrete text features in a principled manner, outperforming various competitive baselines; that our system can generate meme descriptions using a simple pipeline.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 59, 
    "authors": [
      "William Yang Wang", 
      "Miaomiao Wen"
    ], 
    "topics": [
      "Meme", 
      "Information retrieval", 
      "Experiment", 
      "Multimodal interaction", 
      "Reverse image search", 
      "Social media", 
      "BLEU", 
      "Image retrieval", 
      "Baseline (configuration management)", 
      "Sparse matrix", 
      "World Wide Web"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5601", 
    "title": "Findings of the Third Workshop on Neural Generation and Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.", 
    "year": 2019, 
    "venue": "NGT@EMNLP-IJCNLP", 
    "references": 35, 
    "authors": [
      "Hiroaki Hayashi", 
      "Yusuke Oda", 
      "Alexandra Birch", 
      "Ioannis Konstas", 
      "A. Finch", 
      "Minh-Thang Luong", 
      "Graham Neubig", 
      "Katsuhito Sudoh"
    ], 
    "topics": [
      "Testbed", 
      "Empirical Methods in Natural Language Processing", 
      "Neural machine translation", 
      "Procedural generation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1220175.1220226", 
    "title": "Automatic Learning of Textual Entailments with Cross-Pair Similarities", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods.", 
    "year": 2006, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Fabio Massimo Zanzotto", 
      "Alessandro Moschitti"
    ], 
    "topics": [
      "Rewriting", 
      "Computational complexity theory", 
      "Kernel method", 
      "Support vector machine", 
      "Similarity measure", 
      "Approximation algorithm", 
      "Experiment", 
      "Machine learning", 
      "Kernel (operating system)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1143", 
    "title": "Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Fact-checking of textual sources needs to effectively extract relevant information from large knowledge bases. In this paper, we extend an existing pipeline approach to better tackle this problem. We propose a neural ranker using a decomposable attention model that dynamically selects sentences to achieve promising improvement in evidence retrieval F1 by 38.80%, with (x65) speedup compared to a TF-IDF method. Moreover, we incorporate lexical tagging methods into our pipeline framework to simplify the tasks and render the model more generalizable. As a result, our framework achieves promising performance on a large-scale fact extraction and verification dataset with speedup.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Nayeon Lee", 
      "Chien-Sheng Wu", 
      "Pascale Fung"
    ], 
    "topics": [
      "Tf\u2013idf", 
      "Speedup", 
      "Knowledge base", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1188", 
    "title": "Adaptive Parameterization for Neural Dialogue Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an {\\bf Ada}ptive {\\bf N}eural {\\bf D}ialogue generation model, \\textsc{AdaND}, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 38, 
    "authors": [
      "Hengyi Cai", 
      "Hongshen Chen", 
      "C. Zhang", 
      "Yonghao Song", 
      "Xiaofang Zhao", 
      "Dawei Yin"
    ], 
    "topics": [
      "Adaptive grammar", 
      "Experiment", 
      "Encoder", 
      "Programming paradigm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1016", 
    "title": "Weakly Supervised User Profile Extraction from Twitter", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While user attribute extraction on social media has received considerable attention, existing approaches, mostly supervised, encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates (e.g., gender). In this paper, we present a weaklysupervised approach to user profile extraction from Twitter. Users\u2019 profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text. In addition to traditional linguistic features used in distant supervision for information extraction, our approach also takes into account network information, a unique opportunity offered by social media. We test our algorithm on three attribute domains: spouse, education and job; experimental results demonstrate our approach is able to make accurate predictions for users\u2019 attributes based on their tweets. 1", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Jiwei Li", 
      "Alan Ritter", 
      "E. Hovy"
    ], 
    "topics": [
      "User profile", 
      "Social media", 
      "Information extraction", 
      "Google+", 
      "Algorithm", 
      "User-generated content", 
      "Unary operation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1232", 
    "title": "Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences", 
    "fields_of_study": [
      "Computer Science", 
      "Medicine"
    ], 
    "abstract": "We present an unsupervised model of dialogue act sequences in conversation. By modeling topical themes as transitioning more slowly than dialogue acts in conversation, our model de-emphasizes content-related words in order to focus on conversational function words that signal dialogue acts. We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dialogue act prevalence beyond temporal dependencies. According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness of each modeling assumption is found to vary depending on characteristics of the data. De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker tendencies is advantageous on the NPS corpus. The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 29, 
    "authors": [
      "Yohan Jo", 
      "Michael Miller Yoder", 
      "Hyeju Jang", 
      "C. Ros\u00e9"
    ], 
    "topics": [
      "CNET", 
      "Text corpus", 
      "Baseline (configuration management)", 
      "Unsupervised learning", 
      "Body of uterus", 
      "Dopamine", 
      "Content-control software", 
      "Numerous", 
      "Kerrison Predictor", 
      "Sample Variance", 
      "Complement System Proteins"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.572", 
    "title": "Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation. To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper. Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling. Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "Chenggong Gong", 
      "Jianfei Yu", 
      "Rui Xia"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K17-1032", 
    "title": "Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The task of relation classification in the biomedical domain is complex due to the presence of samples obtained from heterogeneous sources such as research articles, discharge summaries, or electronic health records. It is also a constraint for classifiers which employ manual feature engineering. In this paper, we propose a convolutional recurrent neural network (CRNN) architecture that combines RNNs and CNNs in sequence to solve this problem. The rationale behind our approach is that CNNs can effectively identify coarse-grained local features in a sentence, while RNNs are more suited for long-term dependencies. We compare our CRNN model with several baselines on two biomedical datasets, namely the i2b2-2010 clinical relation extraction challenge dataset, and the SemEval-2013 DDI extraction dataset. We also evaluate an attentive pooling technique and report its performance in comparison with the conventional max pooling method. Our results indicate that the proposed model achieves state-of-the-art performance on both datasets.", 
    "year": 2017, 
    "venue": "CoNLL", 
    "references": 54, 
    "authors": [
      "Desh Raj", 
      "Sunil Kumar Sahu", 
      "Ashish Anand"
    ], 
    "topics": [
      "Recurrent neural network", 
      "Feature engineering", 
      "Word embedding", 
      "Network model", 
      "SemEval", 
      "Relationship extraction", 
      "Convolution", 
      "Attentive user interface", 
      "Artificial neural network", 
      "Convolutional neural network", 
      "Design rationale", 
      "One-hot", 
      "Discharger", 
      "Randomness", 
      "Microsoft Word for Mac"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.13", 
    "title": "Jointly Learning Aspect-Focused and Inter-Aspect Relations with Graph Convolutional Networks for Aspect Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we explore a novel solution of constructing a heterogeneous graph for each instance by leveraging aspect-focused and inter-aspect contextual dependencies for the specific aspect and propose an Interactive Graph Convolutional Networks (InterGCN) model for aspect sentiment analysis. Specifically, an ordinary dependency graph is first constructed for each sentence over the dependency tree. Then we refine the graph by considering the syntactical dependencies between contextual words and aspect-specific words to derive the aspect-focused graph. Subsequently, the aspect-focused graph and the corresponding embedding matrix are fed into the aspect-focused GCN to capture the key aspect and contextual words. Besides, to interactively extract the inter-aspect relations for the specific aspect, an inter-aspect GCN is adopted to model the representations learned by aspect-focused GCN based on the inter-aspect graph which is constructed by the relative dependencies between the aspect words and other aspects. Hence, the model can be aware of the significant contextual and aspect words when interactively learning the sentiment features for a specific aspect. Experimental results on four benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods and substantially boosts the performance in comparison with BERT.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 37, 
    "authors": [
      "Bin Liang", 
      "Rongdi Yin", 
      "Lin Gui", 
      "Jiachen Du", 
      "Ruifeng Xu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2095", 
    "title": "Cross-Domain Review Helpfulness Prediction Based on Convolutional Neural Networks with Auxiliary Domain Discriminators", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With the growing amount of reviews in e-commerce websites, it is critical to assess the helpfulness of reviews and recommend them accordingly to consumers. Recent studies on review helpfulness require plenty of labeled samples for each domain/category of interests. However, such an approach based on close-world assumption is not always practical, especially for domains with limited reviews or the \u201cout-of-vocabulary\u201d problem. Therefore, we propose a convolutional neural network (CNN) based model which leverages both word-level and character-based representations. To transfer knowledge between domains, we further extend our model to jointly model different domains with auxiliary domain discriminators. On the Amazon product review dataset, our approach significantly outperforms the state of the art in terms of both accuracy and cross-domain robustness.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 28, 
    "authors": [
      "Cen Chen", 
      "Yinfei Yang", 
      "Jun Zhou", 
      "Xiaolong Li", 
      "F. S. Bao"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Artificial neural network", 
      "Computer multitasking", 
      "E-commerce", 
      "Text-based (computing)", 
      "Vocabulary"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.284", 
    "title": "Dual Attention Network for Cross-lingual Entity Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-lingual Entity alignment is an essential part of building a knowledge graph, which can help integrate knowledge among different language knowledge graphs. In the real KGs, there exists an imbalance among the information in the same hierarchy of corresponding entities, which results in the heterogeneity of neighborhood structure, making this task challenging. To tackle this problem, we propose a dual attention network for cross-lingual entity alignment (DAEA). Specifically, our dual attention consists of relation-aware graph attention and hierarchical attention. The relation-aware graph attention aims at selectively aggregating multi-hierarchy neighborhood information to alleviate the difference of heterogeneity among counterpart entities. The hierarchical attention adaptively aggregates the low-hierarchy and the high-hierarchy information, which is beneficial to balance the neighborhood information of counterpart entities and distinguish non-counterpart entities with similar structures. Finally, we treat cross-lingual entity alignment as a process of linking prediction. Experimental results on three real-world cross-lingual entity alignment datasets have shown the effectiveness of DAEA.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 32, 
    "authors": [
      "Jian Sun", 
      "Yu Zhou", 
      "Chengqing Zong"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1283", 
    "title": "Co-Training for Topic Classification of Scholarly Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With the exponential growth of scholarly data during the past few years, effective methods for topic classification are greatly needed. Current approaches usually require large amounts of expensive labeled data in order to make accurate predictions. In this paper, we posit that, in addition to a research article\u2019s textual content, its citation network also contains valuable information. We describe a co-training approach that uses the text and citation information of a research article as two different views to predict the topic of an article. We show that this method improves significantly over the individual classifiers, while also bringing a substantial reduction in the amount of labeled data required for training accurate classifiers.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 32, 
    "authors": [
      "Cornelia Caragea", 
      "Florin Adrian Bulgarov", 
      "Rada Mihalcea"
    ], 
    "topics": [
      "Co-training", 
      "Latent Dirichlet allocation", 
      "Citation network", 
      "Supervised learning", 
      "Algorithm", 
      "Experiment", 
      "Semi-supervised learning", 
      "Time complexity", 
      "Semiconductor industry", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2103", 
    "title": "Tackling Sparsity, the Achilles Heel of Social Networks: Language Model Smoothing via Social Regularization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Online social networks nowadays have the worldwide prosperity, as they have revolutionized the way for people to discover, to share, and to diffuse information. Social networks are powerful, yet they still have Achilles Heel: extreme data sparsity. Individual posting documents, (e.g., a microblog less than 140 characters), seem to be too sparse to make a difference under various scenarios, while in fact they are quite different. We propose to tackle this specific weakness of social networks by smoothing the posting document language model based on social regularization. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Rui Yan", 
      "Xiang Li", 
      "Mengwen Liu", 
      "Xiaohua Hu"
    ], 
    "topics": [
      "Smoothing", 
      "Language model", 
      "Sparse matrix", 
      "Social network", 
      "Sparse language", 
      "Heel-Ball Index", 
      "Mathematical optimization", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.3115/1219840.1219875", 
    "title": "QARLA: A Framework for the Evaluation of Text Summarization Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems. The input of the framework is a set of manual (reference) summaries, a set of baseline (automatic) summaries and a set of similarity metrics between summaries. It provides i) a measure to evaluate the quality of any set of similarity metrics, ii) a measure to evaluate the quality of a summary using an optimal set of similarity metrics, and iii) a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results.Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weighting of their relative importance. We provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text summarisation systems by combining several similarity metrics.", 
    "year": 2005, 
    "venue": "ACL", 
    "references": 10, 
    "authors": [
      "Enrique Amig\u00f3", 
      "J. Gonzalo", 
      "Anselmo Pe\u00f1as", 
      "M. Verdejo"
    ], 
    "topics": [
      "Baseline (configuration management)", 
      "Automatic summarization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w16-01", 
    "title": "Proceedings of the Workshop on Human-Computer Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2016, 
    "venue": "", 
    "references": 4, 
    "authors": [
      "Mohit Iyyer", 
      "He He", 
      "Jordan L. Boyd-Graber", 
      "Hal Daum\u00e9"
    ], 
    "topics": [
      "Question answering"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-3322", 
    "title": "Augmenting Abstract Meaning Representation for Human-Robot Dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We detail refinements made to Abstract Meaning Representation (AMR) that make the representation more suitable for supporting a situated dialogue system, where a human remotely controls a robot for purposes of search and rescue and reconnaissance. We propose 36 augmented AMRs that capture speech acts, tense and aspect, and spatial information. This linguistic information is vital for representing important distinctions, for example whether the robot has moved, is moving, or will move. We evaluate two existing AMR parsers for their performance on dialogue data. We also outline a model for graph-to-graph conversion, in which output from AMR parsers is converted into our refined AMRs. The design scheme presented here, though task-specific, is extendable for broad coverage of speech acts using AMR in future task-independent work.", 
    "year": 2019, 
    "venue": "", 
    "references": 34, 
    "authors": [
      "Claire Bonial", 
      "L. Donatelli", 
      "S. Lukin", 
      "S. Tratz", 
      "Ron Artstein", 
      "D. Traum", 
      "Clare R. Voss"
    ], 
    "topics": [
      "Situated", 
      "Parsing", 
      "Adaptive Multi-Rate audio codec", 
      "Statistical classification", 
      "Graph rewriting", 
      "Algorithm", 
      "Dialog system", 
      "Extensibility", 
      "Autonomous robot", 
      "ENCODE", 
      "Human\u2013robot interaction"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/S14-2012", 
    "title": "Alpage: Transition-based Semantic Graph Parsing with Syntactic Features", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the systems deployed by the ALPAGE team to participate to the SemEval-2014 Task on Broad-Coverage Semantic Dependency Parsing. We developed two transition-based dependency parsers with extended sets of actions to handle non-planar acyclic graphs. For the open track, we worked over two orthogonal axes \u2010 lexical and syntactic \u2010 in order to provide our models with lexical and syntactic features such as word clusters, lemmas and tree fragments of different types.", 
    "year": 2014, 
    "venue": "*SEMEVAL", 
    "references": 30, 
    "authors": [
      "Corentin Ribeyre", 
      "Eric Villemonte de la Clergerie", 
      "Djam\u00e9 Seddah"
    ], 
    "topics": [
      "Parsing", 
      "SemEval", 
      "Machine learning", 
      "Semantic analysis (machine learning)", 
      "Complex systems", 
      "Directed acyclic graph"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1435", 
    "title": "Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the \u201cleakage features.\u201d In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 48, 
    "authors": [
      "Guanhua Zhang", 
      "Bing Bai", 
      "Jian Liang", 
      "Kun Bai", 
      "Shiyu Chang", 
      "Mo Yu", 
      "Conghui Zhu", 
      "T. Zhao"
    ], 
    "topics": [
      "Selection bias", 
      "Natural language", 
      "Sampling (signal processing)", 
      "Trustworthy computing", 
      "Spectral leakage"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974194.974219", 
    "title": "\"Expertness\" from Structured Text? RECONSIDER: A Diagnostic Prompting Program", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "RECONSIDER is an interactive diagnostic prompting program which uses simple information retrieval techniques to prompt a physician regarding possible diagnoses, given a list of positive patient findings. Its knowledge base consists of \"structured text\" definitions of 3262 diseases and a synonym dictionary Patient findings, and their synonyms, are matched against inverted files of terms from the disease descriptions, the number and selectivity of the patient findings matching terms in a given disease description determine that disease's \"score\", and the matched diseases are sorted on this score to form a preliminary differential diagnosis. Definitions of diseases can be referenced for viewing by name, or by their position in a differential. While its first formal evaluation is not yet complete, the performance of RECONSIDER continues to exceed the expectations of user and designer alike.", 
    "year": 1983, 
    "venue": "ANLP", 
    "references": 39, 
    "authors": [
      "M. Tuttle", 
      "D. Sherertz", 
      "M. S. Blois", 
      "S. J. Nelson"
    ], 
    "topics": [
      "Structured text", 
      "Sed", 
      "Ordinal data", 
      "Skin (computing)", 
      "Code smell", 
      "Dictionary", 
      "Offset binary", 
      "Han unification", 
      "Artificial intelligence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-0906", 
    "title": "Clustering-based Approach to Multiword Expression Extraction and Ranking", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a domain-independent clusteringbased approach for automatic extraction of multiword expressions (MWEs). The method combines statistical information from a general-purpose corpus and texts from Wikipedia articles. We incorporate association measures via dimensions of data points to cluster MWEs and then compute the ranking score for each MWE based on the closest exemplar assigned to a cluster. Evaluation results, achieved for two languages, show that a combination of association measures gives an improvement in the ranking of MWEs compared with simple counts of cooccurrence frequencies and purely statistical measures.", 
    "year": 2015, 
    "venue": "MWE@NAACL-HLT", 
    "references": 30, 
    "authors": [
      "E. Tutubalina"
    ], 
    "topics": [
      "Text corpus", 
      "Cluster analysis", 
      "Minimal Working Example", 
      "Euclidean distance", 
      "Wikipedia", 
      "K-means clustering", 
      "Data point", 
      "Expression (computer science)", 
      "General-purpose markup language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-2013", 
    "title": "Adding Context to Semantic Data-Driven Paraphrasing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context-sensitive inferences have to consider both term meaning in context as well as the fine-grained relation holding between the terms. Hence, to develop suitable lexical inference methods, we need datasets that are annotated with fine-grained semantic relations in-context. Since existing datasets either provide outof-context annotations or refer to coarsegrained relations, we propose a methodology for adding context-sensitive annotations. We demonstrate our methodology by applying it to phrase pairs from PPDB 2.0, creating a novel dataset of finegrained lexical inferences in-context and showing its utility in developing contextsensitive methods.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 26, 
    "authors": [
      "Vered Shwartz", 
      "Ido Dagan"
    ], 
    "topics": [
      "Context-sensitive grammar", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980845.980961", 
    "title": "Large Scale Collocation Data and Their Application to Japanese Word Processor Technology", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word processors or computers used in Japan employ Japanese input method through keyboard stroke combined with Kana (phonetic) character to Kanji (ideographic, Chinese) character conversion technology. The key factor of Kana-to-Kanji conversion technology is how to raise the accuracy of the conversion through the homophone processing, since we have so many homophonic Kanjis. In this paper, we report the results of our Kana-to-Kanji conversion experiments which embody the homophone processing based on large scale collocation data. It is shown that approximately 135,000 collocations yield 9.1 % raise of the conversion accuracy compared with the prototype system which has no collocation data.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 17, 
    "authors": [
      "Yasuo Koyama", 
      "Masako Yasutake", 
      "Kenji Yoshimura", 
      "Kosho Shudo"
    ], 
    "topics": [
      "Collocation", 
      "Processor Technology", 
      "Japanese input methods", 
      "Input method", 
      "Experiment", 
      "Prototype", 
      "Central processing unit", 
      "Computer", 
      "Analog-to-digital converter"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2091", 
    "title": "UA at SemEval-2019 Task 5: Setting A Strong Linear Baseline for Hate Speech Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the system developed at the University of Alicante (UA) for the SemEval 2019 Task 5: Shared Task on Multilingual Detection of Hate. The purpose of this work is to build a strong baseline for hate speech detection, using a traditional machine learning approach with standard textual features, which could serve in a near future as a reference to compare with deep learning systems. We participated in both task A (Hate Speech Detection against Immigrants and Women) and task B (Aggressive behavior and Target Classification). Despite its simplicity, our system obtained a remarkable F1-score of 72.5 (sixth highest) and an accuracy of 73.6 (second highest) in Spanish (task A), outperforming more complex neural models from a total of 40 participant systems.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 18, 
    "authors": [
      "Carlos Perell\u00f3", 
      "D. Tom\u00e1s", 
      "Alberto Garcia-Garcia", 
      "J. G. Rodr\u00edguez", 
      "Jos\u00e9 Camacho-Collados"
    ], 
    "topics": [
      "SemEval", 
      "Sentiment analysis", 
      "Baseline (configuration management)", 
      "F1 score", 
      "Machine learning", 
      "N-gram", 
      "Deep learning", 
      "Word embedding", 
      "Feature engineering", 
      "Grams", 
      "Document classification", 
      "User agent", 
      "Artificial neural network", 
      "Sensor"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.7275/R5ZC812C", 
    "title": "Modeling the Decline in English Passivization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Evidence from the Hansard corpus shows that the passive voice in British English has declined in relative frequency over the last two centuries. We investigate which factors are predictive of whether transitive verb phrases are passivized. We show the increasing importance of the person-hierarchy effects observed by Bresnan et al. (2001), with increasing strength of the constraint against passivizing clauses with local agents, as well as the rising prevalence of such agents. Moreover, our ablation experiments on the Wall Street Journal and Hansard corpora provide support for the unmarked information structure of \u2018given\u2019 before \u2018new\u2019 noted by Halliday (1967).", 
    "year": 2018, 
    "venue": "", 
    "references": 24, 
    "authors": [
      "Liwen Hou", 
      "D. Smith"
    ], 
    "topics": [
      "The Wall Street Journal", 
      "Text corpus", 
      "explanation", 
      "Experiment", 
      "Parallel computing", 
      "Languages", 
      "Phrases", 
      "Body of uterus", 
      "Gain", 
      "Ablation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.89", 
    "title": "Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 57, 
    "authors": [
      "Xavier Garc\u00eda", 
      "Aditya Siddhant", 
      "Orhan Firat", 
      "Ankur P. Parikh"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1022", 
    "title": "Translating Neuralese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 36, 
    "authors": [
      "Jacob Andreas", 
      "A. Dragan", 
      "D. Klein"
    ], 
    "topics": [
      "Machine translation", 
      "Channel (communications)", 
      "Natural language", 
      "Agent-based model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981732.981785", 
    "title": "Conceptional Association for Compound Noun Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes research toward the automatic interpretation of compound nouns using corpus statistics. An initial study aimed at syntactic disambiguation is presented. The approach presented bases associations upon thesaurus categories. Association data is gathered from unambiguous cases extracted from a corpus and is then applied to the analysis of ambiguous compound nouns. While the work presented is still in progress, a first attempt to syntactically analyse a test set of 244 examples shows 75% correctness. Future work is aimed at improving this accuracy and extending the technique to assign semantic role information, thus producing a complete interpretation.", 
    "year": 1994, 
    "venue": "ACL", 
    "references": 9, 
    "authors": [
      "Mark Lauer"
    ], 
    "topics": [
      "Thesaurus", 
      "Test set", 
      "Word-sense disambiguation", 
      "Text corpus", 
      "Semantic interpretation", 
      "Correctness (computer science)", 
      "Online and offline"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00204", 
    "title": "Reflections on the Penn Discourse TreeBank, Comparable Corpora, and Complementary Annotation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Penn Discourse Treebank (PDTB) was released to the public in 2008. It remains the largest manually annotated corpus of discourse relations to date. Its focus on discourse relations that are either lexically-grounded in explicit discourse connectives or associated with sentential adjacency has not only facilitated its use in language technology and psycholinguistics but also has spawned the annotation of comparable corpora in other languages and genres.Given this situation, this paper has four aims: (1) to provide a comprehensive introduction to the PDTB for those who are unfamiliar with it; (2) to correct some wrong (or perhaps inadvertent) assumptions about the PDTB and its annotation that may have weakened previous results or the performance of decision procedures induced from the data; (3) to explain variations seen in the annotation of comparable resources in other languages and genres, which should allow developers of future comparable resources to recognize whether the variations are relevant to them; and (4) to enumerate and explain relationships between PDTB annotation and complementary annotation of other linguistic phenomena. The paper draws on work done by ourselves and others since the corpus was released.", 
    "year": 2014, 
    "venue": "CL", 
    "references": 98, 
    "authors": [
      "R. Prasad", 
      "B. Webber", 
      "A. Joshi"
    ], 
    "topics": [
      "Treebank", 
      "Text corpus", 
      "DAISY Digital Talking Book", 
      "Language technology", 
      "Logical connective", 
      "Enumerated type", 
      "Reflection (computer graphics)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981436.981481", 
    "title": "Word, Phrase and Sentence", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A f i f t h s tudy a rgues t h a t a n a l y s i s of e x i s t i n g n a t u r a l l anguage d i c t i o n a r i e s can be e x p e c t e d to c o n t r i b u t e i m p o r t a n t l y to what i s needed f o r t e x t u n d e r s t a n d i n g programs. The f i n a l s t udy i s an expe r imen t w i t h a s e n t e n c e l e v e l t r a n s l a t o r a p p l i e d to a l a r g e German-Engl i sh t r a n s l a t i o n t a s k . These two s t u d i e s a r e p r i m a r i l y concerned w i t h a n a l y s i s of l anguage a t the s e n t e n c e l e v e l . The most glamourous a r e a s of n a t u r a l l anguage r e s e a r c h a r e a t l e v e l s above the s e n t e n c e , concerned w i t h d i a l o g u e s and d i s c o u r s e , f r e q u e n t l y d i s d a i n f u l of m o r p h o l o g i c a l o r even g rammat i ca l a n a l y s i s i n t h e i r s e a r c h fo r e f f e c t i v e s t r u c t u r e s f o r u n d e r s t a n d i n g what the d i s c o u r s e i s abou t . S c r i p t s , f r ames , s t e r e o t y p e s , schemas a re a l l s t u d i e d i n t h e s e a r e a s ; and o f t e n m o r p h o l o g i c a l and g r a ~ n a t i c a l a n a l y s i s i s bypassed i n f a v o r of keyword s c a n n i n g to e x t r a c t some smal l r e l e v a n t p o r t i o n of the t e x t to be bound as v a l u e s f o r s l o t s i n t h e s e l a r g e r d a t a forms. This s e s s i o n reminds us t h a t much can be accompl i shed w i t h v o c a b u l a r y a n a l y s i s , w i t h keyword s c a n n i n g and statistical treatment of text and with semantic analysis at the single sentence level. Yet, with regard to most of the topics in this and other sessions, there is a stronK sense of de~a vu; the earliest natural language studies featured automatic extracting and information retrieval based on statistical, lexical and associational properties of keywords. Mechanical translation of sentences without regard for larger contexts marked the late sixties high point of MT research amid contemporaneous studies of the English dictionary and thesaurus. Competition among sentence parsing algorithms is an ACL tradition celebrated annually, while psycholinguistics has traditionally applied chronometric studies, and recordings of eye movements to measure this or that aspect of human linguistic processing throughout the", 
    "year": 1980, 
    "venue": "ACL", 
    "references": 0, 
    "authors": [
      "R. F. Simmons"
    ], 
    "topics": [
      "Thesaurus", 
      "Dictionary", 
      "Information retrieval", 
      "FO (complexity)", 
      "Natural language", 
      "Parsing", 
      "Semantic analysis (compilers)", 
      "Digital-to-analog converter", 
      "Artificial intelligence", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1614", 
    "title": "Treebank Translation for Cross-Lingual Parser Induction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-lingual learning has become a popular approach to facilitate the development of resources and tools for low density languages. Its underlying idea is to make use of existing tools and annotations in resource-rich languages to create similar tools and resources for resource-poor languages. Typically, this is achieved by either projecting annotations across parallel corpora, or by transferring models from one or more source languages to a target language. In this paper, we explore a third strategy by using machine translation to create synthetic training data from the original source-side annotations. Specifically, we apply this technique to dependency parsing, using a cross-lingually unified treebank for adequate evaluation. Our approach draws on annotation projection but avoids the use of noisy source-side annotation of an unrelated parallel corpus and instead relies on manual treebank annotation in combination with statistical machine translation, which makes it possible to train fully lexicalized parsers. We show that this approach significantly outperforms delexicalized transfer parsing.% despite the error-prone translation step.", 
    "year": 2014, 
    "venue": "CoNLL", 
    "references": 32, 
    "authors": [
      "J. Tiedemann", 
      "Zeljko Agic", 
      "Joakim Nivre"
    ], 
    "topics": [
      "Treebank", 
      "Parallel text", 
      "Statistical machine translation", 
      "Parser", 
      "Compiler", 
      "Cognitive dimensions of notations", 
      "Text corpus", 
      "Synthetic intelligence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.193", 
    "title": "Neural Deepfake Detection with Factual Structure of Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 28, 
    "authors": [
      "Wanjun Zhong", 
      "Duyu Tang", 
      "Zenan Xu", 
      "Ruize Wang", 
      "Nan Duan", 
      "M. Zhou", 
      "Jiahai Wang", 
      "Jian Yin"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.464", 
    "title": "PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of Chinese characters. Statistics reveal that most Chinese spelling errors belong to phonological or visual errors. However, previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information. Specifically, we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism. To verify its effectiveness, we conduct comprehensive experiments and ablation tests. Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 39, 
    "authors": [
      "Li Huang", 
      "Junjie Li", 
      "Weiwei Jiang", 
      "Zhiyu Zhang", 
      "Minchuan Chen", 
      "Shaojun Wang", 
      "Jing Xiao"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1601", 
    "title": "Explaining Predictions of Non-Linear Classifiers in NLP", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classifiers in terms of input variables. In this paper, we apply LRP for the first time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a specific prediction of the CNN. We compare our technique to standard sensitivity analysis, both qualitatively and quantitatively, using a \"word deleting\" perturbation experiment, a PCA analysis, and various visualizations. All experiments validate the suitability of LRP for explaining the CNN predictions, which is also in line with results reported in recent image classification studies.", 
    "year": 2016, 
    "venue": "Rep4NLP@ACL", 
    "references": 25, 
    "authors": [
      "L. Arras", 
      "F. Horn", 
      "G. Montavon", 
      "K. M\u00fcller", 
      "W. Samek"
    ], 
    "topics": [
      "Natural language processing", 
      "Convolutional neural network", 
      "Linear classifier", 
      "Relevance", 
      "Artificial neural network", 
      "Computer vision", 
      "Categorization", 
      "Nonlinear system", 
      "Experiment", 
      "Software propagation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w15-43", 
    "title": "Proceedings of the Workshop on Noisy User-generated Text, NUT@IJCNLP 2015, Beijing, China, July 31, 2015", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2015, 
    "venue": "NUT@IJCNLP", 
    "references": 0, 
    "authors": [], 
    "topics": [
      "User-generated content"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2059", 
    "title": "Citation Resolution: A method for evaluating context-based citation recommendation systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Wouldn\u2019t it be helpful if your text editor automatically suggested papers that are relevant to your research? Wouldn\u2019t it be even better if those suggestions were contextually relevant? In this paper we name a system that would accomplish this a context-based citation recommendation (CBCR) system. We specifically present Citation Resolution, a method for the evaluation of CBCR systems which exclusively uses readily-available scientific articles. Exploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation. We apply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the surrounding contexts of citations to the document found in other documents, and c) a mixture of the two.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 16, 
    "authors": [
      "Daniel Duma", 
      "Ewan Klein"
    ], 
    "topics": [
      "Recommender system", 
      "Text editor", 
      "Scientific literature", 
      "Contextual advertising", 
      "Relevance", 
      "Document", 
      "Emoticon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4707", 
    "title": "A Framework for the Generation of Computer System Diagnostics in Natural Language using Finite State Methods", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Understanding what has led to a failure is crucial for addressing problems with computer systems. We present a meta-NLG system that can be configured to generate natural explanations from error trace data originating in an external computational system. Distinguishing features are the generic nature of the system, and the underlying finite-state technology. Results of a two-pronged evaluation dealing with naturalness and ease of use are described.", 
    "year": 2015, 
    "venue": "ENLG", 
    "references": 19, 
    "authors": [
      "Rachelyn Farrell", 
      "G. Pace", 
      "M. Rosner"
    ], 
    "topics": [
      "LINC", 
      "Regular language", 
      "Complex systems", 
      "Computer science", 
      "Usability", 
      "Computation", 
      "Natural language generation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-2034", 
    "title": "Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist (this https URL) and observed a significant improvement over the existing online model.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Minghui Qiu", 
      "Liu Yang", 
      "Feng Ji", 
      "Weipeng Zhao", 
      "W. Zhou", 
      "Jun Huang", 
      "Haiqing Chen", 
      "W. Croft", 
      "Wei Lin"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Information seeking", 
      "Experiment", 
      "E-commerce payment system", 
      "Artificial neural network", 
      "Software deployment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5002", 
    "title": "Changing the Level of Directness in Dialogue using Dialogue Vector Models and Recurrent Neural Networks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In cooperative dialogues, identifying the intent of ones conversation partner and acting accordingly is of great importance. While this endeavour is facilitated by phrasing intentions as directly as possible, we can observe in human-human communication that a number of factors such as cultural norms and politeness may result in expressing one\u2019s intent indirectly. Therefore, in human-computer communication we have to anticipate the possibility of users being indirect and be prepared to interpret their actual meaning. Furthermore, a dialogue system should be able to conform to human expectations by adjusting the degree of directness it uses to improve the user experience. To reach those goals, we propose an approach to differentiate between direct and indirect utterances and find utterances of the opposite characteristic that express the same intent. In this endeavour, we employ dialogue vector models and recurrent neural networks.", 
    "year": 2018, 
    "venue": "SIGDIAL Conference", 
    "references": 23, 
    "authors": [
      "Louisa Pragst", 
      "Stefan Ultes"
    ], 
    "topics": [
      "Recurrent neural network", 
      "Unsupervised learning", 
      "Dialog system", 
      "Text corpus", 
      "User experience", 
      "Artificial neural network", 
      "Endeavour (supercomputer)", 
      "Similarity measure", 
      "Norm (social)", 
      "Signal-to-noise ratio", 
      "Neural Networks", 
      "Heart rate variability"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2118", 
    "title": "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 13, 
    "authors": [
      "Chih-Wen Goo", 
      "Guang-Lai Gao", 
      "Yun-Kai Hsu", 
      "Chih-Li Huo", 
      "Tsung-Chieh Chen", 
      "Keng-Wei Hsu", 
      "Yun-Nung (Vivian) Chen"
    ], 
    "topics": [
      "Recurrent neural network", 
      "Microsoft Research", 
      "Global optimization", 
      "Frame language", 
      "Automatic Transmitter Identification System (television)", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Mathematical optimization", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.74", 
    "title": "Low-Complexity Probing via Finding Subnetworks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model\u2019s internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 30, 
    "authors": [
      "Steven Cao", 
      "Victor Sanh", 
      "Alexander M. Rush"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/N19-1182", 
    "title": "Glocal: Incorporating Global Information in Local Convolution for Keyphrase Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform supervised classification directly on graphical structures. While such methods model nodes\u2019 local pairwise importance, they lack the capability to model global importance relative to other nodes of the graph. This causes such models to miss critical information in tasks where global ranking is a key component for the task, such as in keyphrase extraction. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of keyphrase extraction, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed method, we achieve state-of-the-art results, bettering a strong baseline by an absolute 2% increase in F1 score.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 25, 
    "authors": [
      "Animesh Prasad", 
      "Min-Yen Kan"
    ], 
    "topics": [
      "Convolution", 
      "Graphics Core Next", 
      "Spectral clustering", 
      "F1 score", 
      "Keyword extraction", 
      "Sequential consistency", 
      "Experiment", 
      "Graph (discrete mathematics)", 
      "Baseline (configuration management)", 
      "Cluster analysis", 
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075252", 
    "title": "Part-of-Speech Tagging Based on Hidden Markov Model Assuming Joint Independence", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present part-of-speech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts. In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably. To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 13, 
    "authors": [
      "Sang-Zoo Lee", 
      "Junichi Tsujii", 
      "Hae-Chang Rim"
    ], 
    "topics": [
      "Hidden Markov model", 
      "Markov chain", 
      "Part-of-speech tagging"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.665", 
    "title": "Gone at Last: Removing the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Previous work proposed tackling this problem via adversarial training, but this leads to learned sentence representations that still suffer from the same biases. As a solution, we propose using an ensemble of adversaries during the training, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. We show that using an ensemble of adversaries can prevent the bias from being relearned after the model training is completed, further improving how well the model generalises to different NLI datasets. In particular, these models outperformed previous approaches when tested on 12 different NLI datasets not used in the model training. Finally, the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger dimensional representations benefiting when trained with a greater number of adversaries.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 63, 
    "authors": [
      "Joe Stacey", 
      "Pasquale Minervini", 
      "Haim Dubossarsky", 
      "Sebastian Riedel", 
      "Tim Rockt\u00e4schel"
    ], 
    "topics": [
      "Natural language", 
      "Adversary (cryptography)", 
      "Native-language identification", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.58", 
    "title": "YNU-HPCC at SemEval-2021 Task 11: Using a BERT Model to Extract Contributions from NLP Scholarly Articles", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11: NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a knowledge graph with the help of multiclass classification and multi-label classification. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 17, 
    "authors": [
      "Xinge Ma", 
      "Jin Wang", 
      "Xuejie Zhang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3308", 
    "title": "Representing Honorifics via Individual Constraints", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Within the context of grammar engineering, modelling honorifics has been regarded as one of the components for improving machine translation and anaphora resolution. Using the HPSG and MRS framework, this paper provides a computational model of honorifics. The present study incorporates the honorific information into the meaning representation system via Individual Constraints with an eye toward semantics-based processing.", 
    "year": 2015, 
    "venue": "", 
    "references": 20, 
    "authors": [
      "Sanghoun Song"
    ], 
    "topics": [
      "Anaphora (linguistics)", 
      "Machine translation", 
      "Computational model", 
      "Head-driven phrase structure grammar", 
      "Minimal recursion semantics"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974235.974279", 
    "title": "Combinatorial Disambiguation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The disambiguation of sentences is a combinatorial problem. This paper describes a method for treating it as such, directly, by adapting standard combinatorial search optimizations. Traditional disambiguation heuristics are applied but, instead of being embedded in individual decision procedures for specific types of ambiguities, they contribute to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system.", 
    "year": 1988, 
    "venue": "ANLP", 
    "references": 27, 
    "authors": [
      "P. Newman"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Machine translation", 
      "Combinatorial search", 
      "Natural language", 
      "Numerical analysis", 
      "Mathematical optimization", 
      "Embedded system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6418", 
    "title": "JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the current work, we present a description of the system submitted to WMT 2018 News Translation Shared task. The system was created to translate news text from Finnish to English. The system used a Character Based Neural Machine Translation model to accomplish the given task. The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same. Our system garnered a BLEU score of 12.9.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 16, 
    "authors": [
      "S. Mahata", 
      "Dipankar Das", 
      "Sivaji Bandyopadhyay"
    ], 
    "topics": [
      "Neural machine translation", 
      "BLEU", 
      "Preprocessor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980845.980928", 
    "title": "Improving Data Driven Wordclass Tagging by System Combination", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generator (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indvidual tagger.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 37, 
    "authors": [
      "H. V. Halteren", 
      "Jakub Zavrel", 
      "W. Daelemans"
    ], 
    "topics": [
      "Hidden Markov model", 
      "Natural language processing", 
      "Brill tagger", 
      "Markov chain"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.cmcl-1.12", 
    "title": "PIHKers at CMCL 2021 Shared Task: Cosine Similarity and Surprisal to Predict Human Reading Patterns.", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Eye-tracking psycholinguistic studies have revealed that context-word semantic coherence and predictability influence language processing. In this paper we show our approach to predict eye-tracking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a regression model, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features.", 
    "year": 2021, 
    "venue": "CMCL", 
    "references": 25, 
    "authors": [
      "Lavinia Salicchi", 
      "Alessandro Lenci"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.eval4nlp-1.5", 
    "title": "BLEU Neighbors: A Reference-less Approach to Automatic Evaluation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversity can be estimated using statistical measures such as perplexity, measuring language quality requires human evaluation. However, because human evaluation at scale is slow and expensive, it is used sparingly; it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for machine translation. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a kernel function. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that \u2013 on average \u2013 the quality estimation from a BLEU Neighbors model has a lower mean squared error and higher Spearman correlation with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art models on automatically grading essays, including models that have access to a gold-standard reference essay.", 
    "year": 2020, 
    "venue": "EVAL4NLP", 
    "references": 30, 
    "authors": [
      "Kawin Ethayarajh", 
      "D. Sadigh"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.232", 
    "title": "Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In order to facilitate natural language understanding, the key is to engage commonsense or background knowledge. However, how to engage commonsense effectively in question answering systems is still under exploration in both research academia and industry. In this paper, we propose a novel question-answering method by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and the Cambridge Dictionary, to boost the performance. More concretely, we first introduce a novel graph-based iterative knowledge retrieval module, which iteratively retrieves concepts and entities related to the given question and its choices from multiple knowledge sources. Afterward, we use a pre-trained language model to encode the question, retrieved knowledge and choices, and propose an answer choice-aware attention mechanism to fuse all hidden representations of the previous modules. Finally, the linear classifier for specific tasks is used to predict the answer. Experimental results on the CommonsenseQA dataset show that our method significantly outperforms other competitive methods and achieves the new state-of-the-art. In addition, further ablation studies demonstrate the effectiveness of our graph-based iterative knowledge retrieval module and the answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 40, 
    "authors": [
      "Qianglong Chen", 
      "Feng Ji", 
      "Haiqing Chen", 
      "Yin Zhang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1306", 
    "title": "Enriching Wikidata with Frame Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Wikidata is a large-scale, multilingual and freely available knowledge base. It contains more than 14 million facts, however, it is still missing linguistic information. In this paper, we aim to bridge this gap by aligning Wikidata with FrameNet lexicon. We propose an approach based on word embedding to identify a mapping between Wikidata relations, called properties, and FrameNet frames and to annotate the arguments of each relation with the semantic roles of the matching frames. Early empirical results show the advantage of our approach compared to other baseline methods.", 
    "year": 2016, 
    "venue": "AKBC@NAACL-HLT", 
    "references": 20, 
    "authors": [
      "Hatem Mousselly Sergieh", 
      "Iryna Gurevych"
    ], 
    "topics": [
      "Wikidata", 
      "FrameNet", 
      "Word embedding", 
      "Semantic role labeling", 
      "Knowledge base", 
      "Baseline (configuration management)", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.8", 
    "title": "Quantifying the Effects of COVID-19 on Mental Health Support Forums", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 64, 
    "authors": [
      "Laura Biester", 
      "K. Matton", 
      "Janarthanan Rajendran", 
      "E. Provost", 
      "Rada Mihalcea"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1503", 
    "title": "The Importance of Being Recurrent for Modeling Hierarchical Structure", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures\u2014recurrent versus non-recurrent\u2014with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan_vs_rnn", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 19, 
    "authors": [
      "Ke M. Tran", 
      "Arianna Bisazza", 
      "Christof Monz"
    ], 
    "topics": [
      "Recurrent neural network", 
      "Natural language processing", 
      "Neural machine translation", 
      "Language model", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-2001", 
    "title": "Resolving Event Coreference with Supervised Representation Learning and Clustering-Oriented Regularization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present an approach to event coreference resolution by developing a general framework for clustering that uses supervised representation learning. We propose a neural network architecture with novel Clustering-Oriented Regularization (CORE) terms in the objective function. These terms encourage the model to create embeddings of event mentions that are amenable to clustering. We then use agglomerative clustering on these embeddings to build event coreference chains. For both within- and cross-document coreference on the ECB+ corpus, our model obtains better results than models that require significantly more pre-annotated information. This work provides insight and motivating results for a new general approach to solving coreference and clustering problems with representation learning.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 35, 
    "authors": [
      "Kian Kenyon-Dean", 
      "J. C. K. Cheung", 
      "Doina Precup"
    ], 
    "topics": [
      "Cluster analysis", 
      "Feature learning", 
      "Loss function", 
      "Computer cluster", 
      "Artificial neural network", 
      "Feature engineering", 
      "Algorithm", 
      "Cross entropy", 
      "Network architecture", 
      "Feature vector", 
      "End-to-end principle", 
      "End system", 
      "Optimization problem", 
      "Matrix regularization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.codi-1.10", 
    "title": "Joint Modeling of Arguments for Event Understanding", 
    "fields_of_study": [
      "Medicine", 
      "Computer Science"
    ], 
    "abstract": "We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.", 
    "year": 2020, 
    "venue": "CODI", 
    "references": 29, 
    "authors": [
      "Yunmo Chen", 
      "Tongfei Chen", 
      "Benjamin van Durme"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4651", 
    "title": "User Adaptive Restoration for Incorrectly-Segmented Utterances in Spoken Dialogue Systems", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Ideally, the users of spoken dialogue systems should be able to speak at their own tempo. The systems thus need to correctly interpret utterances from various users, even when these utterances contain disfluency. In response to this issue, we propose an approach based on a posteriori restoration for incorrectly segmented utterances. A crucial part of this approach is to classify whether restoration is required or not. We improve the accuracy by adapting the classifier to each user. We focus on the dialogue tempo of each user, which can be obtained during dialogues, and determine the correlation between each user\u2019s tempo and the appropriate thresholds for the classification. A linear regression function used to convert the tempos into thresholds is also derived. Experimental results showed that the proposed user adaptation for two classifiers, thresholding and decision tree, improved the classification accuracies by 3.0% and 7.4%, respectively, in ten-fold cross validation.", 
    "year": 2015, 
    "venue": "SIGDIAL Conference", 
    "references": 18, 
    "authors": [
      "Kazunori Komatani", 
      "Naoki Hotta", 
      "Satoshi Sato", 
      "M. Nakano"
    ], 
    "topics": [
      "Circuit restoration", 
      "Dialog system", 
      "Decision tree", 
      "Thresholding (image processing)", 
      "Baseline (configuration management)", 
      "Statistical classification", 
      "Computer user satisfaction", 
      "Cross-validation (statistics)", 
      "Experiment", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6311", 
    "title": "Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent works in neural machine translation have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce datasets extracted from Europarl v7 and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of BLEU and manual evaluation.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 30, 
    "authors": [
      "Sameen Maruf", 
      "Andr\u00e9 F. T. Martins", 
      "Gholamreza Haffari"
    ], 
    "topics": [
      "BLEU", 
      "Neural machine translation", 
      "Experiment", 
      "Logical connective", 
      "Functional discourse grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2103", 
    "title": "Labelling Topics using Unsupervised Graph-based Methods", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 22, 
    "authors": [
      "Nikolaos Aletras", 
      "Mark Stevenson"
    ], 
    "topics": [
      "PageRank", 
      "Web search engine"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118693.1118721", 
    "title": "A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.", 
    "year": 2002, 
    "venue": "EMNLP", 
    "references": 22, 
    "authors": [
      "Michael Thelen", 
      "E. Riloff"
    ], 
    "topics": [
      "Lexicon", 
      "Bootstrapping (compilers)", 
      "Algorithm", 
      "Text corpus", 
      "Protologism"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2056", 
    "title": "GW_QA at SemEval-2017 Task 3: Question Answer Re-ranking on Arabic Fora", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our submission to SemEval-2017 Task 3 Subtask D, \u201cQuestion Answer Ranking in Arabic Community Question Answering\u201d. In this work, we applied a supervised machine learning approach to automatically re-rank a set of QA pairs according to their relevance to a given question. We employ features based on latent semantic models, namely WTMF, as well as a set of lexical features based on string lengths and surface level matching. The proposed system ranked first out of 3 submissions, with a MAP score of 61.16%.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 12, 
    "authors": [
      "Nada AlMarwani", 
      "Mona T. Diab"
    ], 
    "topics": [
      "Logistic regression", 
      "Machine learning", 
      "Map", 
      "Supervised learning", 
      "Question answering", 
      "Binary classification", 
      "String (computer science)", 
      "Relevance", 
      "GW-BASIC", 
      "SystemVerilog", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K17-3016", 
    "title": "A non-projective greedy dependency parser with bidirectional LSTMs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of the Covington (2001) algorithm for non-projective dependency parsing. The bidirectional LSTM approach by Kipperwasser and Goldberg (2016) is used to train a greedy parser with a dynamic oracle to mitigate error propagation. The model participated in the CoNLL 2017 UD Shared Task. In spite of not using any ensemble methods and using the baseline segmentation and PoS tagging, the parser obtained good results on both macro-average LAS and UAS in the big treebanks category (55 languages), ranking 7th out of 33 teams. In the all treebanks category (LAS and UAS) we ranked 16th and 12th. The gap between the all and big categories is mainly due to the poor performance on four parallel PUD treebanks, suggesting that some `suffixed' treebanks (e.g. Spanish-AnCora) perform poorly on cross-treebank settings, which does not occur with the corresponding `unsuffixed' treebank (e.g. Spanish). By changing that, we obtain the 11th best LAS among all runs (official and unofficial). The code is made available at this https URL", 
    "year": 2017, 
    "venue": "CoNLL Shared Task", 
    "references": 35, 
    "authors": [
      "David Vilares", 
      "Carlos G\u00f3mez-Rodr\u00edguez"
    ], 
    "topics": [
      "Treebank", 
      "Greedy algorithm", 
      "Long short-term memory", 
      "Parsing", 
      "Baseline (configuration management)", 
      "Built-in self-test", 
      "Word embedding", 
      "Unmanned aerial vehicle", 
      "Propagation of uncertainty", 
      "Transition system", 
      "Ensemble learning", 
      "End-to-end principle", 
      "Approximation algorithm", 
      "HTTPS", 
      "Software propagation", 
      "USB Attached SCSI", 
      "Urban Dictionary", 
      "Part-of-speech tagging", 
      "libLAS"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.567", 
    "title": "SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 70, 
    "authors": [
      "Pei Ke", 
      "Haozhe Ji", 
      "Siyang Liu", 
      "Xiaoyan Zhu", 
      "Minlie Huang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4904", 
    "title": "Harvesting Creative Templates for Generating Stylistically Varied Restaurant Reviews", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Many of the creative and figurative elements that make language exciting are lost in translation in current natural language generation engines. In this paper, we explore a method to harvest templates from positive and negative reviews in the restaurant domain, with the goal of vastly expanding the types of stylistic variation available to the natural language generator. We learn hyperbolic adjective patterns that are representative of the strongly-valenced expressive language commonly used in either positive or negative reviews. We then identify and delexicalize entities, and use heuristics to extract generation templates from review sentences. We evaluate the learned templates against more traditional review templates, using subjective measures of \"convincingness\", \"interestingness\", and \"naturalness\". Our results show that the learned templates score highly on these measures. Finally, we analyze the linguistic categories that characterize the learned positive and negative templates. We plan to use the learned templates to improve the conversational style of dialogue systems in the restaurant domain.", 
    "year": 2017, 
    "venue": "ArXiv", 
    "references": 34, 
    "authors": [
      "Shereen Oraby", 
      "Sheideh Homayon", 
      "M. Walker"
    ], 
    "topics": [
      "Natural language generation", 
      "Heuristic (computer science)", 
      "Dialog system", 
      "Entity", 
      "Formal language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2316", 
    "title": "Predicting Discharge Disposition Using Patient Complaint Notes in Electronic Medical Records", 
    "fields_of_study": [
      "Computer Science", 
      "Medicine"
    ], 
    "abstract": "Overcrowding in emergency rooms is a major challenge faced by hospitals across the United States. Overcrowding can result in longer wait times, which, in turn, has been shown to adversely affect patient satisfaction, clinical outcomes, and procedure reimbursements. This paper presents research that aims to automatically predict discharge disposition of patients who received medical treatment in an emergency department. We make use of a corpus that consists of notes containing patient complaints, diagnosis information, and disposition, entered by health care providers. We use this corpus to develop a model that uses the complaint and diagnosis information to predict patient disposition. We show that the proposed model substantially outperforms the baseline of predicting the most common disposition type. The long-term goal of this research is to build a model that can be implemented as a real-time service in an application to predict disposition as patients arrive.", 
    "year": 2018, 
    "venue": "BioNLP", 
    "references": 12, 
    "authors": [
      "M. Salimi", 
      "A. Rozovskaya"
    ], 
    "topics": [
      "Discharger", 
      "Text corpus", 
      "Baseline (configuration management)", 
      "Real-time transcription"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5614", 
    "title": "CAS: French Corpus with Clinical Cases", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations.", 
    "year": 2018, 
    "venue": "Louhi@EMNLP", 
    "references": 39, 
    "authors": [
      "N. Grabar", 
      "V. Claveau", 
      "Cl\u00e9ment Dalloux"
    ], 
    "topics": [
      "Text corpus", 
      "Confidentiality", 
      "Expect", 
      "Archive", 
      "Scientific literature", 
      "Correctness (computer science)", 
      "Part-of-speech tagging", 
      "Natural language processing", 
      "Ambiguous name resolution"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-5908", 
    "title": "Verbal Behaviors and Persuasiveness in Online Multimedia Content", 
    "fields_of_study": [
      "Psychology", 
      "Computer Science"
    ], 
    "abstract": "Persuasive communication is an essential component of our daily lives, whether it is negotiating, reviewing a product, or campaigning for the acceptance of a point of view. With the rapid expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ever more important and useful to understand persuasiveness in social multimedia content. In this paper we present a novel analysis of verbal behavior, based on lexical usage and paraverbal markers of hesitation, in the context of predicting persuasiveness in online multimedia content. Toward the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a positive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal behavior can be a significant predictor of persuasiveness in such online multimedia content.", 
    "year": 2014, 
    "venue": "SocialNLP@COLING", 
    "references": 21, 
    "authors": [
      "Moitreya Chatterjee", 
      "Sunghyun Park", 
      "H. Shim", 
      "Kenji Sagae", 
      "Louis-Philippe Morency"
    ], 
    "topics": [
      "Bigram", 
      "Experiment", 
      "Social media", 
      "Point of View (computer hardware company)", 
      "Kerrison Predictor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1129", 
    "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "J. Devlin", 
      "Rabih Zbib", 
      "Zhongqiang Huang", 
      "Thomas Lamar", 
      "R. Schwartz", 
      "J. Makhoul"
    ], 
    "topics": [
      "Artificial neural network", 
      "Statistical machine translation", 
      "BLEU", 
      "Baseline (configuration management)", 
      "Language model", 
      "Computation", 
      "Utility functions on indivisible goods", 
      "I2O"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1139", 
    "title": "Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model. Experiments on Chinese-English translation show that our approach leads to significant improvements.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 27, 
    "authors": [
      "Jiacheng Zhang", 
      "Yang Liu", 
      "Huanbo Luan", 
      "J. Xu", 
      "Maosong Sun"
    ], 
    "topics": [
      "Neural machine translation", 
      "Knowledge integration", 
      "Matrix regularization", 
      "Linear model", 
      "Log-linear model", 
      "End-to-end principle", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1034", 
    "title": "Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we automatically create sentiment dictionaries for predicting financial outcomes. We compare three approaches: (i) manual adaptation of the domain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a combination consisting of first manual, then automatic adaptation. In our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. In particular, automatic adaptation performs better than manual adaptation. In our analysis, we find that annotation based on an expert\u2019s a priori belief about a word\u2019s meaning can be incorrect \u2013 annotation should be performed based on the word\u2019s contexts in the target domain instead.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 49, 
    "authors": [
      "Marina Sedinkina", 
      "Nikolas Breitkopf", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "Domain adaptation", 
      "Data dictionary", 
      "Experiment", 
      "Volatility", 
      "Offset binary", 
      "Text corpus", 
      "Automatic control", 
      "Java annotation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1042", 
    "title": "Combining Natural Logic and Shallow Reasoning for Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Broad domain question answering is often difficult in the absence of structured knowledge bases, and can benefit from shallow lexical methods (broad coverage) and logical reasoning (high precision). We propose an approach for incorporating both of these signals in a unified framework based on natural logic. We extend the breadth of inferences afforded by natural logic to include relational entailment (e.g., buy \u2192 own) and meronymy (e.g., a person born in a city is born the city\u2019s country). Furthermore, we train an evaluation function \u2013 akin to gameplaying \u2013 to evaluate the expected truth of candidate premises on the fly. We evaluate our approach on answering multiple choice science questions, achieving strong results on the dataset.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 45, 
    "authors": [
      "Gabor Angeli", 
      "Neha Nayak Kennard", 
      "Christopher D. Manning"
    ], 
    "topics": [
      "Question answering", 
      "Evaluation function", 
      "Natural language", 
      "Unified Framework", 
      "On the fly", 
      "Gabor filter", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-2391", 
    "title": "Word embeddings and discourse information for Quality Estimation", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "In this paper we present the results of the University of Sheffield (SHEF) submissions for the WMT16 shared task on document-level Quality Estimation (Task 3). Our submission explore discourse and document-aware information and word embeddings as features, with Support Vector Regression and Gaussian Process used to train the Quality Estimation models. The use of word embeddings (combined with baseline features) and a Gaussian Process model with two kernels led to the winning submission in the shared task.", 
    "year": 2016, 
    "venue": "WMT", 
    "references": 28, 
    "authors": [
      "Carolina Scarton", 
      "Daniel Beck", 
      "Kashif Shah", 
      "Karin Sim Smith", 
      "Lucia Specia"
    ], 
    "topics": [
      "Word embedding", 
      "Gaussian process", 
      "Microsoft Word for Mac", 
      "Support vector machine", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4122", 
    "title": "Sub-character Neural Language Modelling in Japanese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In East Asian languages such as Japanese and Chinese, the semantics of a character are (somewhat) reflected in its sub-character elements. This paper examines the effect of using sub-characters for language modeling in Japanese. This is achieved by decomposing characters according to a range of character decomposition datasets, and training a neural language model over variously decomposed character representations. Our results indicate that language modelling can be improved through the inclusion of sub-characters, though this result depends on a good choice of decomposition dataset and the appropriate granularity of decomposition.", 
    "year": 2017, 
    "venue": "SWCN@EMNLP", 
    "references": 18, 
    "authors": [
      "Nguyen", 
      "J. Brooke", 
      "Timothy Baldwin"
    ], 
    "topics": [
      "Language model", 
      "Perplexity", 
      "Languages", 
      "Part-of-speech tagging", 
      "Personality Character", 
      "Subgroup", 
      "Neural Network Simulation", 
      "Silo (dataset)", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.71", 
    "title": "Evaluating the Efficacy of Summarization Evaluation across Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While automatic summarization evaluation methods developed for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 39, 
    "authors": [
      "Fajri Koto", 
      "Jey Han Lau", 
      "Timothy Baldwin"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1140", 
    "title": "Machine Translation for Machines: the Sentiment Classification Use Case", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (\u201chuman-oriented\u201d quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a \u201cmachine-oriented\u201d criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with \u201cmachine-oriented\u201d translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 30, 
    "authors": [
      "Amirhossein Tebbifakhr", 
      "L. Bentivogli", 
      "M. Negri", 
      "M. Turchi"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Neural machine translation", 
      "Natural language processing", 
      "Downstream (software development)", 
      "Approximation algorithm", 
      "Sampling (signal processing)", 
      "General-purpose modeling"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.12", 
    "title": "Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topicbased methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compare traditional topic models based on word tokens with topic models based on medical concepts, and propose several ways to improve topic coherence and specificity.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 23, 
    "authors": [
      "Yulia Otmakhova", 
      "K. Verspoor", 
      "Timothy Baldwin", 
      "Simon Suster", 
      "Jey Han Lau"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1178", 
    "title": "Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 29, 
    "authors": [
      "Yizhong Wang", 
      "Kai Liu", 
      "Jing Liu", 
      "W. He", 
      "Yajuan Lyu", 
      "Hua Wu", 
      "Sujian Li", 
      "Haifeng Wang"
    ], 
    "topics": [
      "Natural language understanding", 
      "List comprehension", 
      "Shroud of the Avatar:", 
      "Web search engine", 
      "End-to-end principle", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.3115/1119296.1119302", 
    "title": "Parmenides: An Opportunity for ISO TC37 SC4?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Despite the many initiatives in recent years aimed at creating Language Engineering standards, it is often the case that different projects use different approaches and often define their own standards. Even within the same project it often happens that different tools will require different ways to represent their linguistic data.In a recently started EU project focusing on the integration of Information Extraction and Data Mining techniques, we aim at avoiding the problem of incompatibility among different tools by defining a Common Annotation Scheme internal to the project. However, when the project was started (Sep 2002) we were unaware of the standardization effort of ISO TC37/SC4, and so we commenced once again trying to define our own schema. Fortunately, as this work is still at an early stage (the project will last till 2005) it is still possible to redirect it in a way that it will be compatible with the standardization work of ISO. In this paper we describe the status of the work in the project and explore possible synergies with the work in ISO TC37 SC4.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 4, 
    "authors": [
      "Fabio Rinaldi", 
      "James Dowdall", 
      "Michael Hess", 
      "K. Kaljurand", 
      "Andreas Persidis"
    ], 
    "topics": [
      "Isoproterenol", 
      "Switzerland", 
      "Information extraction", 
      "Reinventing the wheel", 
      "Open road tolling", 
      "Magdeburg", 
      "standards characteristics", 
      "Information management", 
      "Computational linguistics", 
      "Software incompatibility", 
      "Documentation", 
      "Data mining", 
      "Synergy", 
      "ISO/IEC 646", 
      "Unevaluable", 
      "Subgroup", 
      "Diabetes Insipidus", 
      "Computation", 
      "Ploidies", 
      "Redirection (computing)", 
      "Limited stage (cancer stage)", 
      "ARHGEF5 gene", 
      "cellular targeting"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2197", 
    "title": "UPV-28-UNITO at SemEval-2019 Task 7: Exploiting Post\u2019s Nesting and Syntax Information for Rumor Stance Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the present paper we describe the UPV-28-UNITO system\u2019s submission to the RumorEval 2019 shared task. The approach we applied for addressing both the subtasks of the contest exploits both classical machine learning algorithms and word embeddings, and it is based on diverse groups of features: stylistic, lexical, emotional, sentiment, meta-structural and Twitter-based. A novel set of features that take advantage of the syntactic information in texts is moreover introduced in the paper.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 31, 
    "authors": [
      "Bilal Ghanem", 
      "Alessandra Teresa Cignarella", 
      "C. Bosco", 
      "P. Rosso", 
      "F. M. R. Pardo"
    ], 
    "topics": [
      "SemEval", 
      "Veracity", 
      "Machine learning", 
      "Word embedding", 
      "Lexicon", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1048", 
    "title": "Automatic Cross-Lingual Similarization of Dependency Grammars for Tree-based Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Structural isomorphism between languages benefits the performance of cross-lingual applications. We propose an automatic algorithm for cross-lingual similarization of dependency grammars, which automatically learns grammars with high cross-lingual similarity. The algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classification decisions, and gradually improves the cross-lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning. The dependency grammars given by cross-lingual similarization have much higher cross-lingual similarity while maintaining non-triviality. As applications, the cross-lingually similarized grammars significantly improve the performance of dependency tree-based machine translation.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 43, 
    "authors": [
      "Wenbin Jiang", 
      "Wen Zhang", 
      "Jinan Xu", 
      "Rangjia Cai"
    ], 
    "topics": [
      "Machine translation", 
      "Dependency grammar", 
      "Algorithm", 
      "Graph (discrete mathematics)", 
      "Iterative method", 
      "Context-free grammar", 
      "Hard coding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1320", 
    "title": "Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks\u2013sentiment transfer, and formality transfer\u2013show that our model outperforms state-of-the-art approaches.Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 43, 
    "authors": [
      "Hongyu Gong", 
      "S. Bhat", 
      "Lingfei Wu", 
      "Jinjun Xiong", 
      "W. Hwu"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Discriminator", 
      "Parallel text", 
      "Interpreter (computing)", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.400", 
    "title": "Living Machines: A study of atypical animacy", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes a new approach to animacy detection, the task of determining whether an entity is represented as animate in a text. In particular, this work is focused on atypical animacy and examines the scenario in which typically inanimate objects, specifically machines, are given animate attributes. To address it, we have created the first dataset for atypical animacy detection, based on nineteenth-century sentences in English, with machines represented as either animate or inanimate. Our method builds on recent innovations in language modeling, specifically BERT contextualized word embeddings, to better capture fine-grained contextual properties of words. We present a fully unsupervised pipeline, which can be easily adapted to different contexts, and report its performance on an established animacy dataset and our newly introduced resource. We show that our method provides a substantially more accurate characterization of atypical animacy, especially when applied to highly complex forms of language use.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 48, 
    "authors": [
      "Mariona Coll Ardanuy", 
      "F. Nanni", 
      "K. Beelen", 
      "Kasra Hosseini", 
      "R. Ahnert", 
      "J. Lawrence", 
      "Katherine McDonough", 
      "Giorgia Tolfo", 
      "D. Wilson", 
      "Barbara McGillivray"
    ], 
    "topics": [
      "Language model", 
      "Word embedding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1393", 
    "title": "Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 30, 
    "authors": [
      "Anjalie Field", 
      "Doron Kliger", 
      "S. Wintner", 
      "Jennifer Pan", 
      "Dan Jurafsky", 
      "Yulia Tsvetkov"
    ], 
    "topics": [
      "Framing (World Wide Web)", 
      "Computation", 
      "Language technology", 
      "Trust (emotion)", 
      "Entity", 
      "Theory", 
      "Framing (social sciences)", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.3115/1119250.1119272", 
    "title": "Chinese Word Segmentation at Peking University", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word segmentation is the first step in Chinese information processing, and the performance of the segmenter, therefore, has a direct and great influence on the processing steps that follow. Different segmenters will give different results when handling issues like word boundary. And we will present in this paper that there is no need for an absolute definition of word boundary for all segmenters, and that different results of segmentation shall be acceptable if they can help to reach a correct syntactic analysis in the end.", 
    "year": 2003, 
    "venue": "", 
    "references": 1, 
    "authors": [
      "Du Huiming", 
      "Bai Xiaojing", 
      "Chang Baobao", 
      "Y. Shiwen"
    ], 
    "topics": [
      "Text segmentation", 
      "Microsoft Word for Mac", 
      "Dyslexia, Acquired", 
      "biologic segmentation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1212", 
    "title": "Fine Grained Citation Span for References in Wikipedia", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "\\emph{Verifiability} is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content. For a Wikipedia article, determining the \\emph{citation span} of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing. \nWe are the first to address the problem of determining the \\emph{citation span} in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered by a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a fine-grained level. \nWe provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 23, 
    "authors": [
      "B. Fetahu", 
      "K. Markert", 
      "Avishek Anand"
    ], 
    "topics": [
      "Wikipedia", 
      "Formal verification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-3603", 
    "title": "Joint Online Spoken Language Understanding and Language Modeling With Recurrent Neural Networks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.", 
    "year": 2016, 
    "venue": "SIGDIAL Conference", 
    "references": 26, 
    "authors": [
      "Bing Liu", 
      "I. Lane"
    ], 
    "topics": [
      "Language model", 
      "Recurrent neural network", 
      "Natural language understanding", 
      "Artificial neural network", 
      "Perplexity", 
      "Automatic Transmitter Identification System (television)", 
      "F1 score", 
      "Dialog system", 
      "Network model", 
      "Experiment", 
      "Elegant degradation", 
      "Protologism", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1210", 
    "title": "Discourse Coherence: Concurrent Explicit and Implicit Relations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments \u2013 some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "H. Rohde", 
      "Alexander Johnson", 
      "Nathan Schneider", 
      "B. Webber"
    ], 
    "topics": [
      "Parsing", 
      "Coherence (physics)", 
      "Naivety"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-2335", 
    "title": "Investigating the Documentation of Electronic Cigarette Use in the Veteran Affairs Electronic Health Record: A Pilot Study", 
    "fields_of_study": [
      "Computer Science", 
      "Medicine"
    ], 
    "abstract": "In this paper, we present pilot work on characterising the documentation of electronic cigarettes (e-cigarettes) in the United States Veterans Administration Electronic Health Record. The Veterans Health Administration is the largest health care system in the United States with 1,233 health care facilities nationwide, serving 8.9 million veterans per year. We identified a random sample of 2000 Veterans Administration patients, coded as current tobacco users, from 2008 to 2014. Using simple keyword matching techniques combined with qualitative analysis, we investigated the prevalence and distribution of e-cigarette terms in these clinical notes, discovering that for current smokers, 11.9% of patient records contain an ecigarette related term.", 
    "year": 2017, 
    "venue": "BioNLP", 
    "references": 23, 
    "authors": [
      "D. Mowery", 
      "B. South", 
      "Olga V. Patterson", 
      "Shu-Hong Zhu", 
      "Mike Conway"
    ], 
    "topics": [
      "Documentation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-1006", 
    "title": "Identification of Caused Motion Construction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This research describes the development of a supervised classifier of English Caused Motion Constructions (CMCs) (e.g. The goalie kicked the ball into the field). Consistent identification of CMCs is a necessary step to a correct interpretation of semantics for sentences where the verb does not conform to the expected semantics of the verb (e.g. The crowd laughed the clown off the stage). We expand on a previous study on the classification CMCs (Hwang et al., 2010) to show that CMCs can be successfully identified in the corpus data. In this paper, we present the classifier and the series of experiments carried out to improve its performance.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 16, 
    "authors": [
      "Jena D. Hwang", 
      "Martha Palmer"
    ], 
    "topics": [
      "Experiment", 
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974499.974540", 
    "title": "Tagging and Alignment of Parallel Texts: Current Status of BCP", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Access to on-line corpora is a useful tool for studies in lexicography, linguistics, and translation. Many means of accessing such corpora are available, but few, if any, provide more than a language for matching character strings. As a result, the user is obliged to spend a great deal of time extracting information herself. As more and more texts are put in machine readable format, it becomes increasingly obvious that more specialized, intelligent tools are required to fully exploit the available data. BCP, the Bilingual Concordancy Program under development at ISSCO, is an instance of such a tool. In previous work done at ISSCO on BCP, a rather oversimplified view of text structure was taken [Warwick et. al., 1989]. Attention was focused on the difficulties of alignment and somewhat less so on access questions. Alignment remains a subject of active research, but experience has proven that text marking and morphology are not to be taken so lightly. Indeed, many small difficulties have shown themselves to be insurmountable without the aid of heuristic decision modules. As a result, the initial approach to text tagging and morphology has been thoroughly revised.", 
    "year": 1992, 
    "venue": "ANLP", 
    "references": 8, 
    "authors": [
      "A. Winarske", 
      "Susan Warwick-Armstrong", 
      "Jan Hajic"
    ], 
    "topics": [
      "Bulk Copy Program", 
      "Lexicography", 
      "Heuristic", 
      "Text corpus", 
      "Galaxy morphological classification", 
      "Online and offline", 
      "Human-readable medium", 
      "Item Unique Identification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1176", 
    "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 18, 
    "authors": [
      "Wenjuan Han", 
      "Yong Jiang", 
      "Kewei Tu"
    ], 
    "topics": [
      "Dependency grammar", 
      "Grammar induction", 
      "Text corpus", 
      "Wikipedia", 
      "Big data"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1077", 
    "title": "Analyzing Stemming Approaches for Turkish Multi-Document Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this study, we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization (MDS) on Turkish, which is an agglutinative and morphologically rich language. We constructed a manually annotated MDS data set, and to our best knowledge, reported the first results on Turkish MDS. Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming, whereas applying complex morphological analysis does not improve Turkish MDS.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 20, 
    "authors": [
      "Muhammed Yavuz Nuzumlali", 
      "Arzucan \u00d6zg\u00fcr"
    ], 
    "topics": [
      "Stemming", 
      "Automatic summarization", 
      "Multi-document summarization", 
      "Truncation", 
      "Algorithm", 
      "Futures studies", 
      "MDS matrix"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.EMNLP-MAIN.246", 
    "title": "Learning a Cost-Effective Annotation Policy for Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 45, 
    "authors": [
      "Bernhard Kratzwald", 
      "S. Feuerriegel", 
      "Huan Sun"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2203", 
    "title": "Fermi at SemEval-2019 Task 8: An elementary but effective approach to Question Discernment in Community QA Forums", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Online Community Question Answering Forums (cQA) have gained massive popularity within recent years. The rise in users for such forums have led to the increase in the need for automated evaluation for question comprehension and fact evaluation of the answers provided by various participants in the forum. Our team, Fermi, participated in sub-task A of Task 8 at SemEval 2019 - which tackles the first problem in the pipeline of factual evaluation in cQA forums, i.e., deciding whether a posed question asks for a factual information, an opinion/advice or is just socializing. This information is highly useful in segregating factual questions from non-factual ones which highly helps in organizing the questions into useful categories and trims down the problem space for the next task in the pipeline for fact evaluation among the available answers. Our system uses the embeddings obtained from Universal Sentence Encoder combined with XGBoost for the classification sub-task A. We also evaluate other combinations of embeddings and off-the-shelf machine learning algorithms to demonstrate the efficacy of the various representations and their combinations. Our results across the evaluation test set gave an accuracy of 84% and received the first position in the final standings judged by the organizers.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 31, 
    "authors": [
      "B. Syed", 
      "Vijayasaradhi Indurthi", 
      "Manish Shrivastava", 
      "Manish Gupta", 
      "Vasudeva Varma"
    ], 
    "topics": [
      "SemEval", 
      "Machine learning", 
      "Language model", 
      "XGBoost", 
      "Question answering", 
      "Text corpus", 
      "Algorithm", 
      "Encoder", 
      "Online community", 
      "Problem domain", 
      "List comprehension", 
      "Socialization", 
      "Organizing (structure)", 
      "Test set", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6532", 
    "title": "Generation of Company descriptions using concept-to-text and text-to-text deep models: dataset collection and systems evaluation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we study the performance of several state-of-the-art sequence-to-sequence models applied to generation of short company descriptions. The models are evaluated on a newly created and publicly available company dataset that has been collected from Wikipedia. The dataset consists of around 51K company descriptions that can be used for both concept-to-text and text-to-text generation tasks. Automatic metrics and human evaluation scores computed on the generated company descriptions show promising results despite the difficulty of the task as the dataset (like most available datasets) has not been originally designed for machine learning. In addition, we perform correlation analysis between automatic metrics and human evaluations and show that certain automatic metrics are more correlated to human judgments.", 
    "year": 2018, 
    "venue": "INLG", 
    "references": 22, 
    "authors": [
      "Raheel Qader", 
      "Khoder Jneid", 
      "F. Portet", 
      "Cyril Labb\u00e9"
    ], 
    "topics": [
      "Wikipedia", 
      "Machine learning", 
      "Natural language generation", 
      "Automatic control"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K19-2015", 
    "title": "\u00daFAL-Oslo at MRP 2019: Garage Sale Semantic Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the \u00daFAL\u2013Oslo system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP, Oepen et al. 2019). The submission is based on several third-party parsers. Within the official shared task results, the submission ranked 11th out of 13 participating systems.", 
    "year": 2019, 
    "venue": "CoNLL Shared Task", 
    "references": 25, 
    "authors": [
      "Kira Droganova", 
      "Andrey Kutuzov", 
      "Nikita Mediankin", 
      "Daniel Zeman"
    ], 
    "topics": [
      "Parsing", 
      "Media Redundancy Protocol", 
      "Adaptive Multi-Rate audio codec", 
      "Experiment", 
      "Software release life cycle", 
      "Electronic Document System", 
      "Third-party software component"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N16-1083", 
    "title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In many languages, sparse availability of resources causes numerous challenges for textual analysis tasks. Text classification is one of such standard tasks that is hindered due to limited availability of label information in lowresource languages. Transferring knowledge (i.e. label information) from high-resource to low-resource languages might improve text classification as compared to the other approaches like machine translation. We introduce BRAVE (Bilingual paRAgraph VEctors), a model to learn bilingual distributed representations (i.e. embeddings) of words without word alignments either from sentencealigned parallel or label-aligned non-parallel document corpora to support cross-language text classification. Empirical analysis shows that classification models trained with our bilingual embeddings outperforms other stateof-the-art systems on three different crosslanguage text classification tasks.", 
    "year": 2016, 
    "venue": "NAACL", 
    "references": 50, 
    "authors": [
      "Aditya Mogadala", 
      "Achim Rettinger"
    ], 
    "topics": [
      "Document classification", 
      "Text corpus", 
      "Parallel text", 
      "Machine translation", 
      "Limited availability", 
      "Sparse matrix"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.221", 
    "title": "Sentiment Forecasting in Dialog", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentiment forecasting in dialog aims to predict the polarity of next utterance to come, and can help speakers revise their utterances in sentimental utterances generation. However, the polarity of next utterance is normally hard to predict, due to the lack of content of next utterance (yet to come). In this study, we propose a Neural Sentiment Forecasting (NSF) model to address inherent challenges. In particular, we employ a neural simulation model to simulate the next utterance based on the context (previous utterances encountered). Moreover, we employ a sequence influence model to learn both pair-wise and seq-wise influence. Empirical studies illustrate the importance of proposed sentiment forecasting task, and justify the effectiveness of our NSF model over several strong baselines.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 24, 
    "authors": [
      "Z. Wang", 
      "Xiujun Zhu", 
      "Y. Zhang", 
      "Shoushan Li", 
      "Guodong Zhou"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-0424", 
    "title": "Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Propelling, and propelled by, the \u201cdeep learning revolution\u201d, recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the \u201clinked to same image\u201d relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.", 
    "year": 2019, 
    "venue": "IWCS", 
    "references": 30, 
    "authors": [
      "David Schlangen"
    ], 
    "topics": [
      "Natural language", 
      "Text corpus", 
      "Computational semantics", 
      "Computation", 
      "Deep learning", 
      "Ontology components", 
      "Entity", 
      "Regular expression", 
      "Semantic HTML", 
      "Cognitive science"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1574", 
    "title": "Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "Adam Fisch", 
      "J. Guo", 
      "R. Barzilay"
    ], 
    "topics": [
      "Biological anthropology", 
      "Parsing", 
      "Neural oscillation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1227", 
    "title": "Modelling the interplay of metaphor and emotion through multitask learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Metaphors allow us to convey emotion by connecting physical experiences and abstract concepts. The results of previous research in linguistics and psychology suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between metaphor and emotion within a computational framework, by proposing the first joint model of these phenomena. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 73, 
    "authors": [
      "Verna Dankers", 
      "Marek Rei", 
      "Martha Lewis", 
      "Ekaterina Shutova"
    ], 
    "topics": [
      "Interface metaphor", 
      "Computer multitasking"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.88", 
    "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as \"what happened before/after [some event]?\" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "Qiang Ning", 
      "Hao Wu", 
      "Rujun Han", 
      "Nanyun Peng", 
      "Matt Gardner", 
      "D. Roth"
    ], 
    "topics": [
      "TORQUE", 
      "List comprehension", 
      "Benchmark (computing)", 
      "Human reliability", 
      "Happened-before", 
      "Natural language understanding", 
      "Test set"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2098", 
    "title": "LIMSI-COT at SemEval-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 25, 
    "authors": [
      "Julien Tourille", 
      "Olivier Ferret", 
      "Xavier Tannier", 
      "Aur\u00e9lie N\u00e9v\u00e9ol"
    ], 
    "topics": [
      "SemEval", 
      "Information extraction", 
      "Relationship extraction", 
      "Domain adaptation", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-srw.20", 
    "title": "Pointwise Paraphrase Appraisal is Potentially Problematic", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models. As a first step towards that goal, we show that although the standard way of fine-tuning BERT for paraphrase identification by pairing two sentences as one sequence results in a model with state-of-the-art performance, that model may perform poorly on simple tasks like identifying pairs with two identical sentences. Moreover, we show that these models may even predict a pair of randomly-selected sentences with higher paraphrase score than a pair of identical ones.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 22, 
    "authors": [
      "Hannah Chen", 
      "Yangfeng Ji", 
      "David E. Evans"
    ], 
    "topics": [
      "Pointwise mutual information", 
      "Benchmark (computing)", 
      "Binary classification", 
      "Turing completeness", 
      "Downstream (software development)", 
      "Reversing: Secrets of Reverse Engineering", 
      "Randomness"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.29", 
    "title": "Cisco at SemEval-2021 Task 5: What\u2019s Toxic?: Leveraging Transformers for Multiple Toxic Span Extraction from Online Comments", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Social network platforms are generally used to share positive, constructive, and insightful content. However, in recent times, people often get exposed to objectionable content like threat, identity attacks, hate speech, insults, obscene texts, offensive remarks or bullying. Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories. This paper describes the system proposed by team Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared task focusing on detecting the spans in the text that attribute to its toxicity, in English language. We approach this problem primarily in two ways: a sequence tagging approach and a dependency parsing approach. In our sequence tagging approach we tag each token in a sentence under a particular tagging scheme. Our best performing architecture in this approach also proved to be our best performing architecture overall with an F1 score of 0.6922, thereby placing us 7th on the final evaluation phase leaderboard. We also explore a dependency parsing approach where we extract spans from the input sentence under the supervision of target span boundaries and rank our spans using a biaffine model. Finally, we also provide a detailed analysis of our results and model performance in our paper.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 49, 
    "authors": [
      "Sreyan Ghosh", 
      "Sonal Kumar"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.151", 
    "title": "Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 42, 
    "authors": [
      "Michael A. Lepori"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.355", 
    "title": "CxGBERT: BERT meets Construction Grammar", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While lexico-semantic elements no doubt capture a large amount of linguistic information, it has been argued that they do not capture all information contained in text. This assumption is central to constructionist approaches to language which argue that language consists of constructions, learned pairings of a form and a function or meaning that are either frequent or have a meaning that cannot be predicted from its component parts. BERT\u2019s training objectives give it access to a tremendous amount of lexico-semantic information, and while BERTology has shown that BERT captures certain important linguistic dimensions, there have been no studies exploring the extent to which BERT might have access to constructional information. In this work we design several probes and conduct extensive experiments to answer this question. Our results allow us to conclude that BERT does indeed have access to a significant amount of information, much of which linguists typically call constructional information. The impact of this observation is potentially far-reaching as it provides insights into what deep learning methods learn from text, while also showing that information contained in constructions is redundantly encoded in lexico-semantics.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 45, 
    "authors": [
      "Harish Tayyar Madabushi", 
      "Laurence Romain", 
      "Dagmar Divjak", 
      "P. Milin"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4110", 
    "title": "Predicting Attrition Along the Way: The UIUC Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Discussion forum and clickstream are two primary data streams that enable mining of student behavior in a massively open online course. A student\u2019s participation in the discussion forum gives direct access to the opinions and concerns of the student. However, the low participation (5-10%) in discussion forums, prompts the modeling of user behavior based on clickstream information. Here we study a predictive model for learner attrition on a given week using information mined just from the clickstream. Features that are related to the quiz attempt/submission and those that capture interaction with various course components are found to be reasonable predictors of attrition in a given week.", 
    "year": 2014, 
    "venue": "EMNLP 2014", 
    "references": 10, 
    "authors": [
      "Bussaba Amnueypornsakul", 
      "S. Bhat", 
      "Phakpoom Chinprutthiwong"
    ], 
    "topics": [
      "Attrition (website)", 
      "Clickstream", 
      "Massive open online course", 
      "Random access", 
      "Tooth Attrition", 
      "MinEd"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-INDUSTRY.7", 
    "title": "Best Practices for Data-Efficient Modeling in NLG:How to Train Production-Ready Neural Models with Less Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Natural language generation (NLG) is a critical component in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs. In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production. We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models (~2Mb) that we can reliably deploy in production. These models achieve the same quality as large pretrained models (~1Gb) as judged by human raters.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 30, 
    "authors": [
      "A. Arun", 
      "Soumya Batra", 
      "Vikas Bhardwaj", 
      "Ashwini Challa", 
      "Pinar Donmez", 
      "P. Heidari", 
      "Hakan Inan", 
      "Shashank Jain", 
      "Anuj Kumar", 
      "Shawn Mei", 
      "K. Mohan", 
      "Michael White"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-3601", 
    "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", 
    "year": 2016, 
    "venue": "SIGDIAL Conference", 
    "references": 40, 
    "authors": [
      "Tiancheng Zhao", 
      "M. Esk\u00e9nazi"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Dialog system", 
      "Natural language understanding", 
      "Hybrid algorithm", 
      "Supervised learning", 
      "Relational database", 
      "Scalability", 
      "20Q", 
      "Artificial neural network", 
      "Futures studies", 
      "End-to-end principle", 
      "Debugging", 
      "Baseline (configuration management)", 
      "Simulation", 
      "IBM Notes", 
      "SDS 940"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1080", 
    "title": "Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Yunsu Kim", 
      "P. Petrov", 
      "Pavel Petrushkov", 
      "Shahram Khadivi", 
      "H. Ney"
    ], 
    "topics": [
      "Neural machine translation", 
      "Encoder", 
      "BLEU", 
      "Smoothing", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2008.34.2.145", 
    "title": "Special Issue Introduction: Semantic Role Labeling: An Introduction to the Special Issue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Semantic role labeling, the computational identification and labeling of arguments in text, has become a leading task in computational linguistics today. Although the issues for this task have been studied for decades, the availability of large resources and the development of statistical machine learning methods have heightened the amount of effort in this field. This special issue presents selected and representative work in the field. This overview describes linguistic background of the problem, the movement from linguistic theories to computational practice, the major resources that are being used, an overview of steps taken in computational systems, and a description of the key issues and results in semantic role labeling (as revealed in several international evaluations). We assess weaknesses in semantic role labeling and identify important challenges facing the field. Overall, the opportunities and the potential for useful further research in semantic role labeling are considerable.", 
    "year": 2008, 
    "venue": "CL", 
    "references": 61, 
    "authors": [
      "Llu\u00eds M\u00e0rquez i Villodre", 
      "X. Carreras", 
      "K. Litkowski", 
      "Suzanne Stevenson"
    ], 
    "topics": [
      "Semantic role labeling", 
      "Computational linguistics", 
      "Computation", 
      "Machine learning", 
      "Theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-2014", 
    "title": "Detecting Translation Direction: A Cross-Domain Study", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Parallel corpora are constructed by taking a document authored in one language and translating it into another language. However, the information about the authored and translated sides of the corpus is usually not preserved. When available, this information can be used to improve statistical machine translation. Existing statistical methods for translation direction detection have low accuracy when applied to the realistic out-of-domain setting, especially when the input texts are short. Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to outperform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 24, 
    "authors": [
      "Sauleh Eetemadi", 
      "Kristina Toutanova"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Sensor", 
      "Text corpus", 
      "Computer cluster", 
      "Parallel text", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-2019", 
    "title": "Interactive Abstractive Summarization for Event News Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel interactive summarization system that is based on abstractive summarization, derived from a recent consolidated knowledge representation for multiple texts. We incorporate a couple of interaction mechanisms, providing a bullet-style summary while allowing to attain the most important information first and interactively drill down to more specific details. A usability study of our implementation, for event news tweets, suggests the utility of our approach for text exploration.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 11, 
    "authors": [
      "Ori Shapira", 
      "H. Ronen", 
      "M. Adler", 
      "Yael Amsterdamer", 
      "J. Bar-Ilan", 
      "Ido Dagan"
    ], 
    "topics": [
      "Automatic summarization", 
      "Usability testing", 
      "Knowledge representation and reasoning", 
      "Machine learning", 
      "Parser", 
      "Interactivity", 
      "sentence", 
      "Text corpus", 
      "Open knowledge", 
      "User interface", 
      "Unique Identifier", 
      "TimeLine Fluoride Releasing Resin"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2020.clinicalnlp-1.18", 
    "title": "Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT\u2019s F1 score was lower by 4 points on average than medical BERT variants.", 
    "year": 2020, 
    "venue": "CLINICALNLP", 
    "references": 18, 
    "authors": [
      "M. Abadeer"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1624", 
    "title": "Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries over a single table. We focus on the Spider dataset, a complex and cross-domain text-to-SQL task, which includes complex queries over multiple tables. In this paper, we propose a SQL clause-wise decoding neural architecture with a self-attention based database schema encoder to address the Spider task. Each of the clause-specific decoders consists of a set of sub-modules, which is defined by the syntax of each clause. Additionally, our model works recursively to support nested queries. When evaluated on the Spider dataset, our approach achieves 4.6\\% and 9.8\\% accuracy gain in the test and dev sets, respectively. In addition, we show that our model is significantly more effective at predicting complex and nested queries than previous work.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 22, 
    "authors": [
      "Dongjun Lee"
    ], 
    "topics": [
      "SQL", 
      "Recursion (computer science)", 
      "Database schema", 
      "Deep learning", 
      "Binary decoder", 
      "Encoder"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-6201", 
    "title": "Identifying and Categorizing Disaster-Related Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a system for classifying disaster-related tweets. The focus is on Twitter data generated before, during, and after Hurricane Sandy, which impacted New York in the fall of 2012. We propose an annotation schema for identifying relevant tweets as well as the more fine-grained categories they represent, and develop feature-rich classifiers for relevance and fine-grained categorization.", 
    "year": 2016, 
    "venue": "SocialNLP@EMNLP", 
    "references": 28, 
    "authors": [
      "Kevin Stowe", 
      "Michael J. Paul", 
      "Martha Palmer", 
      "L. Palen", 
      "K. Anderson"
    ], 
    "topics": [
      "Categorization", 
      "Batch processing", 
      "Relevance", 
      "Software feature", 
      "Sparse matrix", 
      "Precision and recall", 
      "Real-time transcription", 
      "ACM Computing Classification System"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.422", 
    "title": "Automatic Term Name Generation for Gene Ontology: Task and Dataset", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that reflect the gene function are still named by the experts. To fill this gap, we propose a novel task, namely term name generation for GO, and build a large-scale benchmark dataset. Furthermore, we present a graph-based generative model that incorporates the relations between genes, words and terms for term name generation, which exhibits great advantages over the strong baselines.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 25, 
    "authors": [
      "Yanjian Zhang", 
      "Qin Chen", 
      "Yiteng Zhang", 
      "Zhongyu Wei", 
      "Yixu Gao", 
      "Jiajie Peng", 
      "Zengfeng Huang", 
      "Wei-Jian Sun", 
      "Xuanjing Huang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974499.974525", 
    "title": "A Corpus-Based Statistical Approach to Automatic Book Indexing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The paper reports on a new approach to automatic generation of back-of-book indexes for Chinese books. Parsing on the level of complete sentential analysis is avoided because of the inefficiency and unavailability of a Chinese Grammar with enough coverage. Instead, fundamental analysis particular to Chinese text called word segmentation is performed to break up characters into a sequence of lexical units equivalent to words in English. The sequence of words then goes through part-of-speech tagging and noun phrase analysis. All these analyses are done using a corpus-based statistical algorithm. Experimental results have shown satisfactory results.", 
    "year": 1992, 
    "venue": "ANLP", 
    "references": 24, 
    "authors": [
      "J. Chang", 
      "Tsung-Yih Tseng", 
      "Sur-Jin Ker", 
      "Ying Cheng", 
      "Huey-Chyun Chen", 
      "Shun-Der Cheng", 
      "John S. Liu"
    ], 
    "topics": [
      "Text corpus", 
      "Parsing", 
      "Text segmentation", 
      "Algorithm", 
      "Unavailability", 
      "Book"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1136", 
    "title": "Identifying Semantic Divergences in Parallel Text without Annotations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recognizing that even correct translations are not always semantically equivalent, we automatically detect meaning divergences in parallel sentence pairs with a deep neural model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation. We show that our semantic model detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 62, 
    "authors": [
      "Yogarshi Vyas", 
      "Xing Niu", 
      "Marine Carpuat"
    ], 
    "topics": [
      "Parallel text", 
      "Neural machine translation", 
      "Java annotation", 
      "Semantic similarity", 
      "Crowdsourcing", 
      "Text corpus", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5227", 
    "title": "Sentiment Aware Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentiment ambiguous lexicons refer to words where their polarity depends strongly on con- text. As such, when the context is absent, their translations or their embedded sentence ends up (incorrectly) being dependent on the training data. While neural machine translation (NMT) has achieved great progress in recent years, most systems aim to produce one single correct translation for a given source sentence. We investigate the translation variation in two sentiment scenarios. We perform experiments to study the preservation of sentiment during translation with three different methods that we propose. We conducted tests with both sentiment and non-sentiment bearing contexts to examine the effectiveness of our methods. We show that NMT can generate both positive- and negative-valent translations of a source sentence, based on a given input sentiment label. Empirical evaluations show that our valence-sensitive embedding (VSE) method significantly outperforms a sequence-to-sequence (seq2seq) baseline, both in terms of BLEU score and ambiguous word translation accuracy in test, given non-sentiment bearing contexts.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 28, 
    "authors": [
      "Chenglei Si", 
      "Kui Wu", 
      "Ai Ti Aw", 
      "Min-Yen Kan"
    ], 
    "topics": [
      "Neural machine translation", 
      "Transformer", 
      "BLEU", 
      "Baseline (configuration management)", 
      "Experiment", 
      "Lexicon", 
      "Embedded system", 
      "Ambiguous grammar"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.1162/coli.2008.34.2.311", 
    "title": "Book review: A computational model of natural language communication / Roland Hausser. - Springer, 2006, ISBN 3-540-35476-X/978-3-540-35476-5", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2008, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Markus Egg"
    ], 
    "topics": [
      "International Standard Book Number", 
      "Computational model", 
      "Natural language", 
      "Springer (tank)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075096.1075106", 
    "title": "Reliable Measures for Aligning Japanese-English News Articles and Sentences", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 7, 
    "authors": [
      "M. Utiyama", 
      "H. Isahara"
    ], 
    "topics": [
      "Align (company)", 
      "Cross-language information retrieval", 
      "Dynamic programming", 
      "Parallel text"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-demos.14", 
    "title": "ENTYFI: A System for Fine-grained Entity Typing in Fictional Texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-quality type systems for popular fictional domains, and provides recommendations towards reference type systems for given input texts. Users can exploit the richness and diversity of these reference type systems for fine-grained supervised typing, in addition, they can choose among and combine four other typing modules: pre-trained real-world models, unsupervised dependency-based typing, knowledge base lookups, and constraint-based candidate consolidation. The demonstrator is available at: https://d5demos.mpi-inf.mpg.de/entyfi.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "C. Chu", 
      "Simon Razniewski", 
      "G. Weikum"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.348", 
    "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Detecting what emotions are expressed in text is a well-studied problem in natural language processing. However, research on finer grained emotion analysis such as what causes an emotion is still in its infancy. We present solutions that tackle both emotion recognition and emotion cause detection in a joint fashion. Considering that common-sense knowledge plays an important role in understanding implicitly expressed emotions and the reasons for those emotions, we propose novel methods that combine common-sense knowledge via adapted knowledge models with multi-task learning to perform joint emotion classification and emotion cause tagging. We show performance improvement on both tasks when including common-sense reasoning and a multitask framework. We provide a thorough analysis to gain insights into model performance.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 39, 
    "authors": [
      "Elsbeth Turcan", 
      "Shuai Wang", 
      "Rishita Anubhai", 
      "Kasturi Bhattacharjee", 
      "Y. Al-Onaizan", 
      "S. Muresan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2114", 
    "title": "INGEOTEC at SemEval-2019 Task 5 and Task 6: A Genetic Programming Approach for Text Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our participation in HatEval and OffensEval challenges for English and Spanish languages. We used several approaches, B4MSA, FastText, and EvoMSA. Best results were achieved with EvoMSA, which is a multilingual and domain-independent architecture that combines the prediction of different knowledge sources to solve text classification problems.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 15, 
    "authors": [
      "Mario Graff", 
      "Sabino Miranda-Jim\u00e9nez", 
      "Eric Sadit Tellez", 
      "D. A. Ochoa"
    ], 
    "topics": [
      "SemEval", 
      "Genetic programming", 
      "Document classification", 
      "Competitive analysis (online algorithm)", 
      "Sentiment analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5812", 
    "title": "Phonetic Vector Representations for Sound Sequence Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This study explores a number of data-driven vector representations of the IPA-encoded sound segments for the purpose of sound sequence alignment. We test the alternative representations based on the alignment accuracy in the context of computational historical linguistics. We show that the data-driven methods consistently do better than linguistically-motivated articulatoryacoustic features. The similarity scores obtained using the data-driven representations in a monolingual context, however, performs worse than the state-of-the-art distance (or similarity) scoring methods proposed in earlier studies of computational historical linguistics. We also show that adapting representations to the task at hand improves the results, yielding alignment accuracy comparable to the state of the art methods.", 
    "year": 2018, 
    "venue": "", 
    "references": 33, 
    "authors": [
      "Pavel Sofroniev", 
      "\u00c7agri \u00c7\u00f6ltekin"
    ], 
    "topics": [
      "Computational phylogenetics", 
      "One-hot", 
      "Linguistics", 
      "Computation", 
      "Performance", 
      "Experiment", 
      "Baseline (configuration management)", 
      "Large", 
      "Structure of superior cerebellar artery", 
      "Sequence Alignment", 
      "IPA Liver Cancer Regimen", 
      "Inference", 
      "Score"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00112", 
    "title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500\u00d7, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying).", 
    "year": 2016, 
    "venue": "TACL", 
    "references": 39, 
    "authors": [
      "Ehsan Shareghi", 
      "M. Petri", 
      "Gholamreza Haffari", 
      "Trevor Cohn"
    ], 
    "topics": [
      "Language model", 
      "Text corpus", 
      "Suffix tree", 
      "Perplexity", 
      "Runtime system", 
      "Machine translation", 
      "Moses", 
      "Memory footprint", 
      "The Australian", 
      "Requirement", 
      "Vocabulary", 
      "On the fly", 
      "Smoothing", 
      "Teh", 
      "Markov chain", 
      "Image scaling"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.368", 
    "title": "Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BeamDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BeamDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/ henryzhao5852/BeamDR.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 32, 
    "authors": [
      "Chen Zhao", 
      "Chenyan Xiong", 
      "Jordan L. Boyd-Graber", 
      "Hal Daum'e"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-2004", 
    "title": "Relation extraction pattern ranking using word similarity", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our thesis proposal aims at integrating word similarity measures in pattern ranking for relation extraction bootstrapping algorithms. We note that although many contributions have been done on pattern ranking schemas, few explored the use of word-level semantic similarity. Our hypothesis is that word similarity would allow better pattern comparison and better pattern ranking, resulting in less semantic drift commonly problematic in bootstrapping algorithms. In this paper, as a first step into this research, we explore different pattern representations, various existing pattern ranking approaches and some word similarity measures. We also present a methodology and evaluation approach to test our hypothesis.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 43, 
    "authors": [
      "Konstantinos Lambrou-Latreille"
    ], 
    "topics": [
      "Relationship extraction", 
      "Semantic similarity", 
      "Lexicon", 
      "Algorithm", 
      "Baseline (configuration management)", 
      "Never-Ending Language Learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.ngt-1.18", 
    "title": "Expand and Filter: CUNI and LMU Systems for the WNGT 2020 Duolingo Shared Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present our submission to the Simultaneous Translation And Paraphrase for Language Education (STAPLE) challenge. We used a standard Transformer model for translation, with a crosslingual classifier predicting correct translations on the output n-best list. To increase the diversity of the outputs, we used additional data to train the translation model, and we trained a paraphrasing model based on the Levenshtein Transformer architecture to generate further synonymous translations. The paraphrasing results were again filtered using our classifier. While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality. Our model without the paraphrasing component finished in the middle of the field for the shared task, improving over the best baseline by a margin of 10-22 % weighted F1 absolute.", 
    "year": 2020, 
    "venue": "NGT", 
    "references": 22, 
    "authors": [
      "Jind\u0159ich Libovick\u00fd", 
      "Zden\u011bk Kasner", 
      "Jind\u0159ich Helcl", 
      "Ondrej Dusek"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1220175.1220284", 
    "title": "An All-Subtrees Approach to Unsupervised Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We investigate generalizations of the all-subtrees \"DOP\" approach to unsupervised parsing. Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. We report state-of-the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG).", 
    "year": 2006, 
    "venue": "ACL", 
    "references": 33, 
    "authors": [
      "R. Bod"
    ], 
    "topics": [
      "Parsing", 
      "Tree (data structure)", 
      "Unsupervised learning", 
      "Binary tree", 
      "The Wall Street Journal", 
      "Treebank", 
      "Parse tree", 
      "Stochastic context-free grammar", 
      "Coding tree unit"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K18-1018", 
    "title": "Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Aspect-level sentiment analysis aims to identify the sentiment of a specific target in its context. Previous works have proved that the interactions between aspects and the contexts are important. On this basis, we also propose a succinct hierarchical attention based mechanism to fuse the information of targets and the contextual words. In addition, most existing methods ignore the position information of the aspect when encoding the sentence. In this paper, we argue that the position-aware representations are beneficial to this task. Therefore, we propose a hierarchical attention based position-aware network (HAPN), which introduces position embeddings to learn the position-aware representations of sentences and further generate the target-specific representations of contextual words. The experimental results on SemEval 2014 dataset show that our approach outperforms the state-of-the-art methods.", 
    "year": 2018, 
    "venue": "CoNLL", 
    "references": 22, 
    "authors": [
      "Lishuang Li", 
      "Yang Liu", 
      "Anqiao Zhou"
    ], 
    "topics": [
      "Sentiment analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-4013", 
    "title": "NCRF++: An Open-source Neural Sequence Labeling Toolkit", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization. Built on PyTorch http://pytorch.org/, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "Jie Yang", 
      "Yue Zhang"
    ], 
    "topics": [
      "Sequence labeling", 
      "Conditional random field", 
      "Open-source software", 
      "Experiment", 
      "Refinement (computing)", 
      "Graphics processing unit"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1452", 
    "title": "Contrastive Language Adaptation for Cross-Lingual Stance Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective (or stance) of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 49, 
    "authors": [
      "Mitra Mohtarami", 
      "James R. Glass", 
      "Preslav Nakov"
    ], 
    "topics": [
      "Compiler", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.367", 
    "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our spanlevel approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions. 1", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 51, 
    "authors": [
      "Lu Xu", 
      "Lu Xu", 
      "Yew Ken Chia", 
      "Lidong Bing"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1439", 
    "title": "WikiCREM: A Large Unsupervised Corpus for Coreference Resolution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce WikiCREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our WikiCREM dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-the-art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WinoBias, and WinoGender. We release our model to be used off-the-shelf for solving pronoun disambiguation.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 30, 
    "authors": [
      "Vid Kocijan", 
      "Oana-Maria Camburu", 
      "Ana-Maria Cretu", 
      "Yordan Yordanov", 
      "P. Blunsom", 
      "Thomas Lukasiewicz"
    ], 
    "topics": [
      "Natural language understanding", 
      "Anaphora (linguistics)", 
      "Word-sense disambiguation", 
      "Programmed Data Processor", 
      "Wikipedia", 
      "Memory disambiguation", 
      "Language model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3020", 
    "title": "Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a hierarchical model for sequential data that learns a tree on-the-fly, i.e. while reading the sequence. In the model, a recurrent network adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections that ease the learning of long-term dependencies. The tree structure can either be inferred without supervision through reinforcement learning, or learned in a supervised manner. We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is created to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our model in a well-known propositional logic and language modelling tasks. Experimental results have shown the potential of our approach.", 
    "year": 2018, 
    "venue": "Rep4NLP@ACL", 
    "references": 22, 
    "authors": [
      "Athul Paul Jacob", 
      "Zhouhan Lin", 
      "Alessandro Sordoni", 
      "Yoshua Bengio"
    ], 
    "topics": [
      "Tree structure", 
      "Reinforcement learning", 
      "Propositional calculus", 
      "Recursion (computer science)", 
      "Language model", 
      "Recurrent neural network", 
      "Hierarchical database model", 
      "Experiment", 
      "Lazy evaluation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-short.29", 
    "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical problems of the previous benchmark dataset (i.e., human annotations) on image captioning metric, and introduce a new collection of human annotations on the generated captions. We validate UMIC on four datasets, including our new dataset, and show that UMIC has a higher correlation than all previous metrics that require multiple references. We release the benchmark dataset and pre-trained models to compute the UMIC1.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 34, 
    "authors": [
      "Hwanhee Lee", 
      "Seunghyun Yoon", 
      "Franck Dernoncourt", 
      "Trung Bui", 
      "Kyomin Jung"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli_a_00367", 
    "title": "On the Linguistic Representational Power of Neural Machine Translation Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.", 
    "year": 2020, 
    "venue": "CL", 
    "references": 203, 
    "authors": [
      "Yonatan Belinkov", 
      "Nadir Durrani", 
      "Fahim Dalvi", 
      "Hassan Sajjad", 
      "James R. Glass"
    ], 
    "topics": [
      "Neural machine translation", 
      "Natural language processing", 
      "Artificial intelligence", 
      "Substring", 
      "Deep learning", 
      "End-to-end principle", 
      "Encoder", 
      "Operational semantics", 
      "Artificial neural network", 
      "Galaxy morphological classification", 
      "Semantic data model", 
      "Translation unit (programming)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/N19-1028", 
    "title": "FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 18, 
    "authors": [
      "Kelvin Jiang", 
      "Dekun Wu", 
      "Hui Jiang"
    ], 
    "topics": [
      "Freebase", 
      "Question answering", 
      "Natural language understanding", 
      "Download", 
      "Machine learning", 
      "Entity", 
      "Baseline (configuration management)", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-0707", 
    "title": "Identifying Literary Texts with Bigrams", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study perceptions of literariness in a set of contemporary Dutch novels. Experiments with machine learning models show that it is possible to automatically distinguish novels that are seen as highly literary from those that are seen as less literary, using surprisingly simple textual features. The most discriminating features of our classification model indicate that genre might be a confounding factor, but a regression model shows that we can also explain variation between highly literary novels from less literary ones within genre.", 
    "year": 2015, 
    "venue": "CLfL@NAACL-HLT", 
    "references": 17, 
    "authors": [
      "Andreas van Cranenburgh", 
      "Corina Koolen"
    ], 
    "topics": [
      "Bigram", 
      "Machine learning", 
      "Experiment", 
      "Mobile phone", 
      "Principle of good enough"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-2103", 
    "title": "Tweet Classification without the Tweet: An Empirical Examination of User versus Document Attributes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "NLP naturally puts a primary focus on leveraging document language, occasionally considering user attributes as supplemental. However, as we tackle more social scientific tasks, it is possible user attributes might be of primary importance and the document supplemental. Here, we systematically investigate the predictive power of user-level features alone versus document-level features for document-level tasks. We first show user attributes can sometimes carry more task-related information than the document itself. For example, a tweet-level stance detection model using only 13 user-level attributes (i.e. features that did not depend on the specific tweet) was able to obtain a higher F1 than the top-performing SemEval participant. We then consider multiple tasks and a wider range of user attributes, showing the performance of strong document-only models can often be improved (as in stance, sentiment, and sarcasm) with user attributes, particularly benefiting tasks with stable \u201ctrait-like\u201d outcomes (e.g. stance) most relative to frequently changing \u201cstate-like\u201d outcomes (e.g. sentiment). These results not only support the growing work on integrating user factors into predictive systems, but that some of our NLP tasks might be better cast primarily as user-level (or human) tasks.", 
    "year": 2019, 
    "venue": "", 
    "references": 44, 
    "authors": [
      "Veronica E. Lynn", 
      "Salvatore Giorgi", 
      "Niranjan Balasubramanian", 
      "H. A. Schwartz"
    ], 
    "topics": [
      "Social media", 
      "SemEval", 
      "Natural language processing", 
      "Sentiment analysis", 
      "User space", 
      "Operating system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1906", 
    "title": "An incremental model of syntactic bootstrapping", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Syntactic bootstrapping is the hypothesis that learners can use the preliminary syntactic structure of a sentence to identify and characterise the meanings of novel verbs. Previous work has shown that syntactic bootstrapping can begin using only a few seed nouns (Connor et al., 2010; Connor et al., 2012). Here, we relax their key assumption: rather than training the model over the entire corpus at once (batch mode), we train the model incrementally, thus more realistically simulating a human learner. We also improve on the verb prediction method by incorporating the assumption that verb assignments are stable over time. We show that, given a high enough number of seed nouns (around 30), an incremental model achieves similar performance to the batch model. We also find that the number of seed nouns shown to be sufficient in the previous work is not sufficient under the more realistic incremental model. The results demonstrate that adopting more realistic assumptions about the early stages of language acquisition can provide new insights without undermining performance.", 
    "year": 2016, 
    "venue": "", 
    "references": 19, 
    "authors": [
      "Christos Christodoulopoulos", 
      "D. Roth", 
      "C. Fisher"
    ], 
    "topics": [
      "Batch processing", 
      "Simulation", 
      "Hidden Markov model", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-056-4_076", 
    "title": "The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Relation Extraction (RE) consists in detecting and classifying semantic relations between entities in a sentence. The vast majority of the state-of-the-art RE systems relies on morphosyntactic features and supervised machine learning algorithms. This paper tries to answer important questions concerning both the impact of semantic based features, and the integration of external linguistic knowledge resources on RE performance. For that, a RE system based on a logical and relational learning algorithm was used and evaluated on three reference datasets from two distinct domains. The yielded results confirm that the classifiers induced using the proposed richer feature set outperformed the classifiers built with morphosyntactic features in average 4% (F1-measure).", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 21, 
    "authors": [
      "Rinaldo Lima", 
      "B. Espinasse", 
      "F. Freitas"
    ], 
    "topics": [
      "Relationship extraction", 
      "Machine learning", 
      "Scalability", 
      "Algorithm", 
      "Supervised learning", 
      "Inductive reasoning", 
      "Sensor", 
      "Entity", 
      "Deep linguistic processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1392", 
    "title": "Broad-Coverage Semantic Parsing as Transduction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks \u2013 AMR, SDP and UCCA \u2013 demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 67, 
    "authors": [
      "Sheng Zhang", 
      "Xutai Ma", 
      "Kevin Duh", 
      "Benjamin Van Durme"
    ], 
    "topics": [
      "Parsing", 
      "Transduction (machine learning)", 
      "Transducer", 
      "Adaptive Multi-Rate audio codec", 
      "Semantic analysis (machine learning)", 
      "Sockets Direct Protocol", 
      "Programming paradigm", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1302", 
    "title": "Estimating Time to Event from Tweets Using Temporal Expressions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Given a stream of Twitter messages about an event, we investigate the predictive power of temporal expressions in the messages to estimate the time to event (TTE). From labeled training data we learn average TTE estimates of temporal expressions and combinations thereof, and define basic rules to compute the time to event from temporal expressions, so that when they occur in a tweet that mentions an event we can generate a prediction. We show in a case study on soccer matches that our estimations are off by about eight hours on average in terms of mean absolute error.", 
    "year": 2014, 
    "venue": "", 
    "references": 17, 
    "authors": [
      "Ali H\u00fcrriyeto\u01e7lu", 
      "N. Oostdijk", 
      "Antal van den Bosch"
    ], 
    "topics": [
      "Temporal expressions", 
      "Mean squared error", 
      "Approximation error"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.30", 
    "title": "Developing a Curated Topic Model for COVID-19 Medical Research Literature", 
    "fields_of_study": [
      "Computer Science", 
      "Sociology"
    ], 
    "abstract": "Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 22, 
    "authors": [
      "P. Resnik", 
      "K. Goodman", 
      "Mike Moran"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-2095", 
    "title": "Metrics for Evaluation of Word-level Machine Translation Quality Estimation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The aim of this paper is to investigate suitable evaluation strategies for the task of word-level quality estimation of machine translation. We suggest various metrics to replace F1-score for the \u201cBAD\u201d class, which is currently used as main metric. We compare the metrics\u2019 performance on real system outputs and synthetically generated datasets and suggest a reliable alternative to the F1-BAD score \u2014 the multiplication of F1-scores for different classes. Other metrics have lower discriminative power and are biased by unfair labellings.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 14, 
    "authors": [
      "V. Logacheva", 
      "M. Lukasik", 
      "Lucia Specia"
    ], 
    "topics": [
      "Machine translation", 
      "Computer-assisted translation", 
      "Graph labeling", 
      "F1 score", 
      "Downstream (software development)", 
      "Synthetic intelligence", 
      "Query expansion"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.415", 
    "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Mistranslated numbers have the potential to cause serious effects, such as financial loss or medical misinformation. In this work we develop comprehensive assessments of the robustness of neural machine translation systems to numerical text via behavioural testing. We explore a variety of numerical translation capabilities a system is expected to exhibit and design effective test examples to expose system underperformance. We find that numerical mistranslation is a general issue: major commercial systems and state-of-the-art research models fail on many of our test examples, for highand low-resource languages. Our tests reveal novel errors that have not previously been reported in NMT systems, to the best of our knowledge. Lastly, we discuss strategies to mitigate numerical mistranslation.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 20, 
    "authors": [
      "Jun Wang", 
      "Chang Xu", 
      "Francisco Guzm\u00e1n", 
      "Ahmed El-Kishky", 
      "Benjamin I. P. Rubinstein", 
      "Trevor Cohn"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-short.11", 
    "title": "How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 0, 
    "authors": [
      "Sayan Ghosh", 
      "Zheng Qi", 
      "Snigdha Chaturvedi", 
      "Shashank Srivastava"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1191", 
    "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase(PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 27, 
    "authors": [
      "Pradeep Dasigi", 
      "Waleed Ammar", 
      "Chris Dyer", 
      "E. Hovy"
    ], 
    "topics": [
      "WordNet", 
      "Attachments", 
      "Context-sensitive grammar", 
      "Keras", 
      "Freebase", 
      "Question answering", 
      "Recurrent neural network", 
      "Encoder", 
      "Synonym ring", 
      "Julia", 
      "Artificial intelligence", 
      "Natural language processing", 
      "HECToR", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-1702", 
    "title": "Modeling Acoustic-Prosodic Cues for Word Importance Prediction in Spoken Dialogues", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Prosodic cues in conversational speech aid listeners in discerning a message. We investigate whether acoustic cues in spoken dialogue can be used to identify the importance of individual words to the meaning of a conversation turn. Individuals who are Deaf and Hard of Hearing often rely on real-time captions in live meetings. Word error rate, a traditional metric for evaluating automatic speech recognition, fails to capture that some words are more important for a system to transcribe correctly than others. We present and evaluate neural architectures that use acoustic features for 3-class word importance prediction. Our model performs competitively against state-of-the-art text-based word-importance prediction models, and it demonstrates particular benefits when operating on imperfect ASR output.", 
    "year": 2019, 
    "venue": "ArXiv", 
    "references": 44, 
    "authors": [
      "Sushant Kafle", 
      "Cecilia Ovesdotter Alm", 
      "Matt Huenerfauth"
    ], 
    "topics": [
      "Speech recognition", 
      "Acoustic cryptanalysis", 
      "Text-based (computing)", 
      "Word error rate", 
      "Text-based user interface", 
      "Real-time transcription", 
      "Real-time clock", 
      "Automated system recovery"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00227", 
    "title": "The Unified and Holistic Method Gamma (\u03b3) for Inter-Annotator Agreement Measure and Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Agreement measures have been widely used in computational linguistics for more than 15 years to check the reliability of annotation processes. Although considerable effort has been made concerning categorization, fewer studies address unitizing, and when both paradigms are combined even fewer methods are available and discussed. The aim of this article is threefold. First, we advocate that to deal with unitizing, alignment and agreement measures should be considered as a unified process, because a relevant measure should rely on an alignment of the units from different annotators, and this alignment should be computed according to the principles of the measure. Second, we propose the new versatile measure \u03b3, which fulfills this requirement and copes with both paradigms, and we introduce its implementation. Third, we show that this new method performs as well as, or even better than, other more specialized methods devoted to categorization or segmentation, while combining the two paradigms at the same time.", 
    "year": 2015, 
    "venue": "Computational Linguistics", 
    "references": 63, 
    "authors": [
      "Yann Mathet", 
      "Antoine Widl\u00f6cher", 
      "Jean-Philippe M\u00e9tivier"
    ], 
    "topics": [
      "Categorization", 
      "Computational linguistics", 
      "Holism", 
      "Unified Process"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1040", 
    "title": "YNU-HPCC at SemEval-2018 Task 1: BiLSTM with Attention based Sentiment Analysis for Affect in Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We implemented the sentiment system in all five subtasks for English and Spanish. All subtasks involve emotion or sentiment intensity prediction (regression and ordinal classification) and emotions determining (multi-labels classification). The useful BiLSTM (Bidirectional Long-Short Term Memory) model with attention mechanism was mainly applied for our system. We use BiLSTM in order to get word information extracted from both directions. The attention mechanism was used to find the contribution of each word for improving the scores. Furthermore, based on BiLSTMATT (BiLSTM with attention mechanism) a few deep-learning algorithms were employed for different subtasks. For regression and ordinal classification tasks we used domain adaptation and ensemble learning methods to leverage base model. While a single base model was used for multi-labels task.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 19, 
    "authors": [
      "You Zhang", 
      "Jin Wang", 
      "Xuejie Zhang"
    ], 
    "topics": [
      "Sentiment analysis", 
      "HPCC", 
      "Ensemble learning", 
      "Level of measurement", 
      "Deep learning", 
      "Long short-term memory", 
      "Domain adaptation", 
      "Multi-label classification", 
      "Enterprise information management", 
      "Machine learning", 
      "HPC Challenge Benchmark", 
      "Algorithm", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1136", 
    "title": "Semantic Parsing Using Content and Context: A Case Study from Requirements Elicitation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic representation employs live sequence charts, a multi-modal visual language for scenariobased programming, which can be directly translated into executable code. The architecture we propose integrates sentencelevel and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context. We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 32, 
    "authors": [
      "Reut Tsarfaty", 
      "Ilia Pogrebezky", 
      "Guy Weiss", 
      "Y. Natan", 
      "Smadar Szekely", 
      "D. Harel"
    ], 
    "topics": [
      "Requirement", 
      "Requirements elicitation", 
      "Parsing", 
      "Entity", 
      "Visual editor", 
      "Visual language", 
      "Word-sense disambiguation", 
      "Executable", 
      "Chart", 
      "Modal logic", 
      "Natural language processing", 
      "Ink Serialized Format"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-short.78", 
    "title": "Cross-lingual Text Classification with Heterogeneous Graph Neural Network", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as nodes, and linking nodes with different relations, which include part-of-speech roles, semantic similarity, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 38, 
    "authors": [
      "ZiYun Wang", 
      "Xuan Liu", 
      "Pei-Yin Yang", 
      "Shixing Liu", 
      "Zhisheng Wang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1402", 
    "title": "ProSeqo: Projection Sequence Networks for On-Device Text Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multiple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text classification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text classification neural model. It achieved comparable results to previous neural approaches for news article, answers and product categorization, while preserving small memory footprint and maintaining high accuracy.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 29, 
    "authors": [
      "Zornitsa Kozareva", 
      "Sujith Ravi"
    ], 
    "topics": [
      "Document classification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2014", 
    "title": "Using prosodic annotations to improve coreference resolution of spoken text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper is the first to examine the effect of prosodic features on coreference resolution in spoken discourse. We test features from different prosodic levels and investigate which strategies can be applied. Our results on the basis of manual prosodic labelling show that the presence of an accent is a helpful feature in a machine-learning setting. Including prosodic boundaries and determining whether the accent is the nuclear accent further increases results.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Ina R\u00f6siger", 
      "Arndt Riester"
    ], 
    "topics": [
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.figlang-1.3", 
    "title": "A Report on the 2020 VUA and TOEFL Metaphor Detection Shared Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus. The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language.", 
    "year": 2020, 
    "venue": "FIGLANG", 
    "references": 67, 
    "authors": [
      "C. W. Leong", 
      "Beata Beigman Klebanov", 
      "Chris Hamill", 
      "Egon W. Stemle", 
      "Rutuja Ubale", 
      "X. Chen"
    ], 
    "topics": [
      "Language identification", 
      "Native-language identification"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.326", 
    "title": "Modeling the Unigram Distribution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The unigram distribution is the non-contextual probability of finding a specific word form in a corpus. While of central importance to the study of language, it is commonly approximated by each word\u2019s sample frequency in the corpus. This approach, being highly dependent on sample size, assigns zero probability to any out-of-vocabulary (oov) word form. As a result, it produces negatively biased probabilities for any oov word form, while positively biased probabilities to in-corpus words. In this work, we argue in favor of properly modeling the unigram distribution\u2014claiming it should be a central task in natural language processing. With this in mind, we present a novel model for estimating it in a language (a neuralization of Goldwater et al.\u2019s (2011) model) and show it produces much better estimates across a diverse set of 7 languages than the na\u0131\u0308ve use of neural character-level language models.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 40, 
    "authors": [
      "Irene Nikkarinen", 
      "Tiago Pimentel", 
      "D. Blasi", 
      "Ryan Cotterell"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4324", 
    "title": "Assessing Incrementality in Sequence-to-Sequence Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.", 
    "year": 2019, 
    "venue": "RepL4NLP@ACL", 
    "references": 25, 
    "authors": [
      "Dennis Ulmer", 
      "Dieuwke Hupkes", 
      "Elia Bruni"
    ], 
    "topics": [
      "Computational linguistics", 
      "sentence", 
      "Plausibility structure", 
      "Information", 
      "Computation", 
      "Decoder Device Component", 
      "Encoder Device Component"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-3626", 
    "title": "Rapid Prototyping of Form-driven Dialogue Systems Using an Open-source Framework", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most human-machine communication for information access through speech, text and graphical interfaces are mediated by forms \u2013 i.e. lists of named fields. However, deploying form-filling dialogue systems still remains a challenging task due to the effort and skill required to author such systems. We describe an extension to the OpenDial framework that enables the rapid creation of functional dialogue systems by non-experts. The dialogue designer specifies the slots and their types as input and the tool generates a domain specification that drives a slot-filling dialogue system. The presented approach provides several benefits compared to traditional techniques based on flowcharts, such as the use of probabilistic reasoning and flexible grounding strategies.", 
    "year": 2016, 
    "venue": "SIGDIAL Conference", 
    "references": 9, 
    "authors": [
      "Svetlana Stoyanchev", 
      "Pierre Lison", 
      "S. Bangalore"
    ], 
    "topics": [
      "Dialog system", 
      "Rapid prototyping", 
      "Flowchart", 
      "XML", 
      "Information access", 
      "Cloud computing", 
      "Web application", 
      "Graphical user interface", 
      "Natural language understanding", 
      "Bridging (networking)", 
      "Dialog tree"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118108.1118116", 
    "title": "Using GATE as an Environment for Teaching NLP", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we argue that the GATE architecture and visual development environment can be used as an effective tool for teaching language engineering and computational linguistics. Since GATE comes with a customisable and extendable set of components, it allows students to get hands-on experience with building NLP applications. GATE also has tools for corpus annotation and performance evaluation, so students can go through the entire application development process within its graphical development environment. Finally, it offers comprehensive Unicode-compliant multilingual support, thus allowing students to create components for languages other than English. Unlike other NLP teaching tools which were designed specifically and only for this purpose, GATE is a system developed for and used actively in language engineering research. This unique duality allows students to contribute to research projects and gain skills in embedding HLT in practical applications.", 
    "year": 2002, 
    "venue": "ACL 2002", 
    "references": 17, 
    "authors": [
      "Kalina Bontcheva", 
      "H. Cunningham", 
      "V. Tablan", 
      "D. Maynard", 
      "O. Hamza"
    ], 
    "topics": [
      "GATE", 
      "NINL gene", 
      "Natural language processing", 
      "Unicode", 
      "Computational linguistics", 
      "Hands-on computing", 
      "Graphical user interface", 
      "Performance Evaluation", 
      "Documentation", 
      "Extensibility", 
      "Programming Languages", 
      "Meddra High Level Term", 
      "Compliance behavior", 
      "Multilingualism", 
      "Body of uterus", 
      "research grants", 
      "Universities", 
      "Embedding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-3010", 
    "title": "ELCO3: Entity Linking with Corpus Coherence Combining Open Source Annotators", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Entity Linking (EL) systems' performance is uneven across corpora or depending on entity types. To help overcome this issue, we propose an EL workflow that combines the outputs of several open source EL systems, and selects annotations via weighted voting. The results are displayed on a UI that allows the users to navigate the corpus and to evaluate annotation quality based on several metrics.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 14, 
    "authors": [
      "Pablo Ruiz Fabo", 
      "T. Poibeau", 
      "F. M\u00e9lanie"
    ], 
    "topics": [
      "Entity linking", 
      "Text corpus", 
      "User interface", 
      "Open-source software"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1145", 
    "title": "Predicting Dialect Variation in Immigrant Contexts Using Light Verb Constructions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Languages spoken by immigrants change due to contact with the local languages. Capturing these changes is problematic for current language technologies, which are typically developed for speakers of the standard dialect only. Even when dialectal variants are available for such technologies, we still need to predict which dialect is being used. In this study, we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions. We experiment with a number of grammatical and contextual features, achieving over 84% accuracy (56% baseline).", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 28, 
    "authors": [
      "A. S. Dogru\u00f6z", 
      "Preslav Nakov"
    ], 
    "topics": [
      "Language technology", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.378", 
    "title": "Max-Margin Incremental CCG Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 65, 
    "authors": [
      "Milo\u0161 Stanojevi\u0107", 
      "Mark Steedman"
    ], 
    "topics": [
      "Parsing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.55", 
    "title": "Self-Adapter at SemEval-2021 Task 10: Entropy-based Pseudo-Labeler for Source-free Domain Adaptation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the model trained on the source domain data is given. We propose two methods: Self-Adapter and Selective Classifier Training. Self-Adapter is a training method that uses sentence-level pseudo-labels filtered by the self-entropy threshold to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed methods are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 15, 
    "authors": [
      "Sangwon Yoon", 
      "Yanghoon Kim", 
      "Kyomin Jung"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.1162/coli_a_00008", 
    "title": "Does GIZA++ Make Search Errors?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word alignment is a critical procedure within statistical machine translation (SMT). Brown et al. (1993) have provided the most popular word alignment algorithm to date, one that has been implemented in the GIZA (Al-Onaizan et al., 1999) and GIZA++ (Och and Ney 2003) software and adopted by nearly every SMT project. In this article, we investigate whether this algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns alignments that are sub-optimal according to a trained model.", 
    "year": 2010, 
    "venue": "Computational Linguistics", 
    "references": 6, 
    "authors": [
      "Sujith Ravi", 
      "Kevin Knight"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Bitext word alignment", 
      "Viterbi algorithm", 
      "Symbian", 
      "Data structure alignment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073012.1073017", 
    "title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.", 
    "year": 2001, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "Michele Banko", 
      "E. Brill"
    ], 
    "topics": [
      "Text corpus", 
      "Word-sense disambiguation", 
      "Unsupervised learning", 
      "Natural language processing", 
      "Online and offline", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.511", 
    "title": "Semi-supervised Multi-task Learning for Multi-label Fine-grained Sexism Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sexism, a form of oppression based on one\u2019s sex, manifests itself in numerous ways and causes enormous suffering. In view of the growing number of experiences of sexism reported online, categorizing these recollections automatically can assist the fight against sexism, as it can facilitate effective analyses by gender studies researchers and government officials involved in policy making. In this paper, we investigate the fine-grained, multi-label classification of accounts (reports) of sexism. To the best of our knowledge, we work with considerably more categories of sexism than any published work through our 23-class problem formulation. Moreover, we propose a multi-task approach for fine-grained multi-label sexism classification that leverages several supporting tasks without incurring any manual labeling cost. Unlabeled accounts of sexism are utilized through unsupervised learning to help construct our multi-task setup. We also devise objective functions that exploit label correlations in the training data explicitly. Multiple proposed methods outperform the state-of-the-art for multi-label sexism classification on a recently released dataset across five standard metrics.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 36, 
    "authors": [
      "Harika Abburi", 
      "Pulkit Parikh", 
      "Niyati Chhaya", 
      "Vasudeva Varma"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3021", 
    "title": "Morphological Segmentation and OPUS for Finnish-English Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes baseline systems for Finnish-English and English-Finnish machine translation using standard phrasebased and factored models including morphological features. We experiment with compound splitting and morphological segmentation and study the effect of adding noisy out-of-domain data to the parallel and the monolingual training data. Our results stress the importance of training data and demonstrate the effectiveness of morphological pre-processing of Finnish.", 
    "year": 2015, 
    "venue": "WMT@EMNLP", 
    "references": 19, 
    "authors": [
      "J. Tiedemann", 
      "Filip Ginter", 
      "Jenna Kanerva"
    ], 
    "topics": [
      "libopus", 
      "Statistical machine translation", 
      "Preprocessor", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4212", 
    "title": "Semantic Storytelling, Cross-lingual Event Detection and other Semantic Services for a Newsroom Content Curation Dashboard", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a prototypical content curation dashboard, to be used in the newsroom, and several of its underlying semantic content analysis components (such as named entity recognition, entity linking, summarisation and temporal expression analysis). The idea is to enable journalists (a) to process incoming content (agency reports, twitter feeds, reports, blog posts, social media etc.) and (b) to create new articles more easily and more efficiently. The prototype system also allows the automatic annotation of events in incoming content for the purpose of supporting journalists in identifying important, relevant or meaningful events and also to adapt the content currently in production accordingly in a semi-automatic way. One of our long-term goals is to support journalists building up entire storylines with automatic means. In the present prototype they are generated in a backend service using clustering methods that operate on the extracted events.", 
    "year": 2017, 
    "venue": "NLPmJ@EMNLP", 
    "references": 22, 
    "authors": [
      "J. Schneider", 
      "Ankit Srivastava", 
      "Peter Bourgonje", 
      "David Wabnitz", 
      "Georg Rehm"
    ], 
    "topics": [
      "Digital curation", 
      "Dashboard", 
      "Prototype", 
      "Named-entity recognition", 
      "Entity linking", 
      "Cluster analysis", 
      "Linked data", 
      "Semantic similarity", 
      "Social media", 
      "Blog", 
      "Semiconductor industry", 
      "Emoticon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2505", 
    "title": "The risk of sub-optimal use of Open Source NLP Software: UKB is inadvertently state-of-the-art in knowledge-based WSD", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "UKB is an open source collection of programs for performing, among other tasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.", 
    "year": 2018, 
    "venue": "ArXiv", 
    "references": 27, 
    "authors": [
      "Eneko Agirre", 
      "O. L. D. Lacalle", 
      "Aitor Soroa Etxabe"
    ], 
    "topics": [
      "Open-source software", 
      "Word-sense disambiguation", 
      "Natural language processing", 
      "Web Services for Devices", 
      "Word sense", 
      "Software release life cycle", 
      "Preprocessor", 
      "Status message (instant messaging)", 
      "End-to-end principle", 
      "Download", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_059", 
    "title": "A Statistical Machine Translation Model with Forest-to-Tree Algorithm for Semantic Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose a novel supervised model for parsing natural language sentences into their formal semantic representations. This model treats sentenceto-\u03bb-logical expression conversion within the framework of the statistical machine translation with forest-to-tree algorithm. To make this work, we transform the \u03bblogical expression structure into a form suitable for the mechanics of statistical machine translation and useful for modeling. We show that our model is able to yield new state-of-the-art results on both standard datasets with simple features.", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 20, 
    "authors": [
      "Z. Liao", 
      "Yan Xie"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Algorithm", 
      "Parsing", 
      "List of algorithms", 
      "Automatic Transmitter Identification System (television)", 
      "Simple Features", 
      "Natural language", 
      "Performance", 
      "Experiment", 
      "Benchmark (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1136", 
    "title": "Semantic Frame Identification with Distributed Word Representations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "K. Hermann", 
      "Dipanjan Das", 
      "J. Weston", 
      "K. Ganchev"
    ], 
    "topics": [
      "Frame language", 
      "PropBank", 
      "Semantic role labeling", 
      "FrameNet", 
      "Word embedding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4724", 
    "title": "The JHU Machine Translation Systems for WMT 2017", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We report on the efforts of the Johns Hopkins University to develop neural machine translation systems for the shared task for news translation organized around the Conference for Machine Translation (WMT) 2018. We developed systems for German\u2013English, English\u2013 German, and Russian\u2013English. Our novel contributions are iterative back-translation and fine-tuning on test sets from prior years.", 
    "year": 2018, 
    "venue": "WMT", 
    "references": 33, 
    "authors": [
      "Philipp Koehn", 
      "Kevin Duh", 
      "Brian Thompson"
    ], 
    "topics": [
      "Empirical Methods in Natural Language Processing", 
      "Neural machine translation", 
      "Iterative method", 
      "Regulatory Submission", 
      "Genetic Translation Process"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.spnlp-1.2", 
    "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing", 
    "fields_of_study": null, 
    "abstract": "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height \u2264 t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+Grappa.", 
    "year": 2021, 
    "venue": "SPNLP", 
    "references": 35, 
    "authors": [
      "Ohad Rubin", 
      "Jonathan Berant"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1118121.1118142", 
    "title": "Training a Dialogue Act Tagger for Human-human and Human-computer Travel dialogues", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the human-computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", 
    "year": 2002, 
    "venue": "SIGDIAL Workshop", 
    "references": 21, 
    "authors": [
      "R. Prasad", 
      "M. Walker"
    ], 
    "topics": [
      "Brill tagger", 
      "Text corpus", 
      "Tag (metadata)", 
      "Dialog system", 
      "Data (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.503", 
    "title": "Selective Question Answering under Domain Shift", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model\u2019s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model\u2019s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model\u2019s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model\u2019s probabilities only answers 48% at 80% accuracy.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 57, 
    "authors": [
      "A. Kamath", 
      "Robin Jia", 
      "Percy Liang"
    ], 
    "topics": [
      "Question answering", 
      "Softmax function", 
      "Need to know", 
      "Test data", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.469", 
    "title": "What Does BERT with Vision Look At?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments. We denote this ability as syntactic grounding. We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 39, 
    "authors": [
      "Liunian Harold Li", 
      "Mark Yatskar", 
      "Da Yin", 
      "Cho-Jui Hsieh", 
      "Kai-Wei Chang"
    ], 
    "topics": [
      "Testbed", 
      "Entity", 
      "Language model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/V1/W14-1001", 
    "title": "Analytical Approaches to Combining MT Technologies", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The talk will report on recent and ongoing work dedicated to analytical methods for a systematic combination of observed strengths of translation technologies. The focus will be on different ways of exploiting existing data on MT output and performance measures for system combination and for gaining insights on strengths and weaknesses of existing technologies.", 
    "year": 2014, 
    "venue": "HyTra@EACL", 
    "references": 0, 
    "authors": [
      "H. Uszkoreit"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.272", 
    "title": "Knowledge-Grounded Dialogue Generation with Pre-trained Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 60, 
    "authors": [
      "Xueliang Zhao", 
      "Wei Wu", 
      "Can Xu", 
      "Chongyang Tao", 
      "Dongyan Zhao", 
      "Rui Yan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.290", 
    "title": "Embarrassingly Simple Unsupervised Aspect Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 23, 
    "authors": [
      "St\u00e9phan Tulkens", 
      "Andreas van Cranenburgh"
    ], 
    "topics": [
      "Radial basis function kernel", 
      "Sentiment analysis", 
      "Benchmark (computing)", 
      "Part-of-speech tagging", 
      "Word embedding", 
      "Effective method", 
      "Experiment", 
      "Encoder", 
      "Brill tagger", 
      "Hybrid kernel"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2208", 
    "title": "DBMS-KU at SemEval-2019 Task 9: Exploring Machine Learning Approaches in Classifying Text as Suggestion or Non-Suggestion", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the participation of DBMS-KU team in the SemEval 2019 Task 9, that is, suggestion mining from online reviews and forums. To deal with this task, we explore several machine learning approaches, i.e., Random Forest (RF), Logistic Regression (LR), Multinomial Naive Bayes (MNB), Linear Support Vector Classification (LSVC), Sublinear Support Vector Classification (SSVC), Convolutional Neural Network (CNN), and Variable Length Chromosome Genetic Algorithm-Naive Bayes (VLCGA-NB). Our system obtains reasonable results of F1-Score 0.47 and 0.37 on the evaluation data in Subtask A and Subtask B, respectively. In particular, our obtained results outperform the baseline in Subtask A. Interestingly, the results seem to show that our system could perform well in classifying Non-suggestion class.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 19, 
    "authors": [
      "T. Fatyanosa", 
      "A. Siagian", 
      "Masayoshi Aritsugi"
    ], 
    "topics": [
      "Machine learning", 
      "SemEval", 
      "Random forest", 
      "Naive Bayes classifier", 
      "Convolutional neural network", 
      "Baseline (configuration management)", 
      "Feature selection", 
      "Genetic algorithm", 
      "Document classification", 
      "Multinomial logistic regression", 
      "F1 score", 
      "Electronic organizer", 
      "Radio frequency", 
      "Linear logic", 
      "Support vector machine", 
      "LR parser"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1003", 
    "title": "Building a shared world: mapping distributional to model-theoretic semantic spaces", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we introduce an approach to automatically map a standard distributional semantic space onto a set-theoretic model. We predict that there is a functional relationship between distributional information and vectorial concept representations in which dimensions are predicates and weights are generalised quantifiers. In order to test our prediction, we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quantifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 44, 
    "authors": [
      "Aur\u00e9lie Herbelot", 
      "Eva Maria Vecchi"
    ], 
    "topics": [
      "Natural language", 
      "Set theory", 
      "Domain-specific language", 
      "Information theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/982163.982189", 
    "title": "Applications", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Linguistic computation is the fundamental and primitive branch of the art of cumputatlon~ as I have remarked off and on. The insight of yon Neumann~ that operations and data can be represented in the same storage device, is the linguistic insight that anything can have a name in any language. (Whether anything can have a definition is a different question.) I recall surprising a couple of colleagues with this r~ark early in the 1960s, when I had to point out the obvious fact that compillng and interpreting are linguistic procedures and therefore that only in rare instances does a computer spend more time on mathematics than on linguistics. By now we all take the central position of our subject matter for granted. I express this overly familiar truth only for the pragmatic reason that some familiar truths are more helpful than others in preparing for a given discourse.", 
    "year": 1979, 
    "venue": "ACL", 
    "references": 2, 
    "authors": [
      "D. G. Hays"
    ], 
    "topics": [
      "Computation", 
      "Subject matter expert Turing test"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-2089", 
    "title": "Addressing Noise in Multidialectal Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Word embeddings are crucial to many natural language processing tasks. The quality of embeddings relies on large non-noisy corpora. Arabic dialects lack large corpora and are noisy, being linguistically disparate with no standardized spelling. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions\u2019 effects on low and high frequency words. In addition to boosting performance between 2-53%, we specifically improve on noisy, low frequency forms without compromising accuracy on high frequency forms.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 47, 
    "authors": [
      "Alexander Erdmann", 
      "Nasser Zalmout", 
      "Nizar Habash"
    ], 
    "topics": [
      "Word embedding", 
      "Text corpus", 
      "Natural language processing", 
      "Information", 
      "Data dictionary"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.91", 
    "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 46, 
    "authors": [
      "Elena Voita", 
      "Rico Sennrich", 
      "Ivan Titov"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3201", 
    "title": "An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term \u201cvision span\u201d\u2019 means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50% with modest loss in accuracy on English-Japanese and German-English translation tasks.", 
    "year": 2017, 
    "venue": "NMT@ACL", 
    "references": 19, 
    "authors": [
      "Raphael Shu", 
      "Hideki Nakayama"
    ], 
    "topics": [
      "Neural machine translation", 
      "Computation", 
      "Experiment", 
      "Encoder", 
      "Computational complexity theory", 
      "Text-based (computing)", 
      "Window function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00285", 
    "title": "Greedy Transition-Based Dependency Parsing with Stack LSTMs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks\u2014the stack long short-term memory unit (LSTM). Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser\u2019s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: (i) standard word vectors based on look-up tables and (ii) character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in morphologically rich languages. Finally, we discuss the use of dynamic oracles in training the parser. During training, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with dynamic oracles yields a linear-time greedy parser with very competitive performance.", 
    "year": 2017, 
    "venue": "CL", 
    "references": 133, 
    "authors": [
      "Miguel Ballesteros", 
      "Chris Dyer", 
      "Y. Goldberg", 
      "Noah A. Smith"
    ], 
    "topics": [
      "Parsing", 
      "Greedy algorithm", 
      "Long short-term memory", 
      "Text-based (computing)", 
      "Recurrent neural network", 
      "Named-entity recognition", 
      "Natural language processing", 
      "Language model", 
      "Time complexity", 
      "Word embedding", 
      "Vocabulary", 
      "Artificial neural network", 
      "Oracle machine", 
      "Treebank", 
      "Control flow", 
      "Data structure", 
      "Stack (abstract data type)", 
      "Experiment", 
      "Lookup table", 
      "Sampling (signal processing)", 
      "Baseline (configuration management)", 
      "Parser combinator"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-6112", 
    "title": "Leveraging coreference to identify arms in medical abstracts: An experimental study", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Performing systematic reviews is a critical yet manual, labor-intensive step in evidencebased medicine. Automating systematic reviews is an active area of research, requiring innovations in machine learning and computational linguistics. We examine how coreference resolution can aid in identifying the arms of a study, an often overlooked piece of information needed to synthesize the results in a systematic review. A classification model1 that performs better with the coreference features supports the intuition that coreference is able to capture the discourse salience of arms. We note that control arms do not benefit as much from these features.", 
    "year": 2016, 
    "venue": "Louhi@EMNLP", 
    "references": 40, 
    "authors": [
      "Elisa Ferracane", 
      "I. Marshall", 
      "Byron C. Wallace", 
      "Katrin Erk"
    ], 
    "topics": [
      "Coat of arms", 
      "Systematic review", 
      "Experiment", 
      "Machine learning", 
      "Computational linguistics", 
      "Baseline (configuration management)", 
      "F1 score", 
      "Error analysis (mathematics)", 
      "Software propagation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.woah-1.21", 
    "title": "Findings of the WOAH 5 Shared Task on Fine Grained Hateful Memes Detection", 
    "fields_of_study": null, 
    "abstract": "We present the results and main findings of the shared task at WOAH 5 on hateful memes detection. The task include two subtasks relating to distinct challenges in the fine-grained detection of hateful memes: (1) the protected category attacked by the meme and (2) the attack type. 3 teams submitted system description papers. This shared task builds on the hateful memes detection task created by Facebook AI Research in 2020.", 
    "year": 2021, 
    "venue": "WOAH", 
    "references": 29, 
    "authors": [
      "Lambert Mathias", 
      "Shaoliang Nie", 
      "Aida Mostafazadeh Davani", 
      "Douwe Kiela", 
      "Vinodkumar Prabhakaran", 
      "Bertie Vidgen", 
      "Zeerak Waseem"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_072", 
    "title": "Do Not Trust the Trolls: Predicting Credibility in Community Question Answering Forums", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We address information credibility in community forums, in a setting in which the credibility of an answer posted in a question thread by a particular user has to be predicted. First, we motivate the problem and we create a publicly available annotated English corpus by crowdsourcing. Second, we propose a large set of features to predict the credibility of the answers. The features model the user, the answer, the question, the thread as a whole, and the interaction between them. Our experiments with ranking SVMs show that the credibility labels can be predicted with high performance according to several standard IR ranking metrics, thus supporting the potential usage of this layer of credibility information in practical applications. The features modeling the profile of the user (in particular trollness) turn out to be most important, but embedding features modeling the answer and the similarity between the question and the answer are also very relevant. Overall, half of the gap between the baseline performance and the perfect classifier can be covered using the proposed features.", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 31, 
    "authors": [
      "Preslav Nakov", 
      "Tsvetomila Mihaylova", 
      "Llu\u00eds M\u00e0rquez i Villodre", 
      "Yashkumar Shiroya", 
      "Ivan Koychev"
    ], 
    "topics": [
      "Question answering", 
      "Long short-term memory", 
      "Overfitting", 
      "Crowdsourcing", 
      "Semantic similarity", 
      "Feature selection", 
      "Ranking SVM", 
      "Machine learning", 
      "Baseline (configuration management)", 
      "Text corpus", 
      "Convolutional neural network", 
      "Semi-supervised learning", 
      "Supervised learning", 
      "Experiment", 
      "Sensor", 
      "Embedded system", 
      "Semiconductor industry", 
      "Artificial neural network", 
      "Ranking (information retrieval)", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-3801", 
    "title": "Gendered Ambiguous Pronoun (GAP) Shared Task at the Gender Bias in NLP Workshop 2019", 
    "fields_of_study": [
      "Psychology"
    ], 
    "abstract": "The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling.", 
    "year": 2019, 
    "venue": "", 
    "references": 27, 
    "authors": [
      "Kellie Webster", 
      "M. Costa-juss\u00e0", 
      "Christian Hardmeier", 
      "Will Radford"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1163", 
    "title": "Improving Statistical Machine Translation with a Multilingual Paraphrase Database", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages. In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases. In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality. We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional profiles from monolingual source data.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 63, 
    "authors": [
      "Ramtin Mehdizadeh Seraj", 
      "Maryam Siahbani", 
      "Anoop Sarkar"
    ], 
    "topics": [
      "Statistical machine translation", 
      "BLEU", 
      "Test set", 
      "Source data", 
      "Vocabulary", 
      "Test data", 
      "Software propagation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2085", 
    "title": "Are All Languages Equally Hard to Language-Model?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.", 
    "year": 2018, 
    "venue": "NAACL 2018", 
    "references": 17, 
    "authors": [
      "Ryan Cotterell", 
      "Sabrina J. Mielke", 
      "Jason Eisner", 
      "Brian Roark"
    ], 
    "topics": [
      "Language model", 
      "Long short-term memory", 
      "Mathematical morphology"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.70", 
    "title": "What Context Features Can Transformer Language Models Use?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both midand longrange contexts, we find that several extremely destructive context manipulations\u2014including shuffling word order within sentences and deleting all words other than nouns\u2014remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.1", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 41, 
    "authors": [
      "Joe O'Connor", 
      "Jacob Andreas"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.470", 
    "title": "Long-Span Summarization via Local Attention and Content Selection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 63, 
    "authors": [
      "Potsawee Manakul", 
      "M. Gales"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.iwslt-1.28", 
    "title": "Inverted Projection for Robust Speech Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Traditional translation systems trained on written documents perform well for text-based translation but not as well for speech-based applications. We aim to adapt translation models to speech by introducing actual lexical errors from ASR and segmentation errors from automatic punctuation into our translation training data. We introduce an inverted projection approach that projects automatically detected system segments onto human transcripts and then re-segments the gold translations to align with the projected human transcripts. We demonstrate that this overcomes the train-test mismatch present in other training approaches. The new projection approach achieves gains of over 1 BLEU point over a baseline that is exposed to the human transcripts and segmentations, and these gains hold for both IWSLT data and YouTube data.", 
    "year": 2021, 
    "venue": "IWSLT", 
    "references": 30, 
    "authors": [
      "D. Padfield", 
      "Colin Cherry"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981574.981605", 
    "title": "Tailoring Lexical Choice to the User's Vocabulary in Multimedia Explanation Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we discuss the different strategies used in COMET (COordinated Multimedia Explanation Testbed) for selecting words with which the user is familiar. When pictures cannot be used to disambiguate a word or phrase, COMET has four strategies for avoiding unknown words. We give examples for each of these strategies and show how they are implemented in COMET.", 
    "year": 1993, 
    "venue": "ACL", 
    "references": 38, 
    "authors": [
      "K. McKeown", 
      "J. Robin", 
      "Michael A. Tanenblatt"
    ], 
    "topics": [
      "Vocabulary", 
      "Lexical choice", 
      "Testbed", 
      "Word-sense disambiguation", 
      "Image", 
      "Comet (programming)", 
      "Picture-in-picture"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119073", 
    "title": "Overview of Results of the MUC-6 Evaluation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC-6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC-6 evaluation are discussed in the paper by Grishman and Sundheim in this volume. All except the Scenario Template task are defined independently of any particular domain.", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 2, 
    "authors": [
      "B. Sundheim"
    ], 
    "topics": [
      "Standard Generalized Markup Language", 
      "Message Understanding Conference", 
      "Natural language processing", 
      "Information extraction", 
      "Named entity"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.113", 
    "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This task aims to build a model for identifying toxic words in whole posts. We use the BiLSTM-CRF model combining with ToxicBERT Classification to train the detection model for identifying toxic words in posts. Our model achieves 62.23% by F1-score on the Toxic Spans Detection task.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 22, 
    "authors": [
      "Son T. Luu", 
      "N. Nguyen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2008.34.4.621", 
    "title": "What's the Future for Computational Linguistics?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Shaped articles having improved mechanical strengths are comprised of (i) a nylon polyamide resin, (ii) a reinforcing amount of glass fibers therefor, and (iii) an impact strength increasing amount of a (meth)acrylic compound having the structural formula: [CH2=CR-CO-A]n X (I) wherein R is hydrogen or methyl, n is an integer equal to 2, 3 or 4, A is the linkage -O- or -NH- and X is an organic radical of valency n selected from the group consisting of linear or branched chain alkylene radical having from 1 to 20 carbon atoms, such alkylene radical comprising one or more chain-interrupting oxygen bridges and/or one or more hydroxyl groups, an aralkylene radical wherein the aryl moiety contains from 6 to 10 carbon atoms and the alkylene moiety is as defined above, such aralkylene radical comprising one or more alkylene chain-interrupting oxygen bridges, and a triazino heterocycle.", 
    "year": 2008, 
    "venue": "Computational Linguistics", 
    "references": 0, 
    "authors": [
      "R. Dale"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation", 
      "Printing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/S14-2068", 
    "title": "Link\u00f6ping: Cubic-Time Graph Parsing with a Simple Scoring Scheme", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We turn the Eisner algorithm for parsing to projective dependency trees into a cubictime algorithm for parsing to a restricted class of directed graphs. To extend the algorithm into a data-driven parser, we combine it with an edge-factored feature model and online learning. We report and discuss results on the SemEval-2014 Task 8 data sets (Oepen et al., 2014).", 
    "year": 2014, 
    "venue": "*SEMEVAL", 
    "references": 10, 
    "authors": [
      "Marco Kuhlmann"
    ], 
    "topics": [
      "Parsing", 
      "Algorithm", 
      "Feature model", 
      "SemEval", 
      "Directed graph", 
      "Online machine learning", 
      "Cubic function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075178.1075179", 
    "title": "An Ontology-based Semantic Tagger for IE system", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations. This work is part of an ongoing project for an information extraction system in the field of maritime Search And Rescue (SAR). Our purpose is to automatically annotate parts of texts with concepts from a SAR ontology. Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionary-thesaurus, and it uses a similarity measure for the classification. Evaluation is carried out by comparing the output of the system with key answers of predefined extraction templates.", 
    "year": 2003, 
    "venue": "ACL", 
    "references": 18, 
    "authors": [
      "Narj\u00e8s Boufaden"
    ], 
    "topics": [
      "Information extraction", 
      "Brill tagger", 
      "Similarity measure", 
      "Thesaurus", 
      "Dictionary", 
      "Transcription (software)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1258", 
    "title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 58, 
    "authors": [
      "Anusri Pampari", 
      "Preethi Raghavan", 
      "Jennifer J. Liang", 
      "Jian Peng"
    ], 
    "topics": [
      "Question answering", 
      "MIMIC", 
      "Excalibur: Morgana's Revenge", 
      "Software quality assurance", 
      "Natural language processing", 
      "Baseline (configuration management)", 
      "Java annotation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1262", 
    "title": "A Unified Syntax-aware Framework for Semantic Role Labeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 50, 
    "authors": [
      "Z. Li", 
      "Shexia He", 
      "Jiaxun Cai", 
      "Zhuosheng Zhang", 
      "Zhao Hai", 
      "Gongshen Liu", 
      "Linlin Li", 
      "Luo Si"
    ], 
    "topics": [
      "Semantic role labeling", 
      "Parse tree", 
      "Parsing", 
      "Cognitive engineering", 
      "Experiment", 
      "Long short-term memory", 
      "Computer science", 
      "Computational linguistics"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3346", 
    "title": "A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "BLEU is the de facto standard machine translation (MT) evaluation metric. How- ever, because BLEU computes a geo- metric mean of n-gram precisions, it of- ten correlates poorly with human judg- ment on the sentence-level. There- fore, several smoothing techniques have been proposed. This paper systemati- cally compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine transla- tion tuning.", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 13, 
    "authors": [
      "Boxing Chen", 
      "Colin Cherry"
    ], 
    "topics": [
      "Smoothing", 
      "BLEU", 
      "N-gram", 
      "Approximation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w15-2810", 
    "title": "Understanding Urban Land Use through the Visualization of Points of Interest", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Semantic data regarding points of interest in urban areas are hard to visualize. Due to the high number of points and categories they belong, as well as the associated textual information, maps become heavily cluttered and hard to read. Using traditional visualization techniques (e.g. dot distribution maps, typographic maps) partially solve this problem. Although, these techniques address different issues of the problem, their combination is hard and typically results in an efficient visualization. In our approach, we present a method to represent clusters of points of interest as shapes, which is based on vacuum package metaphor. The calculated shapes characterize sets of points and allow their use as containers for textual information. Additionally, we present a strategy for placing text onto polygons. The suggested method can be used in interactive visual exploration of semantic data distributed in space, and for creating maps with similar characteristics of dot distribution maps, but using shapes instead of points.", 
    "year": 2015, 
    "venue": "VL@EMNLP", 
    "references": 22, 
    "authors": [
      "Evgheni Polisciuc", 
      "A. Alves", 
      "P. Machado"
    ], 
    "topics": [
      "Point of interest", 
      "Map", 
      "Web application", 
      "Digital footprint", 
      "Relevance", 
      "Gon", 
      "Concave function", 
      "ENCODE", 
      "International Symposium on Fundamentals of Computation Theory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_015", 
    "title": "Inter-Annotator Agreement in Sentiment Analysis: Machine Learning Perspective", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Manual text annotation is an essential part of Big Text analytics. Although annotators work with limited parts of data sets, their results are extrapolated by automated text classification and affect the final classification results. Reliability of annotations and adequacy of assigned labels are especially important in the case of sentiment annotations. In the current study we examine inter-annotator agreement in multi-class, multi-label sentiment annotation of messages. We used several annotation agreement measures, as well as statistical analysis and Machine Learning to assess the resulting annotations.", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 17, 
    "authors": [
      "Victoria Bobicev", 
      "Marina Sokolova"
    ], 
    "topics": [
      "Machine learning", 
      "Sentiment analysis", 
      "Java annotation", 
      "Document classification", 
      "Statistical classification", 
      "Inter-rater reliability", 
      "Multi-label classification", 
      "Extrapolation", 
      "Text mining"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.560", 
    "title": "Language Embeddings for Typology and Cross-lingual Transfer Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference1.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 44, 
    "authors": [
      "Dian Yu", 
      "Taiqi He", 
      "Kenji Sagae"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1194", 
    "title": "Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question: \u201cCan we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?\u201d. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "Bingfeng Luo", 
      "Yansong Feng", 
      "Zheng Wang", 
      "Songfang Huang", 
      "Rui Yan", 
      "Dongyan Zhao"
    ], 
    "topics": [
      "Regular expression", 
      "Natural language processing", 
      "Natural language understanding", 
      "Supervised learning", 
      "Experiment", 
      "Neural Networks", 
      "Expectation propagation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.213", 
    "title": "Improved Speech Representations with Multi-Target Autoregressive Predictive Coding", 
    "fields_of_study": [
      "Computer Science", 
      "Engineering"
    ], 
    "abstract": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 34, 
    "authors": [
      "Yu-An Chung", 
      "James R. Glass"
    ], 
    "topics": [
      "Autoregressive model", 
      "Speech recognition", 
      "Downstream (software development)", 
      "Machine translation", 
      "Frame language", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-2162", 
    "title": "Lisbon: Evaluating TurboSemanticParser on Multiple Languages and Out-of-Domain Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "As part of the SemEval-2015 shared task on Broad-Coverage Semantic Dependency Parsing, we evaluate the performace of our last year\u2019s system (TurboSemanticParser) on multiple languages and out-of-domain data. Our system is characterized by a feature-rich linear model, that includes scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating directions dual decomposition (AD 3 ). The experiments have shown that, even though the parser\u2019s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 24, 
    "authors": [
      "Mariana S. C. Almeida", 
      "Andr\u00e9 F. T. Martins"
    ], 
    "topics": [
      "Lagrangian relaxation", 
      "Domain adaptation", 
      "Linear model", 
      "Linear programming relaxation", 
      "Parsing", 
      "Software feature", 
      "Experiment", 
      "Alternating Turing machine"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.171", 
    "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 57, 
    "authors": [
      "Daniel Khashabi", 
      "Sewon Min", 
      "Tushar Khot", 
      "Ashish Sabharwal", 
      "Oyvind Tafjord", 
      "P. Clark", 
      "Hannaneh Hajishirzi"
    ], 
    "topics": [
      "Question answering", 
      "Software quality assurance", 
      "Language model", 
      "File spanning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1053", 
    "title": "Deep Reinforcement Learning for Chinese Zero Pronoun Resolution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Qingyu Yin", 
      "Y. Zhang", 
      "Weinan Zhang", 
      "Ting Liu", 
      "William Yang Wang"
    ], 
    "topics": [
      "Reinforcement learning", 
      "Anaphora (linguistics)", 
      "Parse tree", 
      "Ground truth", 
      "Artificial neural network", 
      "Network model", 
      "Parsing", 
      "F1 score", 
      "Experiment", 
      "Benchmark (computing)", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075253", 
    "title": "Language Independent, Minimally Supervised Induction of Lexical Probabilities", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A central problem in part-of-speech tagging, especially for new languages for which limited annotated resources are available, is estimating the distribution of lexical probabilities for unknown words. This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 15, 
    "authors": [
      "Silviu Cucerzan", 
      "David Yarowsky"
    ], 
    "topics": [
      "Probability"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2163", 
    "title": "Fermi at SemEval-2019 Task 4: The sarah-jane-smith Hyperpartisan News Detector", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our system (Fermi) for Task 4: Hyper-partisan News detection of SemEval-2019. We use simple text classification algorithms by transforming the input features to a reduced feature set. We aim to find the right number of features useful for efficient classification and explore multiple training models to evaluate the performance of these text classification algorithms. Our team - Fermi\u2019s model achieved an accuracy of 59.10% and an F1 score of 69.5% on the official test data set. In this paper, we provide a detailed description of the approach as well as the results obtained in the task.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 10, 
    "authors": [
      "Nikhil Chakravartula", 
      "Vijayasaradhi Indurthi", 
      "B. Syed"
    ], 
    "topics": [
      "Jane (software)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1117769.1117774", 
    "title": "Using Co-occurrence Statistics as an Information Source for Partial Parsing of Chinese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our partial parser for Chinese uses a learned classifier to guide a bottom-up parsing process. We describe improvements in performance obtained by expanding the information available to the classifier, from POS sequences only, to include measures of word association derived from co-occurrence statistics. We compare performance using different measures of association, and find that Yule's coefficient of colligation Y gives somewhat better results over other measures.", 
    "year": 2000, 
    "venue": "ACL 2000", 
    "references": 28, 
    "authors": [
      "E. Dr\u00e1bek", 
      "Qiang Zhou"
    ], 
    "topics": [
      "Information source", 
      "Statistical classification", 
      "Bottom-up parsing", 
      "Coefficient", 
      "Jaccard index"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00289", 
    "title": "Efficient Contextual Representation Learning With Continuous Outputs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Abstract Contextual representation models have achieved great success in improving various downstream natural language processing tasks. However, these language-model-based encoders are difficult to train due to their large parameter size and high computational complexity. By carefully examining the training procedure, we observe that the softmax layer, which predicts a distribution of the target word, often induces significant overhead, especially when the vocabulary size is large. Therefore, we revisit the design of the output layer and consider directly predicting the pre-trained embedding of the target word for a given context. When applied to ELMo, the proposed approach achieves a 4-fold speedup and eliminates 80% trainable parameters while achieving competitive performance on downstream tasks. Further analysis shows that the approach maintains the speed advantage under various settings, even when the sentence encoder is scaled up.", 
    "year": 2019, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 104, 
    "authors": [
      "Liunian Harold Li", 
      "Patrick H. Chen", 
      "Cho-Jui Hsieh", 
      "Kai-Wei Chang"
    ], 
    "topics": [
      "Softmax function", 
      "Natural language processing", 
      "Downstream (software development)", 
      "Encoder", 
      "Computational complexity theory", 
      "Vocabulary", 
      "Experiment", 
      "Speedup", 
      "Machine learning", 
      "Overhead (computing)", 
      "Language model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.232", 
    "title": "Learning to execute instructions in a Minecraft dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B\u2019s past actions as well as B\u2019s perspective leads to a significant improvement in performance on this challenging language understanding problem.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 44, 
    "authors": [
      "Prashant Jayannavar", 
      "Anjali Narayan-Chen", 
      "J. Hockenmaier"
    ], 
    "topics": [
      "Minecraft"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/w14-15", 
    "title": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality, CVSC@EACL 2014, Gothenburg, Sweden, April 26-30, 2014", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we introduce several vector space manipulation methods that are applied to trained vector space models in a post-hoc fashion, and present an application of these techniques in semantic role labeling for Finnish and English. Specifically, we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments. Further, we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments, within the same vector space.", 
    "year": 2014, 
    "venue": "CVSC@EACL", 
    "references": 83, 
    "authors": [
      "A. Allauzen", 
      "R. Bernardi", 
      "Edward Grefenstette", 
      "H. Larochelle", 
      "Christopher D. Manning", 
      "S. Yih"
    ], 
    "topics": [
      "Semantic role labeling", 
      "Computational linguistics", 
      "Computation", 
      "Predicate (mathematical logic)", 
      "Esthesia", 
      "Hoc (programming language)", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-0105", 
    "title": "Predicting and Explaining Human Semantic Search in a Cognitive Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent work has attempted to characterize the structure of semantic memory and the search algorithms which, together, best approximate human patterns of search revealed in a semantic fluency task. There are a number of models that seek to capture semantic search processes over networks, but they vary in the cognitive plausibility of their implementation. Existing work has also neglected to consider the constraints that the incremental process of language acquisition must place on the structure of semantic memory. Here we present a model that incrementally updates a semantic network, with limited computational steps, and replicates many patterns found in human semantic fluency using a simple random walk. We also perform thorough analyses showing that a combination of both structural and semantic features are correlated with human performance patterns.", 
    "year": 2018, 
    "venue": "CMCL", 
    "references": 27, 
    "authors": [
      "Filip Miscevic", 
      "Aida Nematzadeh", 
      "Suzanne Stevenson"
    ], 
    "topics": [
      "Semantic network", 
      "Semantic search", 
      "Cognitive model", 
      "Text corpus", 
      "Search algorithm", 
      "Wikipedia", 
      "Human reliability", 
      "Plausibility structure", 
      "Batch processing", 
      "Markov property", 
      "Natural language", 
      "Semantic similarity", 
      "Self-replicating machine", 
      "Word lists by frequency", 
      "Item response theory", 
      "Beagle", 
      "Cosine similarity", 
      "Interdependence", 
      "Learnability", 
      "Experiment", 
      "Approximation algorithm", 
      "Lexicon", 
      "CHILDES", 
      "LOCUS", 
      "Align (company)", 
      "Emoticon", 
      "Markov chain", 
      "Jones calculus", 
      "Incremental backup", 
      "Software performance testing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/E14-1026", 
    "title": "Source-side Preordering for Translation using Logistic Regression and Depth-first Branch-and-Bound Search", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes. We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.", 
    "year": 2014, 
    "venue": "EACL", 
    "references": 34, 
    "authors": [
      "Laura Jehl", 
      "A. Gispert", 
      "Mark Hopkins", 
      "B. Byrne"
    ], 
    "topics": [
      "Logistic regression", 
      "Branch and bound", 
      "Parse tree", 
      "Depth-first search", 
      "Machine translation", 
      "Hot swapping", 
      "BLEU", 
      "Software feature", 
      "Experiment", 
      "Run time (program lifecycle phase)", 
      "Baseline (configuration management)", 
      "Parsing", 
      "Emoticon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.166", 
    "title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset will be released to the public.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Tiangang Zhu", 
      "Yue Wang", 
      "Haoran Li", 
      "Youzheng Wu", 
      "X. He", 
      "Bowen Zhou"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.316", 
    "title": "DagoBERT: Generating Derivational Morphology with a Pretrained Language Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT's derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT's derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 45, 
    "authors": [
      "Valentin Hofmann", 
      "J. Pierrehumbert", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.225", 
    "title": "Multitask Easy-First Dependency Parsing: Exploiting Complementarities of Different Dependency Representations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present a parsing model for projective dependency trees which takes advantage of the existence of complementary dependency annotations which is the case in Arabic, with the availability of CATiB and UD treebanks. Our system performs syntactic parsing according to both annotation types jointly as a sequence of arc-creating operations, and partially created trees for one annotation are also available to the other as features for the score function. This method gives error reduction of 9.9% on CATiB and 6.1% on UD compared to a strong baseline, and ablation tests show that the main contribution of this reduction is given by sharing tree representation between tasks, and not simply sharing BiLSTM layers as is often performed in NLP multitask systems.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 33, 
    "authors": [
      "Yash Kankanampati", 
      "Joseph Le Roux", 
      "Nadi Tomeh", 
      "Dima Taji", 
      "Nizar Habash"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1220175.1220272", 
    "title": "Semi-Supervised Training for Statistical Word Alignment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.", 
    "year": 2006, 
    "venue": "ACL", 
    "references": 200, 
    "authors": [
      "Alexander M. Fraser", 
      "D. Marcu"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Semiconductor industry", 
      "Expectation\u2013maximization algorithm", 
      "Semi-supervised learning", 
      "Data structure alignment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.413", 
    "title": "Visuo-Lingustic Question Answering (VLQA) Challenge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 42, 
    "authors": [
      "Shailaja Keyur Sampat", 
      "Yezhou Yang", 
      "Chitta Baral"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5033", 
    "title": "Conversational Image Editing: Incremental Intent Identification in a New Dialogue Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present \u201cconversational image editing\u201d, a novel real-world application domain combining dialogue, visual information, and the use of computer vision. We discuss the importance of dialogue incrementality in this task, and build various models for incremental intent identification based on deep learning and traditional classification algorithms. We show how our model based on convolutional neural networks outperforms models based on random forests, long short term memory networks, and conditional random fields. By training embeddings based on image-related dialogue corpora, we outperform pre-trained out-of-the-box embeddings, for intention identification tasks. Our experiments also provide evidence that incremental intent processing may be more efficient for the user and could save time in accomplishing tasks.", 
    "year": 2018, 
    "venue": "SIGDIAL Conference", 
    "references": 46, 
    "authors": [
      "R. Manuvinakurike", 
      "Trung Bui", 
      "W. Chang", 
      "Kallirroi Georgila"
    ], 
    "topics": [
      "Image editing", 
      "Computer vision", 
      "Deep learning", 
      "Random forest", 
      "Convolutional neural network", 
      "Dialog system", 
      "Application domain", 
      "Conditional random field", 
      "Experiment", 
      "Long short-term memory", 
      "Out of the box (feature)", 
      "Algorithm", 
      "Text corpus", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.519", 
    "title": "100,000 Podcasts: A Spoken English Document Corpus", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Podcasts are a large and growing repository of spoken audio. As an audio format, podcasts are more varied in style and production type than broadcast news, contain more genres than typically studied in video data, and are more varied in style and format than previous corpora of conversations. When transcribed with automatic speech recognition they represent a noisy but fascinating collection of documents which can be studied through the lens of natural language processing, information retrieval, and linguistics. Paired with the audio files, they are also a resource for speech processing and the study of paralinguistic, sociolinguistic, and acoustic aspects of the domain. We introduce the Spotify Podcast Dataset, a new corpus of 100,000 podcasts. We demonstrate the complexity of the domain with a case study of two tasks: (1) passage search and (2) summarization. This is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 48, 
    "authors": [
      "Ann Clifton", 
      "S. Reddy", 
      "Yongze Yu", 
      "Aasish Pappu", 
      "R. Rezapour", 
      "Hamed Bonab", 
      "Maria Eskevich", 
      "G. Jones", 
      "Jussi Karlgren", 
      "Ben Carterette", 
      "R. Jones"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.540", 
    "title": "An Element-aware Multi-representation Model for Law Article Prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 14, 
    "authors": [
      "Huilin Zhong", 
      "Junsheng Zhou", 
      "Weiguang Qu", 
      "Yunfei Long", 
      "Y. Gu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2901", 
    "title": "Augmenting FrameNet Via PPDB", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "FrameNet is a lexico-semantic dataset that embodies the theory of frame semantics. Like other semantic databases, FrameNet is incomplete. We augment it via the paraphrase database, PPDB, and gain a threefold increase in coverage at 65% precision.", 
    "year": 2014, 
    "venue": "EVENTS@ACL", 
    "references": 18, 
    "authors": [
      "Pushpendre Rastogi", 
      "Benjamin Van Durme"
    ], 
    "topics": [
      "FrameNet", 
      "Database", 
      "Lexico"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-8653", 
    "title": "Can Neural Image Captioning be Controlled via Forced Attention?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The weights applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight into the internal workings of the generator. In this paper, we reverse the direction of this connection and ask whether through the control of the attention of the model we can control its output. Specifically, we take a standard neural image captioning model that uses attention, and fix the attention to pre-determined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the attention and find that these are producing expected results in up to 28.56% of the cases.", 
    "year": 2019, 
    "venue": "INLG", 
    "references": 16, 
    "authors": [
      "P. Sadler", 
      "Tatjana Scheffler", 
      "D. Schlangen"
    ], 
    "topics": [
      "Natural language generation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.bionlp-1.17", 
    "title": "Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes", 
    "fields_of_study": [
      "Computer Science", 
      "Medicine"
    ], 
    "abstract": "Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort.", 
    "year": 2020, 
    "venue": "BIONLP", 
    "references": 55, 
    "authors": [
      "Brian Hur", 
      "Timothy Baldwin", 
      "Karin M. Verspoor", 
      "L. Hardefeldt", 
      "J. Gilkerson"
    ], 
    "topics": [
      "Domain adaptation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.310", 
    "title": "It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Commonsense reasoning is one of the key problems in natural language processing, but the relative scarcity of labeled data holds back the progress for languages other than English. Pretrained cross-lingual models are a source of powerful language-agnostic representations, yet their inherent reasoning capabilities are still actively studied. In this work, we design a simple approach to commonsense reasoning which trains a linear classifier with weights of multi-head attention as features. To evaluate this approach, we create a multilingual Winograd Schema corpus by processing several datasets from prior work within a standardized pipeline and measure cross-lingual generalization ability in terms of out-of-sample performance. The method performs competitively with recent supervised and unsupervised approaches for commonsense reasoning, even when applied to other languages in a zero-shot manner. Also, we demonstrate that most of the performance is given by the same small subset of attention heads for all studied languages, which provides evidence of universal reasoning capabilities in multilingual encoders.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 50, 
    "authors": [
      "Alexey Tikhonov", 
      "Max Ryabinin"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-8627", 
    "title": "Neural Conversation Model Controllable by Given Dialogue Act Based on Adversarial Learning and Label-aware Objective", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Building a controllable neural conversation model (NCM) is an important task. In this paper, we focus on controlling the responses of NCMs by using dialogue act labels of responses as conditions. We introduce an adversarial learning framework for the task of generating conditional responses with a new objective to a discriminator, which explicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed method with some existing methods for generating conditional responses. The experimental results show that our proposed method has higher controllability for dialogue acts even though it has higher or comparable naturalness to existing methods.", 
    "year": 2019, 
    "venue": "INLG", 
    "references": 37, 
    "authors": [
      "Seiya Kawano", 
      "Koichiro Yoshino", 
      "Satoshi Nakamura"
    ], 
    "topics": [
      "Discriminator", 
      "Word lists by frequency", 
      "Presto", 
      "Adversary (cryptography)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1244", 
    "title": "Be Consistent! Improving Procedural Text Comprehension using Label Consistency", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 27, 
    "authors": [
      "X. Du", 
      "Bhavana Dalvi Mishra", 
      "Niket Tandon", 
      "Antoine Bosselut", 
      "Wen-tau Yih", 
      "P. Clark", 
      "Claire Cardie"
    ], 
    "topics": [
      "List comprehension", 
      "Semi-supervised learning", 
      "Optimistic concurrency control", 
      "Supervised learning", 
      "Benchmark (computing)", 
      "Entity", 
      "Semiconductor industry", 
      "Google Cloud Messaging"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2006.32.2.263", 
    "title": "The PARADISE Evaluation Framework: Issues and Findings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "There has been a great deal of interest over the past 20 years in developing metrics and frameworks for evaluating and comparing the performance of spoken-language dialogue systems. One of the results of this interest is a potential general methodology, known as the PARADISE framework. This squib highlights some important issues concerning the application of PARADISE that have, up to now, not been sufficiently emphasized or have even been neglected by the dialogue-system community. These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the influence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments. These evaluations include different dependent variables and examination of individual user-satisfaction measures.", 
    "year": 2006, 
    "venue": "Computational Linguistics", 
    "references": 25, 
    "authors": [
      "M. Hajdinjak", 
      "F. Mihelic"
    ], 
    "topics": [
      "Speech recognition", 
      "Dialog system", 
      "Integrated Woz Machine", 
      "Experiment", 
      "Specular highlight", 
      "Learning to rank", 
      "Dialog tree"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.184", 
    "title": "Fusion: Towards Automated ICD Coding via Feature Compression", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "ICD coding aims to automatically assign International Classification of Diseases (ICD) codes from unstructured clinical notes or discharge summaries, which saves human labor and reduces errors. Although several studies are proposed to solve this challenging task, none distinguishes the importance of different phrases with a word window. Intuitively, informative phrases should be more useful for the prediction. This paper proposes a feature compressed ICD coding model named Fusion to address this issue. In particular, we propose an attentive soft-pooling approach to compress the sparse and redundant word representations into informative and dense ones as local features. Besides, we use the key-query attention mechanism for modeling the inner relations among local features to generate the global features, which are further used to predict ICD codes. Experiments on two widely used datasets demonstrate that Fusion outperforms baselines. However, on the MIMIC-III Full dataset, we find that none of the state-ofthe-art approaches significantly perform better than others. Thus, automated ICD coding is still a challenging task.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 23, 
    "authors": [
      "Junyu Luo", 
      "Cao Xiao", 
      "Lucas Glass", 
      "Jimeng Sun", 
      "Fenglong Ma"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2020.nlpcovid19-2.10", 
    "title": "Explaining the Trump Gap in Social Distancing Using COVID Discourse", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on heterogeneous meanings in communities across the United States and that these disparate meanings shaped communities\u2019 response to the virus during the early, vital stages of the outbreak in the U.S. Using word embeddings, we demonstrate that counties where residents socially distanced less on average (as measured by residential mobility) more semantically associated the virus in their COVID discourse with concepts of fraud, the political left, and more benign illnesses like the flu. We also show that the different meanings the virus took on in different communities explains a substantial fraction of what we call the \u201cTrump Gap,\u201d or the empirical tendency for more Trump-supporting counties to socially distance less. This work demonstrates that community-level processes of meaningmaking determined behavioral responses to the COVID-19 pandemic and that these processes can be measured unobtrusively using Twitter.", 
    "year": 2020, 
    "venue": "NLP4COVID@EMNLP", 
    "references": 20, 
    "authors": [
      "Austin van Loon", 
      "Sheridan Stewart", 
      "Brandon Waldon", 
      "S. K. Lakshmikanth", 
      "Ishan Shah", 
      "Sharath Chandra Guntuku", 
      "G. Sherman", 
      "J. Zou", 
      "J. Eichstaedt"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.42", 
    "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don't have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 46, 
    "authors": [
      "Saadia Gabriel", 
      "A. \u00c7elikyilmaz", 
      "Rahul Jha", 
      "Yejin Choi", 
      "Jianfeng Gao"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1104", 
    "title": "Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Many machine learning datasets are noisy with a substantial number of mislabeled instances. This noise yields sub-optimal classification performance. In this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations. We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost. Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques. Our techniques save a considerable amount of time.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "Justin Martineau", 
      "Lu Chen", 
      "D. Cheng", 
      "A. Sheth"
    ], 
    "topics": [
      "Algorithm", 
      "Amazon Mechanical Turk", 
      "Active learning (machine learning)", 
      "Data quality", 
      "Machine learning", 
      "Analysis of algorithms", 
      "Nonlinear system", 
      "Baseline (configuration management)", 
      "Statistical classification", 
      "Crowdsourcing", 
      "Interactivity", 
      "Data point", 
      "Experiment", 
      "Computation", 
      "The Turk", 
      "Tf\u2013idf"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1296", 
    "title": "Weakly Supervised Multilingual Causality Extraction from Wikipedia", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a method for extracting causality knowledge from Wikipedia, such as Protectionism \u2192 Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 61, 
    "authors": [
      "Chikara Hashimoto"
    ], 
    "topics": [
      "Causality"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.86", 
    "title": "Answering Legal Questions by Learning Neural Attentive Text Representation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text representation plays a vital role in retrieval-based question answering, especially in the legal domain where documents are usually long and complicated. The better the question and the legal documents are represented, the more accurate they are matched. In this paper, we focus on the task of answering legal questions at the article level. Given a legal question, the goal is to retrieve all the correct and valid legal articles, that can be used as the basic to answer the question. We present a retrieval-based model for the task by learning neural attentive text representation. Our text representation method first leverages convolutional neural networks to extract important information in a question and legal articles. Attention mechanisms are then used to represent the question and articles and select appropriate information to align them in a matching process. Experimental results on an annotated corpus consisting of 5,922 Vietnamese legal questions show that our model outperforms state-of-the-art retrieval-based methods for question answering by large margins in terms of both recall and NDCG.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 29, 
    "authors": [
      "Phi Manh Kien", 
      "Nguyen Ha Thanh", 
      "Ngo Xuan Bach", 
      "V. Tran", 
      "M. Nguyen", 
      "Tu Minh Phuong"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-5410", 
    "title": "Weakly supervised construction of a repository of iconic images", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a first attempt at semi-automatically harvesting a dataset of iconic images. Iconic images are depicting objects or scenes, which arouse associations to abstract topics. Our method starts with representative topic-evoking images from Wikipedia, which are labeled with relevant concepts and entities found in their associated captions. These are used to query an online image repository (i.e., Flickr), in order to further acquire additional examples of topic-specific iconic relations. To this end, we leverage a combination of visual similarity measures, image clustering and matching algorithms to acquire clusters of iconic images that are topically connected to the original seed images, while also allowing for various degrees of diversity. Our first results are promising in that they indicate the feasibility of the task and that we are able to build a first version of our resource with minimal supervision.", 
    "year": 2014, 
    "venue": "VL@COLING", 
    "references": 9, 
    "authors": [
      "Lydia Weiland", 
      "W. Effelsberg", 
      "Simone Paolo Ponzetto"
    ], 
    "topics": [
      "Supervised learning", 
      "Computer vision", 
      "Flickr", 
      "Wikipedia", 
      "HTTPS", 
      "Scale-invariant feature transform", 
      "Matching (graph theory)", 
      "Local-density approximation", 
      "Region growing", 
      "Algorithm", 
      "Cluster analysis", 
      "Entity", 
      "Semiconductor industry"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w18-5433", 
    "title": "Context-Free Transductions with Neural Stacks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper analyzes the behavior of stack-augmented recurrent neural network (RNN) models. Due to the architectural similarity between stack RNNs and pushdown transducers, we train stack RNN models on a number of tasks, including string reversal, context-free language modelling, and cumulative XOR evaluation. Examining the behavior of our networks, we show that stack-augmented RNNs can discover intuitive stack-based strategies for solving our tasks. However, stack RNNs are more difficult to train than classical architectures such as LSTMs. Rather than employ stack-based strategies, more complex networks often find approximate solutions by using the stack as unstructured memory.", 
    "year": 2018, 
    "venue": "BlackboxNLP@EMNLP", 
    "references": 17, 
    "authors": [
      "Yiding Hao", 
      "William Cooper Merrill", 
      "D. Angluin", 
      "R. Frank", 
      "Noah Amsel", 
      "A. Benz", 
      "S. Mendelsohn"
    ], 
    "topics": [
      "Stack-oriented programming language", 
      "Recurrent neural network", 
      "Approximation algorithm", 
      "Language model", 
      "Complex network", 
      "Artificial neural network", 
      "Transducer", 
      "Mathematical optimization", 
      "Stack (abstract data type)", 
      "Greedy algorithm", 
      "Random neural network", 
      "Experiment", 
      "Exclusive or", 
      "XOR"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1117729.1117730", 
    "title": "Comparing Corpora using Frequency Profiling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a method of comparing corpora which uses frequency profiling. The method can be used to discover key words in the corpora which differentiate one corpus from another. Using annotated corpora, it can be applied to discover key grammatical or word-sense categories. This can be used as a quick way in to find the differences between the corpora and is shown to have applications in the study of social differentiation in the use of English vocabulary, profiling of learner English and document analysis in the software engineering process.", 
    "year": 2000, 
    "venue": "", 
    "references": 28, 
    "authors": [
      "Paul Rayson", 
      "R. Garside"
    ], 
    "topics": [
      "Text corpus", 
      "Software development process", 
      "Software engineering", 
      "Vocabulary", 
      "Word sense"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00273", 
    "title": "A Statistical, Grammar-Based Approach to Microplanning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although there has been much work in recent years on data-driven natural language generation, little attention has been paid to the fine-grained interactions that arise during microplanning between aggregation, surface realization, and sentence segmentation. In this article, we propose a hybrid symbolic/statistical approach to jointly model the constraints regulating these interactions. Our approach integrates a small handwritten grammar, a statistical hypertagger, and a surface realization algorithm. It is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence. We evaluate our approach in several ways. A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage. Results from a human study indicate that users find the output of this hybrid statistic/symbolic system more fluent than both a template-based and a purely symbolic grammar-based approach. Finally, we illustrate by means of examples that our approach can account for various factors impacting aggregation, sentence segmentation, and surface realization.", 
    "year": 2017, 
    "venue": "Computational Linguistics", 
    "references": 48, 
    "authors": [
      "Claire Gardent", 
      "Laura Perez-Beltrachini"
    ], 
    "topics": [
      "Natural language generation", 
      "Linked data", 
      "Resource Description Framework", 
      "Knowledge base", 
      "Interaction", 
      "Text corpus", 
      "Tag cloud", 
      "Stochastic grammar", 
      "HTML element", 
      "Data model", 
      "Algorithm", 
      "Natural language understanding", 
      "Hard coding", 
      "Logical connective", 
      "SimpleText", 
      "CA-Realizer", 
      "WS-Coordination", 
      "Hoc (programming language)", 
      "VP-Info", 
      "Phylogenetic tree", 
      "Functional discourse grammar", 
      "Word lists by frequency", 
      "Boxing", 
      "Social network aggregation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1369", 
    "title": "Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In news and discussions, many articles and posts are provided without their related previous articles or posts. Hence, it is difficult to understand the context from which the articles and posts have occurred. In this paper, we propose the Hierarchical Dirichlet Gaussian Marked Hawkes process (HD-GMHP) for reconstructing the narratives and thread structures of news articles and discussion posts. HD-GMHP unifies three modeling strategies in previous research: temporal characteristics, triggering event relations, and meta information of text in news articles and discussion threads. To show the effectiveness of the model, we perform experiments in narrative reconstruction and thread reconstruction with real world datasets: articles from the New York Times and a corpus of Wikipedia conversations. The experimental results show that HD-GMHP outperforms the baselines of LDA, HDP, and HDHP for both tasks.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 33, 
    "authors": [
      "Yeon Seonwoo", 
      "Alice H. Oh", 
      "Sungjoon Park"
    ], 
    "topics": [
      "The New York Times", 
      "Text corpus", 
      "Wikipedia", 
      "Experiment", 
      "Cluster analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-1126", 
    "title": "Orthogonality of Syntax and Semantics within Distributional Spaces", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A recent distributional approach to wordanalogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, posinduction and word-analogy tasks.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 21, 
    "authors": [
      "Jeff Mitchell", 
      "Mark Steedman"
    ], 
    "topics": [
      "Semantic similarity", 
      "Mark Steedman", 
      "Wikipedia", 
      "Word embedding", 
      "Spaces", 
      "Artificial neural network", 
      "Recursion", 
      "Network model", 
      "Parsing", 
      "Experiment", 
      "Part-of-speech tagging", 
      "Frank Soltis", 
      "Galaxy morphological classification", 
      "STELLA", 
      "Kerrison Predictor"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2920", 
    "title": "Mining HEXACO personality traits from Enterprise Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we introduce a novel computational technique of extraction of personality traits (HEXACO) of employees from Enterprise Social Media posts. We deal with challenges such as not being able to use existing survey instruments for scoring and not being able to directly use existing psychological studies on written text due to lack of overlapping words between the existing dictionary and words used in Enterprise Social Media. Using our approach we are able to infer personality traits (HEXACO) from posts and find better coverage and usage of the extended dictionary.", 
    "year": 2015, 
    "venue": "WASSA@EMNLP", 
    "references": 18, 
    "authors": [
      "Priyanka Sinha", 
      "Lipika Dey", 
      "Pabitra Mitra", 
      "A. Basu"
    ], 
    "topics": [
      "Social media", 
      "Vocabulary", 
      "Dictionary", 
      "First-person (video games)", 
      "Blog", 
      "Word lists by frequency"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2080", 
    "title": "Learning Translational and Knowledge-based Similarities from Relevance Rankings for Cross-Language Retrieval", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present an approach to cross-language retrieval that combines dense knowledgebased features and sparse word translations. Both feature types are learned directly from relevance rankings of bilingual documents in a pairwise ranking framework. In large-scale experiments for patent prior art search and cross-lingual retrieval in Wikipedia, our approach yields considerable improvements over learningto-rank with either only dense or only sparse features, and over very competitive baselines that combine state-of-the-art machine translation and retrieval.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 31, 
    "authors": [
      "Shigehiko Schamoni", 
      "F. Hieber", 
      "A. Sokolov", 
      "S. Riezler"
    ], 
    "topics": [
      "Relevance", 
      "Wikipedia", 
      "Learning to rank", 
      "Machine translation", 
      "Sparse matrix", 
      "Supervised learning", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.DEELIO-1.13", 
    "title": "Attention vs non-attention for a Shapley-based explanation method", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods \u2013 that are often proposed and tested in the domain of computer vision \u2013 are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) \u2013 a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models \u2013 and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.", 
    "year": 2021, 
    "venue": "DEELIO", 
    "references": 36, 
    "authors": [
      "Tom Kersten", 
      "Hugh Mee Wong", 
      "Jaap Jumelet", 
      "Dieuwke Hupkes"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-1522", 
    "title": "Distributed Word Representations Improve NER for e-Commerce", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a case study of using distributed word representations, word2vec in particular, for improving performance of Named Entity Recognition for the eCommerce domain. We also demonstrate that distributed word representations trained on a smaller amount of in-domain data are more effective than word vectors trained on very large amount of out-of-domain data, and that their combination gives the best results.", 
    "year": 2015, 
    "venue": "VS@HLT-NAACL", 
    "references": 22, 
    "authors": [
      "Mahesh Joshi", 
      "Ethan Hart", 
      "Mirko Vogel", 
      "Jean-David Ruvini"
    ], 
    "topics": [
      "Named-entity recognition", 
      "E-commerce", 
      "Natural language processing", 
      "Text corpus", 
      "Word2vec", 
      "Word embedding", 
      "Experiment", 
      "Named entity"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K18-2005", 
    "title": "Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford\u2019s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84%) and outperformed the other systems by a large margin.", 
    "year": 2018, 
    "venue": "CoNLL", 
    "references": 32, 
    "authors": [
      "Wanxiang Che", 
      "Yijia Liu", 
      "Yuxuan Wang", 
      "Bo Zheng", 
      "Ting Liu"
    ], 
    "topics": [
      "Treebank", 
      "Concatenation", 
      "Parsing", 
      "Part-of-speech tagging", 
      "Brill tagger", 
      "Urban Dictionary", 
      "libLAS"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3334", 
    "title": "Estimating Word Alignment Quality for SMT Reordering Tasks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quali ...", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 51, 
    "authors": [
      "Sara Stymne", 
      "J. Tiedemann", 
      "Joakim Nivre"
    ], 
    "topics": [
      "Bitext word alignment", 
      "Precision and recall", 
      "Align (company)", 
      "Crossing number (graph theory)", 
      "Hidden Markov model", 
      "Sequence alignment", 
      "Word lists by frequency", 
      "Decibel", 
      "Machine translation", 
      "Sparse", 
      "Experiment", 
      "Data structure alignment", 
      "F1 score", 
      "Translation unit (programming)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N16-1098", 
    "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the `Story Cloze Test\u2019. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", 
    "year": 2016, 
    "venue": "NAACL", 
    "references": 53, 
    "authors": [
      "N. Mostafazadeh", 
      "Nathanael Chambers", 
      "Xiaodong He", 
      "Devi Parikh", 
      "Dhruv Batra", 
      "Lucy Vanderwende", 
      "P. Kohli", 
      "James F. Allen"
    ], 
    "topics": [
      "Text corpus", 
      "Natural language understanding", 
      "Causality", 
      "Commonsense knowledge (artificial intelligence)", 
      "Baseline (configuration management)", 
      "Display resolution", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1003", 
    "title": "Practical Obstacles to Deploying Active Learning", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 39, 
    "authors": [
      "David Lowell", 
      "Zachary Chase Lipton", 
      "Byron C. Wallace"
    ], 
    "topics": [
      "Machine learning", 
      "Document classification", 
      "Natural language processing", 
      "Active learning (machine learning)", 
      "Test set", 
      "Tracer", 
      "Learning Disorders", 
      "Silo (dataset)", 
      "benefit", 
      "Artificial life"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1506", 
    "title": "Multi-source annotation projection of coreference chains: assessing strategies and testing opportunities", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we examine the possibility of using annotation projection from multiple sources for automatically obtaining coreference annotations in the target language. We implement a multi-source annotation projection algorithm and apply it on an English-German-Russian parallel corpus in order to transfer coreference chains from two sources to the target side. Operating in two settings \u2013 a low-resource and a more linguistically-informed one \u2013 we show that automatic coreference transfer could benefit from combining information from multiple languages, and assess the quality of both the extraction and the linking of target coreference mentions.", 
    "year": 2017, 
    "venue": "", 
    "references": 21, 
    "authors": [
      "Yulia Grishina", 
      "Manfred Stede"
    ], 
    "topics": [
      "Precision and recall", 
      "Numerous", 
      "Multi-source", 
      "Parallel text", 
      "Programming Languages", 
      "Phrases", 
      "Concatenation", 
      "Algorithm", 
      "Compiler", 
      "Body of uterus", 
      "Greater Than", 
      "Annotation", 
      "Exhibits as Topic", 
      "Projections and Predictions", 
      "algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2320", 
    "title": "MeSH-based dataset for measuring the relevance of text retrieval", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Creating simulated search environments has been of a significant interest in infor-mation retrieval, in both general and bio-medical search domains. Existing collec-tions include modest number of queries and are constructed by manually evaluat-ing retrieval results. In this work we pro-pose leveraging MeSH term assignments for creating synthetic test beds. We select a suitable subset of MeSH terms as queries, and utilize MeSH term assignments as pseudo-relevance rankings for retrieval evaluation. Using well studied retrieval functions, we show that their performance on the proposed data is consistent with similar findings in previous work. We further use the proposed retrieval evaluation framework to better understand how to combine heterogeneous sources of textual information.", 
    "year": 2018, 
    "venue": "BioNLP", 
    "references": 19, 
    "authors": [
      "W. Kim", 
      "Lana Yeganova", 
      "Donald C. Comeau", 
      "W. Wilbur", 
      "Zhiyong Lu"
    ], 
    "topics": [
      "Relevance", 
      "Document retrieval", 
      "PubMed", 
      "Information retrieval", 
      "Mesh networking", 
      "Digital curation", 
      "Synthetic data"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1281", 
    "title": "What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multi-hop textual question answering requires combining information from multiple sentences. We focus on a natural setting where, unlike typical reading comprehension, only partial information is provided with each question. The model must retrieve and use additional knowledge to correctly answer the question. To tackle this challenge, we develop a novel approach that explicitly identifies the knowledge gap between a key span in the provided knowledge and the answer choices. The model, GapQA, learns to fill this gap by determining the relationship between the span and an answer choice, based on retrieved knowledge targeting this gap. We propose jointly training a model to simultaneously fill this knowledge gap and compose it with the provided partial knowledge. On the OpenBookQA dataset, given partial knowledge, explicitly identifying what\u2019s missing substantially outperforms previous approaches.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 52, 
    "authors": [
      "Tushar Khot", 
      "Ashish Sabharwal", 
      "P. Clark"
    ], 
    "topics": [
      "Question answering", 
      "List comprehension", 
      "Open reading frame"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-4011", 
    "title": "JEDI: Joint Entity and Relation Detection using Type Inference", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "FREEBASE contains entities and relation information but is highly incomplete. Relevant information is ubiquitous in web text, but extraction deems challenging. We present JEDI, an automated system to jointly extract typed named entities and FREEBASE relations using dependency pattern from text. An innovative method for constraint solving on entity types of multiple relations is used to disambiguate pattern. The high precision in the evaluation supports our claim that we can detect entities and relations together, alleviating the need to train a custom classifier for an entity type1.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 23, 
    "authors": [
      "Johannes Kirschnick", 
      "Holmer Hemsen", 
      "V. Markl"
    ], 
    "topics": [
      "Type inference", 
      "Freebase", 
      "Constraint satisfaction problem", 
      "Precision and recall", 
      "Named entity", 
      "Binary pattern (image generation)", 
      "Apache UIMA", 
      "Jedi", 
      "Pipeline (computing)", 
      "Named-entity recognition", 
      "Entity Bean"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-3009", 
    "title": "Ckylark: A More Robust PCFG-LA Parser", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes Ckylark, a PCFG-LA style phrase structure parser that is more robust than other parsers in the genre. PCFG-LA parsers are known to achieve highly competitive performance, but sometimes the parsing process fails completely, and no parses can be generated. Ckylark introduces three new techniques that prevent possible causes for parsing failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 8, 
    "authors": [
      "Yusuke Oda", 
      "Graham Neubig", 
      "S. Sakti", 
      "T. Toda", 
      "Satoshi Nakamura"
    ], 
    "topics": [
      "Stochastic context-free grammar", 
      "Parser", 
      "Arithmetic underflow", 
      "C++", 
      "Programming language", 
      "Phrase structure rules", 
      "Smoothing", 
      "Open-source software", 
      "Lexicon", 
      "Image scaling", 
      "Hasta la vista, baby"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.58", 
    "title": "TIMBERT: Toponym Identifier For The Medical Domain Based on BERT", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose an approach to automate the process of place name detection in the medical domain to enable epidemiologists to better study and model the spread of viruses. We created a family of Toponym Identification Models based on BERT (TIMBERT), in order to learn in an end-to-end fashion the mapping from an input sentence to the associated sentence labeled with toponyms. When evaluated with the SemEval 2019 task 12 test set (Weissenbacher et al., 2019), our best TIMBERT model achieves an F1 score of 90.85%, a significant improvement compared to the state-of-the-art of 89.13% (Wang et al., 2019).", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 44, 
    "authors": [
      "M. Davari", 
      "Leila Kosseim", 
      "T. Bui"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/0891201054223986", 
    "title": "A General Technique to Train Language Models on Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We show that under certain conditions, a language model can be trained on the basis of a second language model. The main instance of the technique trains a finite automaton on the basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between grammar and trained automaton is provably minimal. This is a substantial generalization of an existing algorithm to train an n-gram model on the basis of a probabilistic context-free grammar.", 
    "year": 2005, 
    "venue": "Computational Linguistics", 
    "references": 34, 
    "authors": [
      "M. Nederhof"
    ], 
    "topics": [
      "Language model", 
      "Stochastic context-free grammar", 
      "Context-free language", 
      "N-gram", 
      "Automaton", 
      "Finite-state machine", 
      "Algorithm", 
      "Kullback\u2013Leibler divergence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981344.981366", 
    "title": "Aligning Sentences in Parallel Corpora", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried.", 
    "year": 1991, 
    "venue": "ACL", 
    "references": 13, 
    "authors": [
      "P. Brown", 
      "J. Lai", 
      "R. Mercer"
    ], 
    "topics": [
      "Text corpus", 
      "Parallel text", 
      "Align (company)", 
      "Computation", 
      "Offset binary"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-5225", 
    "title": "PLN-PUCRS at EmoInt-2017: Psycholinguistic features for emotion intensity prediction in tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Linguistic Inquiry and Word Count (LIWC) is a rich dictionary that map words into several psychological categories such as Affective, Social, Cognitive, Perceptual and Biological processes. In this work, we have used LIWC psycholinguistic categories to train regression models and predict emotion intensity in tweets for the EmoInt-2017 task. Results show that LIWC features may boost emotion intensity prediction on the basis of a low dimension set.", 
    "year": 2017, 
    "venue": "WASSA@EMNLP", 
    "references": 21, 
    "authors": [
      "H. D. P. D. Santos", 
      "Renata Vieira"
    ], 
    "topics": [
      "Dictionary", 
      "Probabilistic logic network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.375", 
    "title": "Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models' syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision (Dyer et al.,2016; Charniak et al., 2016). We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 30, 
    "authors": [
      "Ethan Gotlieb Wilcox", 
      "Peng Qian", 
      "Richard Futrell", 
      "Ryosuke Kohita", 
      "R. Levy", 
      "Miguel Ballesteros"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3621", 
    "title": "TECHLIMED system description for the Shared Task on Automatic Arabic Error Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article is a system description paper and reports on the participation of Techlimed in the \u201dQALB-2014 shared task\u201d on evaluation of automatic arabic error correction systems organized in conjunction with the EMNLP 2014 Workshop on Arabic Natural Language Processing. Correcting automatically texts in Arabic is a challenging task due to the complexity and rich morphology of the Arabic language and the lack of appropriate resources, (e.g. publicly available corpora and tools). To develop our systems, we considered several approaches from rule based systems to statistical methods. Our results on the development set show that the statistical system outperforms the lexicon driven approach with a precision of 71%, a recall of 50% and a F-measure of 59%.", 
    "year": 2014, 
    "venue": "ANLP@EMNLP", 
    "references": 13, 
    "authors": [
      "D. Mostefa", 
      "Omar Asbayou", 
      "Ramzi Abb\u00e8s"
    ], 
    "topics": [
      "Error detection and correction", 
      "Lexicon", 
      "Text corpus", 
      "Empirical Methods in Natural Language Processing", 
      "Complexity", 
      "QR code", 
      "Rule-based system", 
      "F1 score", 
      "Galaxy morphological classification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1221", 
    "title": "CLUZH at VarDial GDI 2017: Testing a Variety of Machine Learning Tools for the Classification of Swiss German Dialects", 
    "fields_of_study": [
      "Computer Science", 
      "Engineering"
    ], 
    "abstract": "Our submissions for the GDI 2017 Shared Task are the results from three different types of classifiers: Na\u00efve Bayes, Conditional Random Fields (CRF), and Support Vector Machine (SVM). Our CRF-based run achieves a weighted F1 score of 65% (third rank) being beaten by the best system by 0.9%. Measured by classification accuracy, our ensemble run (Na\u00efve Bayes, CRF, SVM) reaches 67% (second rank) being 1% lower than the best system. We also describe our experiments with Recurrent Neural Network (RNN) architectures. Since they performed worse than our non-neural approaches we did not include them in the submission.", 
    "year": 2017, 
    "venue": "VarDial", 
    "references": 27, 
    "authors": [
      "S. Clematide", 
      "Peter Makarov"
    ], 
    "topics": [
      "Graphics Device Interface", 
      "F1 score", 
      "Support vector machine", 
      "Recurrent neural network", 
      "Test set", 
      "Conditional random field", 
      "Machine learning", 
      "Naive Bayes classifier", 
      "ARHGDIB protein, human", 
      "Experiment", 
      "N-gram", 
      "Baseline (configuration management)", 
      "Switzerland", 
      "Random neural network", 
      "GDP-dissociation inhibitor activity", 
      "Bayes Theorem", 
      "Regulatory Submission", 
      "Architecture as Topic"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-2603", 
    "title": "A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Reading Comprehension (RC) of text is one of the fundamental tasks in natural language processing. In recent years, several end-to-end neural network models have been proposed to solve RC tasks. However, most of these models suffer in reasoning over long documents. In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension. We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA. We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.", 
    "year": 2018, 
    "venue": "QA@ACL", 
    "references": 32, 
    "authors": [
      "Seunghak Yu", 
      "S. Indurthi", 
      "Seohyun Back", 
      "Haejun Lee"
    ], 
    "topics": [
      "List comprehension", 
      "Natural language processing", 
      "Natural language understanding", 
      "Artificial neural network", 
      "Network model", 
      "End-to-end principle", 
      "Experiment", 
      "Benchmark (computing)", 
      "Scalability", 
      "Software quality assurance", 
      "Squad"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.146", 
    "title": "Documents Representation via Generalized Coupled Tensor Chain with the Rotation Group constraint", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Continuous representations of linguistic structures are an important part of modern natural language processing systems. Despite the diversity, most of the existing log-multilinear embedding models are organized under vector operations. However, these operations can not precisely represent the compositionality of natural language due to a lack of order-preserving properties. In this work, we focus on one of the promising alternatives based on the embedding of documents and words in the rotation group through the generalization of the coupled tensor chain decomposition to the exponential family of the probability distributions. In this model, documents and words are represented as matrices, and n-grams representations are combined from word representations by matrix multiplication. The proposed model is optimized via noise-contrastive estimation. We show empirically that capturing word order and higher-order word interactions allows our model to achieve the best results in several document classification benchmarks.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 52, 
    "authors": [
      "I. Vorona", 
      "A. Phan", 
      "A. Panchenko", 
      "A. Cichocki"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1284", 
    "title": "Interpretable Neural Predictions with Differentiable Binary Variables", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification\u2013a rationale\u2013for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 44, 
    "authors": [
      "Jasmijn Bastings", 
      "W. Aziz", 
      "Ivan Titov"
    ], 
    "topics": [
      "Design rationale", 
      "Selection (user interface)", 
      "Information", 
      "Gradient", 
      "Sparse matrix", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1153", 
    "title": "Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language specific resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, outperform orthographic representations and incorporating both attention and phonological features improves statistical efficiency of the model in 0-shot and low data transfer settings with no task specific feature engineering in the source or target language.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 43, 
    "authors": [
      "Akash Bharadwaj", 
      "David R. Mortensen", 
      "Chris Dyer", 
      "J. Carbonell"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Feature engineering", 
      "Orthographic projection", 
      "Information extraction", 
      "Text corpus", 
      "I2O", 
      "Word embedding", 
      "Compiler", 
      "Authorization", 
      "One-to-one (data model)", 
      "Programming language", 
      "Galaxy morphological classification", 
      "Spatial variability", 
      "Emergent"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-1015", 
    "title": "Neural User Factor Adaptation for Text Classification: Learning to Generalize Across Author Demographics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Language use varies across different demographic factors, such as gender, age, and geographic location. However, most existing document classification methods ignore demographic variability. In this study, we examine empirically how text data can vary across four demographic factors: gender, age, country, and region. We propose a multitask neural model to account for demographic variations via adversarial training. In experiments on four English-language social media datasets, we find that classification performance improves when adapting for user factors.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 47, 
    "authors": [
      "Xiaolei Huang", 
      "Michael J. Paul"
    ], 
    "topics": [
      "Document classification", 
      "Facial recognition system", 
      "Domain adaptation", 
      "Social media", 
      "Geographic coordinate system", 
      "Computer multitasking", 
      "Experiment", 
      "Text corpus", 
      "Weitao Yang", 
      "Application programming interface", 
      "Heart rate variability"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-1033", 
    "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This work was sponsored in part by the U. S. Army Research Laboratory and the U. S. Army Research Office/nunder contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319./nMiguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 63, 
    "authors": [
      "Chris Dyer", 
      "Miguel Ballesteros", 
      "Wang Ling", 
      "A. Matthews", 
      "Noah A. Smith"
    ], 
    "topics": [
      "Parsing", 
      "Backpropagation", 
      "Long short-term memory", 
      "Control flow", 
      "Data structure", 
      "Time complexity", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4014", 
    "title": "Transformation and Decomposition for Efficiently Implementing and Improving Dependency-to-String Model In Moses", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Dependency structure provides grammatical relations between words, which have shown to be effective in Statistical Machine Translation (SMT). In this paper, we present an open source module in Moses which implements a dependency-to-string model. We propose a method to transform the input dependency tree into a corresponding constituent tree for reusing the tree-based decoder in Moses. In our experiments, this method achieves comparable results with the standard model. Furthermore, we enrich this model via the decomposition of dependency structure, including extracting rules from the substructures of the dependency tree during training and creating a pseudo-forest instead of the tree per se as the input during decoding. Large-scale experiments on Chinese\u2010English and German\u2010English tasks show that the decomposition approach improves the baseline dependencyto-string model significantly. Our system achieves comparable results with the state-of-the-art hierarchical phrase-based model (HPB). Finally, when resorting to phrasal rules, the dependency-to-string model performs significantly better than Moses HPB.", 
    "year": 2014, 
    "venue": "SSST@EMNLP", 
    "references": 33, 
    "authors": [
      "Liangyou Li", 
      "Jun Xie", 
      "Andy Way", 
      "Qun Liu"
    ], 
    "topics": [
      "Moses", 
      "Statistical machine translation", 
      "Experiment", 
      "Open-source software", 
      "Baseline (configuration management)", 
      "Java", 
      "Formal language", 
      "Dependency grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-2807", 
    "title": "Scrutable Feature Sets for Stance Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes and evaluates a novel feature set for stance classification of argumentative texts; i.e. deciding whether a post by a user is for or against the issue being debated. We model the debate both as attitude bearing features, including a set of automatically acquired \u2018topic terms\u2019 associated with a Distributional Lexical Model (DLM) that captures the writer\u2019s attitude towards the topic term, and as dependency features that represent the points being made in the debate. The stance of the text towards the issue being debated is then learnt in a supervised framework as a function of these features. The main advantage of our feature set is that it is scrutable: The reasons for a classification can be explained to a human user in natural language. We also report that our method outperforms previous approaches to stance classification as well as a range of baselines based on sentiment analysis and topic-sentiment analysis.", 
    "year": 2016, 
    "venue": "ArgMining@ACL", 
    "references": 18, 
    "authors": [
      "Angrosh Mandya", 
      "Advaith Siddharthan", 
      "A. Wyner"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Natural language", 
      "Statistical classification", 
      "Disk staging"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974557.974603", 
    "title": "Fast Statistical Parsing of Noun Phrases for Document Indexing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new Probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.", 
    "year": 1997, 
    "venue": "ANLP", 
    "references": 84, 
    "authors": [
      "ChengXiang Zhai"
    ], 
    "topics": [
      "Information retrieval", 
      "Statistical parsing", 
      "Natural language processing", 
      "Megabyte", 
      "Query expansion", 
      "Statistical model", 
      "Archive", 
      "The Wall Street Journal", 
      "Experiment", 
      "Bigram", 
      "Relevance", 
      "Hoc (programming language)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5612", 
    "title": "Automatically Detecting the Position and Type of Psychiatric Evaluation Report Sections", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Psychiatric evaluation reports represent a rich and still mostly-untapped source of information for developing systems for automatic diagnosis and treatment of mental health problems. These reports contain free-text structured within sections using a convention of headings. We present a model for automatically detecting the position and type of different psychiatric evaluation report sections. We developed this model using a corpus of 150 sample reports that we gathered from the Web, and used sentences as a processing unit while section headings were used as labels of section type. From these labels we generated a unified hierarchy of labels of section types, and then learned n-gram models of the language found in each section. To model conventions for section order, we integrated these n-gram models with a Hierarchical Hidden Markov Model (HHMM) representing the probabilities of observed section orders found in the corpus, and then used this HHMM n-gram model in a decoding framework to infer the most likely section boundaries and section types for documents with their section labels removed. We evaluated our model over two tasks, namely, identifying section boundaries and identifying section types and orders. Our model significantly outperformed baselines for each task with an F1 of 0.88 for identifying section types, and a 0.26 WindowDiff (Wd) and 0.20 and (Pk) scores, respectively, for identifying section boundaries.", 
    "year": 2018, 
    "venue": "Louhi@EMNLP", 
    "references": 39, 
    "authors": [
      "Deya Banisakher", 
      "N. Rishe", 
      "Mark A. Finlayson"
    ], 
    "topics": [
      "Hierarchical hidden Markov model", 
      "N-gram", 
      "Sensor", 
      "Text corpus", 
      "Information source", 
      "World Wide Web", 
      "Markov chain", 
      "Mathematical model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-3020", 
    "title": "Journalist-in-the-Loop: Continuous Learning as a Service for Rumour Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatically identifying rumours in social media and assessing their veracity is an important task with downstream applications in journalism. A significant challenge is how to keep rumour analysis tools up-to-date as new information becomes available for particular rumours that spread in a social network. This paper presents a novel open-source web-based rumour analysis tool that can continuous learn from journalists. The system features a rumour annotation service that allows journalists to easily provide feedback for a given social media post through a web-based interface. The feedback allows the system to improve an underlying state-of-the-art neural network-based rumour classification model. The system can be easily integrated as a service into existing tools and platforms used by journalists using a REST API.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 22, 
    "authors": [
      "T. Karmakharm", 
      "Kalina Bontcheva", 
      "Nikolaos Aletras"
    ], 
    "topics": [
      "Social media", 
      "4chan", 
      "Deep learning", 
      "Social network", 
      "Open-source software", 
      "Web service", 
      "Prototype", 
      "Artificial neural network", 
      "Web application", 
      "Downstream (software development)", 
      "Veracity", 
      "Application programming interface", 
      "Statistical classification", 
      "Dialed Number Identification Service"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.171", 
    "title": "StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 59, 
    "authors": [
      "Yiwei Lyu", 
      "Paul Pu Liang", 
      "H. Pham", 
      "E. Hovy", 
      "Barnab'as P'oczos", 
      "R. Salakhutdinov", 
      "Louis-Philippe Morency"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1077", 
    "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 47, 
    "authors": [
      "Shijie Wu", 
      "Mark Dredze"
    ], 
    "topics": [
      "Document classification", 
      "Parsing", 
      "Part-of-speech tagging", 
      "Native-language identification", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.566", 
    "title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the first large-scale meta-evaluation of machine translation (MT). We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than BLEU have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 20, 
    "authors": [
      "Benjamin Marie", 
      "Atsushi Fujita", 
      "Rapha\u00ebl Rubino"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.296", 
    "title": "Verbal Multiword Expression Identification: Do We Need a Sledgehammer to Crack a Nut?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Automatic identification of multiword expressions (MWEs), like \u2018to cut corners\u2019 (to do an incomplete job), is a pre-requisite for semantically-oriented downstream applications. This task is challenging because MWEs, especially verbal ones (VMWEs), exhibit surface variability. This paper deals with a subproblem of VMWE identification: the identification of occurrences of previously seen VMWEs. A simple language-independent system based on a combination of filters competes with the best systems from a recent shared task: it obtains the best averaged F-score over 11 languages (0.6653) and even the best score for both seen and unseen VMWEs due to the high proportion of seen VMWEs in texts. This highlights the fact that focusing on the identification of seen VMWEs could be a strategy to improve VMWE identification in general.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 28, 
    "authors": [
      "C. Pasquer", 
      "Agata Savary", 
      "Carlos Ramisch", 
      "Jean-Yves Antoine"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1804", 
    "title": "Universal Dependencies to Logical Forms with Negation Scope", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using Universal Dependencies. To do so, we enhance UDepLambda, a framework that converts dependency graphs to logical forms. The resulting UDepLambda$\\lnot$ is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.", 
    "year": 2017, 
    "venue": "ArXiv", 
    "references": 44, 
    "authors": [
      "Federico Fancellu", 
      "Siva Reddy", 
      "Adam Lopez", 
      "B. Webber"
    ], 
    "topics": [
      "Universal quantification", 
      "Type theory", 
      "First-order logic", 
      "Language technology", 
      "First-order predicate"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073083.1073125", 
    "title": "The Descent of Hierarchy, and Selection in Relational Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds.", 
    "year": 2002, 
    "venue": "ACL", 
    "references": 39, 
    "authors": [
      "Barbara Rosario", 
      "Marti A. Hearst", 
      "C. Fillmore"
    ], 
    "topics": [
      "Kripke semantics", 
      "Natural language understanding", 
      "Ontology components", 
      "Descent", 
      "Algorithm", 
      "Entity", 
      "Embedded system"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-042-7_006", 
    "title": "Translation Memory Systems Have a Long Way to Go", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The TM memory systems changed the work of translators and now the translators not benefiting from these tools are a tiny minority. These tools operate on fuzzy (surface) matching mostly and cannot benefit from already translated texts which are synonymous to (or paraphrased versions of) the text to be translated. The match score is mostly based on character-string similarity, calculated through Levenshtein distance. The TM tools have difficulties with detecting similarities even in sentences which represent a minor revision of sentences already available in the translation memory. This shortcoming of the current TM systems was the subject of the present study and was empirically proven in the experiments we conducted. To this end, we compiled a small translation memory (English-Spanish) and applied several lexical and syntactic transformation rules to the source sentences with both English and Spanish being the source language. The results of this study show that current TM systems have a long way to go and highlight the need for TM systems equipped with NLP capabilities which will offer the translator the advantage of he/she not having to translate a sentence again if an almost identical sentence has already been already translated.", 
    "year": 2017, 
    "venue": "", 
    "references": 26, 
    "authors": [
      "Andrea Silvestre Baquero", 
      "R. Mitkov"
    ], 
    "topics": [
      "Translation memory", 
      "Way to Go", 
      "Levenshtein distance", 
      "Natural language processing", 
      "Parsing", 
      "sentence", 
      "Device Translator Device Component", 
      "String metric", 
      "Experiment", 
      "Sensor", 
      "Compiler", 
      "NINL gene", 
      "benefit", 
      "Rule (guideline)", 
      "Version", 
      "Genetic Translation Process", 
      "Surgical revision"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-056-4_068", 
    "title": "Study on Unsupervised Statistical Machine Translation for Backtranslation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Machine Translation systems have drastically improved over the years for several language pairs. Monolingual data is often used to generate synthetic sentences to augment the training data which has shown to improve the performance of machine translation models. In our paper, we make use of an Unsupervised Statistical Machine Translation (USMT) to generate synthetic sentences. Our study compares the performance improvements in Neural Machine Translation model when using synthetic sentences from supervised and unsupervised Machine Translation models. Our approach of using USMT for backtranslation shows promise in low resource conditions and achieves an improvement of 3.2 BLEU score over the Neural Machine Translation model.", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 0, 
    "authors": [
      "Anush Kumar", 
      "Nihal V. Nayak", 
      "Aditya Chandra", 
      "Mydhili K. Nair"
    ], 
    "topics": [
      "Statistical machine translation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/w14-03", 
    "title": "Proceedings of the EACL 2014 Workshop on Humans and Computer-assisted Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2014, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Philipp Koehn", 
      "Ulrich Germann"
    ], 
    "topics": [
      "Humans", 
      "Computer-assisted translation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3541", 
    "title": "Neural Response Generation for Customer Service based on Personality Traits", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a neural response generation model that generates responses conditioned on a target personality. The model learns high level features based on the target personality, and uses them to update its hidden state. Our model achieves performance improvements in both perplexity and BLEU scores over a baseline sequence-to-sequence model, and is validated by human judges.", 
    "year": 2017, 
    "venue": "INLG", 
    "references": 20, 
    "authors": [
      "Jonathan Herzig", 
      "Michal Shmueli-Scheuer", 
      "T. Sandbank", 
      "D. Konopnicki"
    ], 
    "topics": [
      "Perplexity", 
      "Baseline (configuration management)", 
      "Social media", 
      "BLEU", 
      "Experiment", 
      "High-level programming language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00112", 
    "title": "Empirical Methods for the Study of Denotation in Nominalizations in Spanish", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This article deals with deverbal nominalizations in Spanish; concretely, we focus on the denotative distinction between event and result nominalizations. The goals of this work is twofold: first, to detect the most relevant features for this denotative distinction; and, second, to build an automatic classification system of deverbal nominalizations according to their denotation. We have based our study on theoretical hypotheses dealing with this semantic distinction and we have analyzed them empirically by means of Machine Learning techniques which are the basis of the ADN-Classifier. This is the first tool that aims to automatically classify deverbal nominalizations in event, result, or underspecified denotation types in Spanish. The ADN-Classifier has helped us to quantitatively evaluate the validity of our claims regarding deverbal nominalizations. We set up a series of experiments in order to test the ADN-Classifier with different models and in different realistic scenarios depending on the knowledge resources and natural language processors available. The ADN-Classifier achieved good results (87.20% accuracy).", 
    "year": 2012, 
    "venue": "Computational Linguistics", 
    "references": 84, 
    "authors": [
      "A. Peris", 
      "M. Taul\u00e9", 
      "H. Rodr\u00edguez"
    ], 
    "topics": [
      "Lexicon", 
      "Application delivery network", 
      "Binary classification", 
      "Natural language processing", 
      "Text corpus", 
      "Backoff", 
      "Baseline (configuration management)", 
      "Experiment", 
      "Neural coding", 
      "Central processing unit", 
      "Error analysis (mathematics)", 
      "Statistical classification", 
      "NL (complexity)", 
      "Machine learning", 
      "Molecular dynamics", 
      "Result type", 
      "Word sense", 
      "Relevance", 
      "Sensor", 
      "Sparse matrix", 
      "Denotational semantics", 
      "Deductive classifier", 
      "Web Services for Devices"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4517", 
    "title": "Ranking Passages for Argument Convincingness", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models. We examine this in a case study on ranking text passages for argument convincingness. Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic. Using data from a deployed system within the Bing search engine, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources. We detail the process of extracting topical passages for queries submitted to a search engine, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation. Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages. Our results show pairwise training outperforms training that regresses to a target score for each passage. Our results also show a simple \u2018win-rate\u2019 score is a better regression target than the previously proposed page-rank target. Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples.", 
    "year": 2019, 
    "venue": "ArgMining@ACL", 
    "references": 37, 
    "authors": [
      "P. Potash", 
      "Adam Ferguson", 
      "Timothy J. Hazen"
    ], 
    "topics": [
      "Web search engine", 
      "Cross entropy", 
      "Crowdsourcing", 
      "Vertex-transitive graph", 
      "PageRank", 
      "Ranking (information retrieval)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1042", 
    "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an \"explanation\" consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 27, 
    "authors": [
      "David Alvarez-Melis", 
      "T. Jaakkola"
    ], 
    "topics": [
      "Black box", 
      "Autoencoder", 
      "Causal filter", 
      "Partition problem", 
      "Variational principle", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.371", 
    "title": "Reference Language based Unsupervised Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel corpus. The rise of unsupervised neural machine translation (UNMT) almost completely relieves the parallel corpus curse, though UNMT is still subject to unsatisfactory performance due to the vagueness of the clues available for its core back-translation training. Further enriching the idea of pivot translation by extending the use of parallel corpora beyond the source-target paradigm, we propose a new reference language-based framework for UNMT, RUNMT, in which the reference language only shares a parallel corpus with the source, but this corpus still indicates a signal clear enough to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline that uses only one auxiliary language, demonstrating the usefulness of the proposed reference language-based UNMT and establishing a good start for the community.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 53, 
    "authors": [
      "Z. Li", 
      "Hai Zhao", 
      "Rui Wang", 
      "M. Utiyama", 
      "E. Sumita"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1115", 
    "title": "Vancouver Welcomes You! Minimalist Location Metonymy Resolution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 39, 
    "authors": [
      "Milan Gritta", 
      "Mohammad Taher Pilehvar", 
      "Nut Limsopatham", 
      "Nigel Collier"
    ], 
    "topics": [
      "SemEval", 
      "Wikipedia", 
      "Named-entity recognition", 
      "Parsing", 
      "Entity", 
      "Dictionary attack", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.bionlp-1.8", 
    "title": "Experimental Evaluation and Development of a Silver-Standard for the MIMIC-III Clinical Coding Dataset", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Clinical coding is currently a labour-intensive, error-prone, but a critical administrative process whereby hospital patient episodes are manually assigned codes by qualified staff from large, standardised taxonomic hierarchies of codes. Automating clinical coding has a long history in NLP research and has recently seen novel developments setting new benchmark results. A popular dataset used in this task is MIMIC-III, a large database of clinical free text notes and their associated codes amongst other data. We argue for the reconsideration of the validity MIMIC-III\u2019s assigned codes, as MIMIC-III has not undergone secondary validation. This work presents an open-source, reproducible experimental methodology for assessing the validity of EHR discharge summaries. We exemplify the methodology with MIMIC-III discharge summaries and show the most frequently assigned codes in MIMIC-III are undercoded up to 35%.", 
    "year": 2020, 
    "venue": "BIONLP", 
    "references": 27, 
    "authors": [
      "T. Searle", 
      "Zina M. Ibrahim", 
      "R. Dobson"
    ], 
    "topics": [
      "MIMIC", 
      "Code", 
      "Discharger", 
      "Open-source software", 
      "Cognitive dimensions of notations", 
      "Exemplification", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981574.981603", 
    "title": "F-PATR: A Functional Constraints for Unification-Based Grammars", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Motivation for including relational constraints other than equality within grammatical formalisms has come from discontinuous constituency and partially free word order for natural languages as well as from the need to define combinatory operations at the most basic level for languages with a two-dimensional syntax (e.g., mathematical notation, chemical equations, and various diagramming languages). This paper presents F-PATR, a generalization of the PATR-II unification-based formalism, which incorporates relational constraints expressed as user-defined functions. An operational semantics is given for unification that is an adaptation and extension of the approach taken by Ait-Kaci and Nasr (1989). It is designed particularly for unification-based formalisms implemented in functional programming environments such as Lisp. The application of unification in a chart parser for relational set languages is discussed briefly.", 
    "year": 1993, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "K. Wittenburg"
    ], 
    "topics": [
      "PATR-II", 
      "Unification (computer science)", 
      "Chart parser", 
      "Operational semantics", 
      "Functional programming", 
      "Natural language", 
      "Lisp", 
      "List of concept- and mind-mapping software", 
      "Semantics (computer science)", 
      "Han unification"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2020.acl-srw.3", 
    "title": "Unsupervised Paraphasia Classification in Aphasic Speech", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Aphasia is a speech and language disorder which results from brain damage, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia). Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods cant be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel unsupervised method which can be implemented without the need for labeled paraphasia data. Our evaluations show that our method outperforms previous work based on supervised learning and transfer learning approaches for English. We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from aphasia in any language.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Sharan Pai", 
      "Nikhil Sachdeva", 
      "Prince Sachdeva", 
      "R. Shah"
    ], 
    "topics": [
      "Supervised learning", 
      "Unsupervised learning", 
      "Advanced Audio Coding"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-2039", 
    "title": "A Study of the Importance of External Knowledge in the Named Entity Recognition Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this work, we discuss the importance of external knowledge for performing Named Entity Recognition (NER). We present a novel modular framework that divides the knowledge into four categories according to the depth of knowledge they convey. Each category consists of a set of features automatically generated from different information sources, such as a knowledge-base, a list of names, or document-specific semantic annotations. Further, we show the effects on performance when incrementally adding deeper knowledge and discuss effectiveness/efficiency trade-offs.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 22, 
    "authors": [
      "Dominic Seyler", 
      "Tatiana Dembelova", 
      "Luciano Del Corro", 
      "Johannes Hoffart", 
      "G. Weikum"
    ], 
    "topics": [
      "Named entity", 
      "Kilobyte", 
      "Knowledge-based systems", 
      "Experiment", 
      "Category utility", 
      "Name"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.128", 
    "title": "BennettNLP at SemEval-2021 Task 5: Toxic Spans Detection using Stacked Embedding Powered Toxic Entity Recognizer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With the rapid growth in technology, social media activity has seen a boom across all age groups. It is humanly impossible to check all the tweets, comments and status manually whether they follow proper community guidelines. A lot of toxicity is regularly posted on these social media platforms. This research aims to find toxic words in a sentence so that a healthy social community is built across the globe and the users receive censored content with specific warnings and facts. To solve this challenging problem, authors have combined concepts of Linked List for pre-processing and then used the idea of stacked embeddings like BERT Embeddings, Flair Embeddings and Word2Vec on the flairNLP framework to get the desired results. F1 metric was used to evaluate the model. The authors were able to produce a 0.74 F1 score on their test set.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 19, 
    "authors": [
      "H. Kataria", 
      "Ambuje Gupta", 
      "V. Mishra"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2120", 
    "title": "Recognizing Implied Predicate-Argument Relationships in Textual Inference", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure. While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. Such scenarios provide prior information, which substantially eases the task. We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations. 1 Motivation and Task", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Asher Stern", 
      "Ido Dagan"
    ], 
    "topics": [
      "Semantic role labeling"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00098", 
    "title": "Cross-Genre and Cross-Domain Detection of Semantic Uncertainty", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Uncertainty is an important linguistic phenomenon that is relevant in various Natural Language Processing applications, in diverse genres from medical to community generated, newswire or scientific discourse, and domains from science to humanities. The semantic uncertainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical cues) and the key steps of uncertainty detection in an application include the steps of locating the (genre- and domain-specific) lexical cues, disambiguating them, and linking them with the units of interest for the particular application (e.g., identified events in information extraction). In this study, we focus on the genre and domain differences of the context-dependent semantic uncertainty cue recognition task.We introduce a unified subcategorization of semantic uncertainty as different domain applications can apply different uncertainty categories. Based on this categorization, we normalized the annotation of three corpora and present results with a state-of-the-art uncertainty cue recognition model for four fine-grained categories of semantic uncertainty.Our results reveal the domain and genre dependence of the problem; nevertheless, we also show that even a distant source domain data set can contribute to the recognition and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to cover a new domain. Thus, the unified subcategorization and domain adaptation for training the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty recognition.", 
    "year": 2012, 
    "venue": "CL", 
    "references": 72, 
    "authors": [
      "Gy\u00f6rgy Szarvas", 
      "V. Vincze", 
      "Rich\u00e1rd Farkas", 
      "G. M\u00f3ra", 
      "Iryna Gurevych"
    ], 
    "topics": [
      "Information extraction", 
      "Domain adaptation", 
      "Natural language processing", 
      "Context-sensitive language", 
      "Categorization", 
      "Word-sense disambiguation", 
      "Dictionary", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1102", 
    "title": "Joint Mention Extraction and Classification with Mention Hypergraphs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a novel model for the task of joint mention extraction and classification. Unlike existing approaches, our model is able to effectively capture overlapping mentions with unbounded lengths. The model is highly scalable, with a time complexity that is linear in the number of words in the input sentence and linear in the number of possible mention classes. Our model can be extended to additionally capture mention heads explicitly in a joint manner under the same time complexity. We demonstrate the effectiveness of our model through extensive experiments on standard datasets.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Wei Lu", 
      "D. Roth"
    ], 
    "topics": [
      "Time complexity", 
      "Relationship extraction", 
      "Global optimization", 
      "Information extraction", 
      "Experiment", 
      "Scalability", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.3115/1075218.1075282", 
    "title": "Mapping WordNets Using Structural Information", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a robust approach for linking already existing lexical/semantic hierarchies. We used a constraint satisfaction algorithm (relaxation labeling) to select - among a set of candidates- the node in a target taxonomy that bests matches each node in a source taxonomy. In particular, we use it to map the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision and a very low remaining ambiguity.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 25, 
    "authors": [
      "J. Daud\u00e9", 
      "Llu\u00eds Padr\u00f3", 
      "German Rigau"
    ], 
    "topics": [
      "WordNet", 
      "Constraint satisfaction", 
      "Algorithm", 
      "Linear programming relaxation", 
      "Taxonomy", 
      "Node - plant part"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.283", 
    "title": "HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Open-domain dialog systems have a usercentric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an annotation efficient framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to automatically label training samples. We then denoise the weakly labeled data using Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora. Our implementation is available at https://github.com/Weixin-Liang/ HERALD/.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 57, 
    "authors": [
      "Weixin Liang", 
      "Kai-Hui Liang", 
      "Zhou Yu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_070", 
    "title": "Classifying Frames at the Sentence Level in News Articles", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Previous approaches to generic frame classification analyze frames at the document level. Here, we propose a supervised based approach based on deep neural networks and distributional representations for classifying frames at the sentence level in news articles. We conduct our experiments on the publicly available Media Frames Corpus compiled from the U.S. Newspapers. Using (B)LSTMs and GRU networks to represent the meaning of frames, we demonstrate that our approach yields at least 14-point improvement over several baseline methods.", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 26, 
    "authors": [
      "Nona Naderi", 
      "Graeme Hirst"
    ], 
    "topics": [
      "The Sentence", 
      "Baseline (configuration management)", 
      "Artificial neural network", 
      "Supervised learning", 
      "Deep learning", 
      "Experiment", 
      "Recurrent neural network", 
      "Compiler", 
      "Frame language", 
      "Jumbo frame", 
      "Streaming media"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119018.1119081", 
    "title": "Approaches in MET (Multi-Lingual Entity Task)", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "BBN and FinCEN participated jointly in the Spanish language task for MET. BBN also participated in Chinese. We also fielded two approaches. The first approach is pattern based and has an architecture as shown in Figure 1. This approach was applied to both Chinese and Spanish. The algorithms (rectangles in the Figure) were used in the two languages; the only component difference was the New Mexico State University segmenter, used to find the word boundaries in Chinese. The components common to both languages are the message reader, which dealt with the input format and SGML conventions via a declarative format description; the part-of-speech tagger (BBN POST); a lexical pattern matcher driven by knowledge bases of patterns and lexicons specific to each language; and the SGML annotation generator. While not shown in Figure 1, an alias prediction algorithm was shared by both languages, using patterns unique to each language.", 
    "year": 1996, 
    "venue": "TIPSTER", 
    "references": 0, 
    "authors": [
      "Damaris M. Ayuso", 
      "D. Bikel", 
      "Tasha Hall", 
      "Erik Peterson", 
      "R. Weischedel", 
      "Patrick Jost"
    ], 
    "topics": [
      "Standard Generalized Markup Language", 
      "Part-of-speech tagging", 
      "Algorithm", 
      "Lexicon", 
      "Brill tagger"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.462", 
    "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield the same predictions as greedy decoding but with a significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL14 and 72.9 F0.5 in the BEA-19 test set with an almost 10\u00d7 online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/ Shallow-Aggressive-Decoding.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 37, 
    "authors": [
      "Xin Sun", 
      "Tao Ge", 
      "Furu Wei", 
      "Houfeng Wang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.205", 
    "title": "Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy\u2019s layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 31, 
    "authors": [
      "Kervy Rivas Rojas", 
      "Gina Bustamante", 
      "Marco Antonio Sobrevilla Cabezudo", 
      "Arturo Oncevay"
    ], 
    "topics": [
      "Document classification", 
      "Beam search", 
      "DBpedia", 
      "Neural machine translation", 
      "Artificial neural network", 
      "Class hierarchy", 
      "Word embedding", 
      "Dictionary", 
      "Tree (data structure)", 
      "Baseline (configuration management)", 
      "Web of Science", 
      "Bottom-up parsing", 
      "Synthetic intelligence", 
      "Graphics processing unit", 
      "Titan"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1442", 
    "title": "Neural Gaussian Copula for Variational Autoencoder", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 49, 
    "authors": [
      "Prince Zizhuang Wang", 
      "William Yang Wang"
    ], 
    "topics": [
      "Autoencoder", 
      "Variational principle", 
      "Latent variable", 
      "Language model", 
      "Calculus of variations", 
      "Approximation algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.261", 
    "title": "Towards Interpreting BERT for Reading Comprehension Based QA", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at this https URL .", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 39, 
    "authors": [
      "Sahana Ramnath", 
      "Preksha Nema", 
      "Deep Sahni", 
      "Mitesh M. Khapra"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1510", 
    "title": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE\u2014a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 29, 
    "authors": [
      "Nicky Ringland", 
      "Xiang Dai", 
      "Ben Hachey", 
      "Sarvnaz Karimi", 
      "C\u00e9cile Paris", 
      "J. Curran"
    ], 
    "topics": [
      "Named entity", 
      "Natural language processing", 
      "Treebank", 
      "The Wall Street Journal", 
      "Downstream (software development)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.133", 
    "title": "Learning Slice-Aware Representations with Mixture of Attentions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Real-world machine learning systems are achieving remarkable performance in terms of coarse-grained metrics like overall accuracy and F-1 score. However, model improvement and development often require fine-grained modeling on individual data subsets or slices, for instance, the data slices where the models have unsatisfactory results. In practice, it gives tangible values for developing such models that can pay extra attention to critical or interested slices while retaining the original overall performance. This work extends the recent slice-based learning (SBL) (Chen et al., 2019) with a mixture of attentions (MoA) to learn slice-aware dual attentive representations. We empirically show that the MoA approach outperforms the baseline method as well as the original SBL approach on monitored slices with two natural language understanding (NLU) tasks.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 28, 
    "authors": [
      "Cheng Wang", 
      "Sungjin Lee", 
      "Sunghyun Park", 
      "Hang Li", 
      "Young-Bum Kim", 
      "R. Sarikaya"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4119", 
    "title": "Reconstruction of Word Embeddings from Sub-Word Parameters", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size. We propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Our approach is quite simple: before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures. We report interesting results on a variety of tasks: word similarity, word analogy, and part-of-speech tagging.", 
    "year": 2017, 
    "venue": "SWCN@EMNLP", 
    "references": 35, 
    "authors": [
      "K. Stratos"
    ], 
    "topics": [
      "Word embedding", 
      "Microsoft Word for Mac", 
      "Part-of-speech tagging"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.spnlp-1.9", 
    "title": "Reading the Manual: Event Extraction as Definition Comprehension", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, \u201cSome person was born in some location at some time.\u201d We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.", 
    "year": 2020, 
    "venue": "SPNLP", 
    "references": 38, 
    "authors": [
      "Yunmo Chen", 
      "Tongfei Chen", 
      "Seth Ebner", 
      "Benjamin Van Durme"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.wnut-1.42", 
    "title": "TATL at WNUT-2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.", 
    "year": 2020, 
    "venue": "WNUT", 
    "references": 22, 
    "authors": [
      "Anh Tuan Nguyen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.256", 
    "title": "Evidence-based Factual Error Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.1", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 43, 
    "authors": [
      "James Thorne", 
      "Andreas Vlachos"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/N19-4017", 
    "title": "LT Expertfinder: An Evaluation Framework for Expert Finding Methods", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Expert finding is the task of ranking persons for a predefined topic or search query. Finding experts for a specified area is an important task and has attracted much attention in the information retrieval community. Most approaches for this task are evaluated in a supervised fashion, which depend on predefined topics of interest as well as gold standard expert rankings. Famous representatives of such datasets are enriched versions of DBLP provided by the ArnetMiner projet or the W3C Corpus of TREC. However, manually ranking experts can be considered highly subjective and detailed rankings are hardly distinguishable. Evaluating these datasets does not necessarily guarantee a good or bad performance of the system. Particularly for dynamic systems, where topics are not predefined but formulated as a search query, we believe a more informative approach is to perform user studies for directly comparing different methods in the same view. In order to accomplish this in a user-friendly way, we present the LT Expert Finder web-application, which is equipped with various query-based expert finding methods that can be easily extended, a detailed expert profile view, detailed evidence in form of relevant documents and statistics, and an evaluation component that allows the qualitative comparison between different rankings.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 30, 
    "authors": [
      "Tim Fischer", 
      "Steffen Remus", 
      "Chris Biemann"
    ], 
    "topics": [
      "Google Scholar", 
      "Web application", 
      "Supervised learning", 
      "Text corpus", 
      "Usability testing", 
      "Wikidata", 
      "Dynamical system", 
      "Scientific literature", 
      "Syntax (logic)", 
      "Information", 
      "Open-source software", 
      "Portable Document Format"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.183", 
    "title": "Focus-Constrained Attention Mechanism for CVAE-based Response Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To model diverse responses for a given post, one promising way is to introduce a latent variable into Seq2Seq models. The latent variable is supposed to capture the discourse-level information and encourage the informativeness of target responses. However, such discourse-level information is often too coarse for the decoder to be utilized. To tackle it, our idea is to transform the coarse-grained discourse-level information into fine-grained word-level information. Specifically, we firstly measure the semantic concentration of corresponding target response on the post words by introducing a fine-grained focus signal. Then, we propose a focus-constrained attention mechanism to take full advantage of focus in well aligning the input to the target response. The experimental results demonstrate that by exploiting the fine-grained signal, our model can generate more diverse and informative responses compared with several state-of-the-art models.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 32, 
    "authors": [
      "Zhi Cui", 
      "Yanran Li", 
      "Jiayi Zhang", 
      "Jianwei Cui", 
      "Chen Wei", 
      "Bin Wang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.wat-1.24", 
    "title": "How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.", 
    "year": 2021, 
    "venue": "WAT", 
    "references": 32, 
    "authors": [
      "Rahul Aralikatte", 
      "H\u00e9ctor Murrieta Bello", 
      "Miryam de Lhoneux", 
      "Daniel Hershcovich", 
      "Marcel Bollmann", 
      "Anders S\u00f8gaard"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-049-6_086", 
    "title": "Identifying the Authors' National Variety of English in Social Media Texts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present a study for the identification of authors\u2019 national variety of English in texts from social media. In data from Facebook and Twitter, information about the author\u2019s social profile is annotated, and the national English variety (US, UK, AUS, CAN, NNS) that each author uses is attributed. We tested four feature types: formal linguistic features, POS features, lexicon-based features related to the different varieties, and data-based features from each English variety. We used various machine learning algorithms for the classification experiments, and we implemented a feature selectionprocess. The classification accuracy achieved, when the 31 highest rankedfeatures were used, was up to 77.32%. The experimental results are evaluated, and the efficacy of the ranked features discussed. (Less)", 
    "year": 2017, 
    "venue": "RANLP", 
    "references": 43, 
    "authors": [
      "Vasiliki Simaki", 
      "Panagiotis Simakis", 
      "C. Paradis", 
      "A. Kerren"
    ], 
    "topics": [
      "Social media", 
      "Machine learning", 
      "Feature selection", 
      "Lexicon", 
      "Experiment", 
      "Nearest neighbor search", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/w15-05", 
    "title": "Proceedings of the 2nd Workshop on Argumentation Mining", 
    "fields_of_study": [
      "Engineering"
    ], 
    "abstract": null, 
    "year": 2015, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Claire Cardie", 
      "Nancy L. Green", 
      "Iryna Gurevych", 
      "D. Litman", 
      "S. Muresan", 
      "G. Petasis", 
      "Manfred Stede", 
      "M. Walker", 
      "J. Wiebe"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1122", 
    "title": "Inter-Weighted Alignment Network for Sentence Pair Modeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentence pair modeling is a crucial problem in the field of natural language processing. In this paper, we propose a model to measure the similarity of a sentence pair focusing on the interaction information. We utilize the word level similarity matrix to discover fine-grained alignment of two sentences. It should be emphasized that each word in a sentence has a different importance from the perspective of semantic composition, so we exploit two novel and efficient strategies to explicitly calculate a weight for each word. Although the proposed model only use a sequential LSTM for sentence modeling without any external resource such as syntactic parser tree and additional lexicon features, experimental results show that our model achieves state-of-the-art performance on three datasets of two tasks.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 48, 
    "authors": [
      "Gehui Shen", 
      "Yunlun Yang", 
      "Zhihong Deng"
    ], 
    "topics": [
      "Natural language processing", 
      "Interaction information", 
      "Similarity measure", 
      "Lexicon", 
      "Long short-term memory", 
      "Parsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1403", 
    "title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "S. Angelidis", 
      "Mirella Lapata"
    ], 
    "topics": [
      "Baseline (configuration management)", 
      "Multiple instance learning", 
      "Computer multitasking", 
      "Test set", 
      "Randomness extractor", 
      "Kerrison Predictor", 
      "Sentiment analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3052", 
    "title": "LeBLEU: N-gram-based Translation Evaluation Score for Morphologically Complex Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task. LeBLEU extends the popular BLEU score to consider fuzzy matches between word n-grams. While there are several variants of BLEU that allow to non-exact matches between words either by character-based distance measures or morphological preprocessing, none of them use fuzzy comparison between longer chunks of text. The results on WMT data sets show that fuzzy n-gram matching improves correlations to human evaluation especially for highly compounding languages.", 
    "year": 2015, 
    "venue": "WMT@EMNLP", 
    "references": 23, 
    "authors": [
      "Sami Virpioja", 
      "S. Gr\u00f6nroos"
    ], 
    "topics": [
      "BLEU", 
      "Machine translation", 
      "N-gram", 
      "Grams", 
      "Usability", 
      "Text-based (computing)", 
      "Preprocessor", 
      "Logic programming"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.71", 
    "title": "Generative Semantic Hashing Enhanced via Boltzmann Machines", 
    "fields_of_study": [
      "Computer Science", 
      "Mathematics"
    ], 
    "abstract": "Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Lin Zheng", 
      "Qinliang Su", 
      "Dinghan Shen", 
      "Changyou Chen"
    ], 
    "topics": [
      "Boltzmann machine", 
      "Hash function", 
      "Concatenation", 
      "Memory footprint", 
      "Information retrieval", 
      "Code", 
      "Approximation algorithm", 
      "Euler\u2013Bernoulli beam theory", 
      "Variational principle", 
      "Generative model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2152", 
    "title": "ECNU at SemEval-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our systems submitted to the Fine-Grained Sentiment Analysis on Financial Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively.", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 12, 
    "authors": [
      "M. Jiang", 
      "Man Lan", 
      "Yuanbin Wu"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Lexicon", 
      "Artificial neural network", 
      "Algorithm", 
      "Deep learning", 
      "Word embedding", 
      "Network model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/S14-2007", 
    "title": "SemEval-2014 Task 7: Analysis of Clinical Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the SemEval-2014, Task 7 on the Analysis of Clinical Text and presents the evaluation results. It focused on two subtasks: (i) identification (Task A) and (ii) normalization (Task B) of diseases and disorders in clinical reports as annotated in the Shared Annotated Resources (ShARe) 1 corpus. This task was a follow-up to the ShARe/CLEF eHealth 2013 shared task, subtasks 1a and 1b, 2 but using a larger test set. A total of 21 teams competed in Task A, and 18 of those also participated in Task B. For Task A, the best system had a strict F1-score of 81.3, with a precision of 84.3 and recall of 78.6. For Task B, the same group had the best strict accuracy of 74.1. The organizers have made the text corpora, annotations, and evaluation tools available for future research and development at the shared task website. 3", 
    "year": 2014, 
    "venue": "*SEMEVAL", 
    "references": 35, 
    "authors": [
      "Sameer Pradhan", 
      "No\u00e9mie Elhadad", 
      "W. Chapman", 
      "S. Manandhar", 
      "G. Savova"
    ], 
    "topics": [
      "Named-entity recognition", 
      "SemEval", 
      "F1 score", 
      "Database normalization", 
      "Word embedding", 
      "End-to-end principle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.403", 
    "title": "Analyzing Political Parody in Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 67, 
    "authors": [
      "Antonis Maronikolakis", 
      "Danae S\u00e1nchez Villegas", 
      "Daniel Preotiuc-Pietro", 
      "Nikolaos Aletras"
    ], 
    "topics": [
      "Social media", 
      "Sentiment analysis", 
      "Sensor", 
      "Machine learning", 
      "Supervised learning"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.138", 
    "title": "Modeling Event Plausibility with Consistent Conceptual Abstraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models\u2014most recently pre-trained, Transformer language models\u2014have demonstrated improvements in modeling event plausibility, their performance still falls short of humans\u2019. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that \u201ca person breathing\u201d is plausible while \u201ca dentist breathing\u201d is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 55, 
    "authors": [
      "Ian Porada", 
      "Kaheer Suleman", 
      "Adam Trischler", 
      "J. C. K. Cheung"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.bionlp-1.24", 
    "title": "BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models.", 
    "year": 2021, 
    "venue": "BIONLP", 
    "references": 27, 
    "authors": [
      "Sultan Alrowili", 
      "Vijay K. Shanker"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.dialdoc-1.7", 
    "title": "Technical Report on Shared Task in DialDoc21", 
    "fields_of_study": null, 
    "abstract": "We participate in the DialDoc Shared Task sub-task 1 (Knowledge Identification). The task requires identifying the grounding knowledge in form of a document span for the next dialogue turn. We employ two well-known pre-trained language models (RoBERTa and ELECTRA) to identify candidate document spans and propose a metric-based ensemble method for span selection. Our methods include data augmentation, model pre-training/fine-tuning, post-processing, and ensemble. On the submission page, we rank 2nd based on the average of normalized F1 and EM scores used for the final evaluation. Specifically, we rank 2nd on EM and 3rd on F1.", 
    "year": 2021, 
    "venue": "DIALDOC", 
    "references": 17, 
    "authors": [
      "Jiapeng Li", 
      "Mingda Li", 
      "Longxuan Ma", 
      "Weinan Zhang", 
      "Ting Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5817", 
    "title": "Sanskrit n-Retroflexion is Input-Output Tier-Based Strictly Local", 
    "fields_of_study": [
      "Mathematics"
    ], 
    "abstract": "Sanskrit /n/-retroflexion is one of the most complex segmental processes in phonology. While it is still star-free, it does not fit in any of the subregular classes that are commonly entertained in the literature. We show that when construed as a phonotactic dependency, the process fits into a class we call input-output tier-based strictly local (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL\u2019s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.", 
    "year": 2018, 
    "venue": "", 
    "references": 34, 
    "authors": [
      "T. Graf", 
      "Connor Mayer"
    ], 
    "topics": [
      "Multitier architecture", 
      "Learnability", 
      "phonology", 
      "FITS", 
      "Posterior displacement", 
      "Complexity", 
      "Rewriting", 
      "Right-to-left", 
      "Transduction (machine learning)", 
      "Meteorological reanalysis", 
      "Map", 
      "Substance-Related Disorders", 
      "GUCY2C protein, human", 
      "emotional dependency", 
      "Rule (guideline)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/W19-1417", 
    "title": "Ensemble Methods to Distinguish Mainland and Taiwan Chinese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the IUCL system at VarDial 2019 evaluation campaign for the task of discriminating between Mainland and Taiwan variation of mandarin Chinese. We first build several base classifiers, including a Naive Bayes classifier with word n-gram as features, SVMs with both character and syntactic features, and neural networks with pre-trained character/word embeddings. Then we adopt ensemble methods to combine output from base classifiers to make final predictions. Our ensemble models achieve the highest F1 score (0.893) in simplified Chinese track and the second highest (0.901) in traditional Chinese track. Our results demonstrate the effectiveness and robustness of the ensemble methods.", 
    "year": 2019, 
    "venue": "", 
    "references": 28, 
    "authors": [
      "Hai Hu", 
      "Wen Li", 
      "He Zhou", 
      "Zuoyu Tian", 
      "Yiwen Zhang", 
      "Liang Zou"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-short.99", 
    "title": "Issues with Entailment-based Zero-shot Text Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 0, 
    "authors": [
      "Tingting Ma", 
      "Jin-Ge Yao", 
      "Chin-Yew Lin", 
      "Tiejun Zhao"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1057", 
    "title": "A Comparison of Selectional Preference Models for Automatic Verb Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German. We find that all the models we compare are effective for verb clustering; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner. A very simple model based on lexical preferences is also found to perform well.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 55, 
    "authors": [
      "W. Roberts", 
      "Markus Egg"
    ], 
    "topics": [
      "Unsupervised learning", 
      "Cluster analysis", 
      "Lexical substitution", 
      "statistical cluster", 
      "Class"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1167", 
    "title": "Learning to Generate Textual Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While defining generic data generators is difficult, we propose to allow generators to be \u201cweakly\u201d specified in the sense that a set of parameters controls how the data is generated. Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters. Instead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an efficient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters. We illustrate its benefits by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 18, 
    "authors": [
      "Guillaume Bouchard", 
      "Pontus Stenetorp", 
      "S. Riedel"
    ], 
    "topics": [
      "Algorithm", 
      "Bayesian optimization", 
      "Sampling (signal processing)", 
      "Artificial neural network", 
      "Experiment", 
      "Deep learning", 
      "Stochastic optimization", 
      "Text corpus", 
      "Cross-validation (statistics)", 
      "Coefficient", 
      "Mathematical optimization", 
      "Information", 
      "Vocabulary", 
      "Gaussian quadrature", 
      "Generic programming"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075219", 
    "title": "Invited Talk: Processes that Shape Conversation and their Implications for Computational Linguistics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Experimental studies of interactive language use have shed light on the cognitive and interpersonal processes that shape conversation; corpora are the emergent products of these processes. I will survey studies that focus on under-modelled aspects of interactive language use, including the processing of spontaneous speech and disfluencies; metalinguistic displays such as hedges; interactive processes that affect choices of referring expressions; and how communication media shape conversations. The findings suggest some agendas for computational linguistics.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 52, 
    "authors": [
      "S. Brennan"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation", 
      "Spontaneous order", 
      "Text corpus", 
      "Emergence"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K18-3016", 
    "title": "BME-HAS System for CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents an encoder-decoder neural network based solution for both subtasks of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection. All of our models are sequence-to-sequence neural networks with multiple encoders and a single decoder.", 
    "year": 2018, 
    "venue": "CoNLL", 
    "references": 8, 
    "authors": [
      "Judit \u00c1cs"
    ], 
    "topics": [
      "Encoder", 
      "Artificial neural network", 
      "Random seed", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1006", 
    "title": "QCRI at SemEval-2016 Task 4: Probabilistic Methods for Binary and Ordinal Quantification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe the systems we have used for participating in Subtasks D (binary quantification) and E (ordinal quantification) of SemEval-2016 Task 4 \u201cSentiment Analysis in Twitter\u201d. The binary quantification system uses a \u201cProbabilistic Classify and Count\u201d (PCC) approach that leverages the calibrated probabilities obtained from the output of an SVM. The ordinal quantification approach uses an ordinal tree of PCC binary quantifiers, where the tree is generated via a splitting criterion that minimizes the ordinal quantification loss.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 20, 
    "authors": [
      "Giovanni Da San Martino", 
      "Wei Gao", 
      "F. Sebastiani"
    ], 
    "topics": [
      "Ordinal data", 
      "Sentiment analysis", 
      "Portable C Compiler"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/980691.980786", 
    "title": "A Tabular Interpretation of a Class of 2-Stack Automata", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The paper presents a tabular interpretation for a kind of 2-Stack Automata. These automata may be used to describe various parsing strategies, ranging from purely top-down to purely bottom-up, for LIGs and TAGs. The tabular interpretation ensures, for all strategies, a time complexity in O(n6) and space complexity in O(n5) where n is the length of the input string.", 
    "year": 1998, 
    "venue": "COLING-ACL", 
    "references": 19, 
    "authors": [
      "Eric Villemonte de la Clergerie", 
      "Miguel A. Alonso"
    ], 
    "topics": [
      "Table (information)", 
      "Dynamic programming", 
      "Parsing", 
      "Time complexity", 
      "Automaton", 
      "Turing machine", 
      "Tree-adjoining grammar", 
      "Efficient XML Interchange", 
      "Emoticon", 
      "Automata theory", 
      "Bottom-up parsing", 
      "DSPACE", 
      "Trabb Pardo\u2013Knuth algorithm", 
      "Indexed grammar", 
      "Correctness (computer science)", 
      "Top-down and bottom-up design", 
      "Disk mirroring", 
      "Network switch", 
      "Personal digital assistant", 
      "Bottom-up proteomics", 
      "Embedded system", 
      "Session ID", 
      "Context-free grammar"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1402", 
    "title": "Few-Shot Representation Learning for Out-Of-Vocabulary Words", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 33, 
    "authors": [
      "Ziniu Hu", 
      "Ting Chen", 
      "Kai-Wei Chang", 
      "Yizhou Sun"
    ], 
    "topics": [
      "Vocabulary", 
      "Downstream (software development)", 
      "Text corpus", 
      "Word embedding", 
      "Encoder", 
      "Experiment", 
      "Oracle NoSQL DB", 
      "Benchmark (computing)", 
      "Machine learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-5113", 
    "title": "Pronunciation Adaptation For Disordered Speech Recognition Using State-Specific Vectors of Phone-Cluster Adaptive Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pronunciation variation is a major problem in disordered speech recognition. This paper focus on handling the pronunciation variations in dysarthric speech by forming speaker-specific lexicons. A novel approach is proposed for identifying mispronunciations made by each dysarthric speaker, using state-specific vector (SSV) of phone-cluster adaptive training (Phone-CAT) acoustic model. SSV is low-dimensional vector estimated for each tied-state where each element in a vector denotes the weight of a particular monophone. The SSV indicates the pronounced phone using its dominant weight. This property of SSV is exploited in adapting the pronunciation of a particular dysarthric speaker using speaker-specific lexicons. Experimental validation on Nemours database showed an average relative improvement of 9% across all the speakers compared to the system built with canonical lexicon. Index Terms: Dysarthric speech recognition, phone-CAT, lexical modeling, pronunciations, phone confusion matrix", 
    "year": 2015, 
    "venue": "SLPAT@Interspeech", 
    "references": 24, 
    "authors": [
      "R. Sriranjani", 
      "S. Umesh", 
      "M. Reddy"
    ], 
    "topics": [
      "Speech recognition", 
      "Lexicon", 
      "Acoustic model", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1028", 
    "title": "Modeling Tweet Arrival Times using Log-Gaussian Cox Processes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a logGaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 20, 
    "authors": [
      "M. Lukasik", 
      "P. K. Srijith", 
      "Trevor Cohn", 
      "Kalina Bontcheva"
    ], 
    "topics": [
      "Text corpus", 
      "Time series", 
      "Baseline (configuration management)", 
      "Gaussian blur"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1102", 
    "title": "A Graph-Based Analysis of Medical Queries of a Swedish Health Care Portal", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Today web portals play an increasingly important role in health care allowing information seekers to learn about diseases and treatments, and to administrate their care. Therefore, it is important that the portals are able to support this process as well as possible. In this paper, we \nstudy the search logs of a public Swedish health portal to address the questions if health information seeking differs from other types of Internet search and if there is a potential for utilizing network analysis methods in combination with semantic annotation to gain insights into search behaviors. Using a semantic-based method and a graph-based analysis of word cooccurrences in queries, we show there is an overlap among the results indicating a potential role of these types of methods to gain insights and facilitate improved information search. In addition we show that samples, windows of a month, of search logs may be sufficient to obtain similar results as using larger windows. We also show that medical queries share the same \nstructural properties found for other types of information searches, thereby indicating an ability to reuse existing analysis methods for this type of search data.", 
    "year": 2014, 
    "venue": "Louhi@EACL", 
    "references": 27, 
    "authors": [
      "Farnaz Moradi", 
      "A. Eklund", 
      "D. Kokkinakis", 
      "T. Olovsson", 
      "P. Tsigas"
    ], 
    "topics": [
      "Information seeking", 
      "Microsoft Windows", 
      "Portals", 
      "Algorithm", 
      "Experiment", 
      "Semantic analysis (compilers)", 
      "Social network analysis"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.1162/tacl_a_00354", 
    "title": "Deciphering Undersegmented Ancient Scripts Using Phonetic Prior", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning character embeddings based on the International Phonetic Alphabet (IPA). The resulting generative framework jointly models word segmentation and cognate alignment, informed by phonological constraints. We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic. For Iberian, the method does not show strong evidence supporting Basque as a related language, concurring with the favored position by the current scholarship.1", 
    "year": 2021, 
    "venue": "TACL", 
    "references": 61, 
    "authors": [
      "Jiaming Luo", 
      "F. Hartmann", 
      "Enrico Santus", 
      "Yuan Cao", 
      "R. Barzilay"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1653", 
    "title": "Distilling Translations with Visual Awareness", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 51, 
    "authors": [
      "Julia Ive", 
      "P. Madhyastha", 
      "Lucia Specia"
    ], 
    "topics": [
      "Machine translation", 
      "Preprocessor", 
      "Compiler", 
      "First Draft of a Report on the EDVAC", 
      "Multimodal interaction", 
      "HTTPS"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-3002", 
    "title": "Leveraging Compounds to Improve Noun Phrase Translation from Chinese and German", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents a method to improve the translation of polysemous nouns, when a previous occurrence of the noun as the head of a compound noun phrase is available in a text. The occurrences are identified through pattern matching rules, which detect XY compounds followed closely by a potentially coreferent occurrence of Y , such as \u201cNordwand ... Wand\u201d. Two strategies are proposed to improve the translation of the second occurrence of Y : re-using the cached translation of Y from the XY compound, or post-editing the translation of Y using the head of the translation of XY . Experiments are performed on Chinese-toEnglish and German-to-French statistical machine translation, over the WIT3 and Text+Berg corpora respectively, with 261 XY/Y pairs each. The results suggest that while the overall BLEU scores increase only slightly, the translations of the targeted polysemous nouns are significantly improved.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "X. Pu", 
      "L. Mascarell", 
      "A. Popescu-Belis", 
      "M. Fishel", 
      "N. Luong", 
      "M. Volk"
    ], 
    "topics": [
      "Pattern matching", 
      "BLEU", 
      "Statistical machine translation", 
      "Baseline (configuration management)", 
      "Postediting", 
      "Text corpus", 
      "X\u2013Y plotter", 
      "Language Translations", 
      "Experiment", 
      "Rule (guideline)", 
      "Classical XY model", 
      "Genetic Translation Process"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-6615", 
    "title": "FEVER Breaker\u2019s Run of Team NbAuzDrLqg", 
    "fields_of_study": null, 
    "abstract": "We describe our submission for the Breaker phase of the second Fact Extraction and VERification (FEVER) Shared Task. Our adversarial data can be explained by two perspectives. First, we aimed at testing model\u2019s ability to retrieve evidence, when appropriate query terms could not be easily generated from the claim. Second, we test model\u2019s ability to precisely understand the implications of the texts, which we expect to be rare in FEVER 1.0 dataset. Overall, we suggested six types of adversarial attacks. The evaluation on the submitted systems showed that the systems were only able get both the evidence and label correct in 20% of the data. We also demonstrate our adversarial run analysis in the data development process.", 
    "year": 2019, 
    "venue": "EMNLP", 
    "references": 7, 
    "authors": [
      "Youngwoo Kim", 
      "J. Allan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4219", 
    "title": "An NLP Analysis of Exaggerated Claims in Science News", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The discrepancy between science and media has been affecting the effectiveness of science communication. Original findings from science publications may be distorted with altered claim strength when reported to the public, causing misinformation spread. This study conducts an NLP analysis of exaggerated claims in science news, and then constructed prediction models for identifying claim strength levels in science reporting. The results demonstrate different writing styles journal articles and news/press releases use for reporting scientific findings. Preliminary prediction models reached promising result with room for further improvement.", 
    "year": 2017, 
    "venue": "NLPmJ@EMNLP", 
    "references": 13, 
    "authors": [
      "Yingya Li", 
      "Jieke Zhang", 
      "Bei Yu"
    ], 
    "topics": [
      "F1 score", 
      "Fear, uncertainty and doubt", 
      "Natural language processing", 
      "Correlation does not imply causation", 
      "Parsing", 
      "Science communication", 
      "Causality", 
      "Discrepancy function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/d17-2", 
    "title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2017, 
    "venue": "EMNLP 2017", 
    "references": 0, 
    "authors": [
      "Lucia Specia", 
      "Matt Post", 
      "Michael J. Paul"
    ], 
    "topics": [
      "Empirical Methods in Natural Language Processing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.466", 
    "title": "Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 0, 
    "authors": [
      "Liang Li", 
      "Can Ma", 
      "Yinliang Yue", 
      "Dayong Hu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.444", 
    "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To provide consistent emotional interaction with users, dialog systems should be capable to automatically select appropriate emotions for responses like humans. However, most existing works focus on rendering specified emotions in responses or empathetically respond to the emotion of users, yet the individual difference in emotion expression is overlooked. This may lead to inconsistent emotional expressions and disinterest users. To tackle this issue, we propose to equip the dialog system with personality and enable it to automatically select emotions in responses by simulating the emotion transition of humans in conversation. In detail, the emotion of the dialog system is transitioned from its preceding emotion in context. The transition is triggered by the preceding dialog context and affected by the specified personality trait. To achieve this, we first model the emotion transition in the dialog system as the variation between the preceding emotion and the response emotion in the Valence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks to encode the preceding dialog context and the specified personality traits to compose the variation. Finally, the emotion for response is selected from the sum of the preceding emotion and the variation. We construct a dialog dataset with emotion and personality labels and conduct emotion prediction tasks for evaluation. Experimental results validate the effectiveness of the personality-affected emotion transition.1.", 
    "year": 2021, 
    "venue": "", 
    "references": 60, 
    "authors": [
      "Weng Zhiyuan", 
      "Cao Jiannong", 
      "Yang Ruosong", 
      "Liu Shuaiqi", 
      "Shen Jiaxing"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1225753.1225757", 
    "title": "CL Research\u2019s Knowledge Management System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "CL Research began experimenting with massive XML tagging of texts to answer questions in TREC 2002. In DUC 2003, the experiments were extended into text summarization. Based on these experiments, The Knowledge Management System (KMS) was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration. KMS has been extended to include web question answering, both general and topic-based summarization, information extraction, and document exploration. The document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation. As development of KMS has continued, user modeling has become a key research issue: how will different users want to use the information they identify.", 
    "year": 2005, 
    "venue": "ACL", 
    "references": 17, 
    "authors": [
      "K. Litkowski"
    ], 
    "topics": [
      "Management system", 
      "Question answering", 
      "Experiment", 
      "Information extraction", 
      "User modeling", 
      "Knowledge management", 
      "Software repository", 
      "XML", 
      "Web Ontology Language", 
      "Automatic summarization", 
      "Upsampling"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.sigmorphon-1.9", 
    "title": "The IMS\u2013CUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS--CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.", 
    "year": 2020, 
    "venue": "SIGMORPHON", 
    "references": 56, 
    "authors": [
      "Manuel Mager", 
      "Katharina Kann"
    ], 
    "topics": [
      "Baseline (configuration management)", 
      "Programming paradigm", 
      "Pointer (computer programming)", 
      "Long short-term memory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W15-0509", 
    "title": "From Argumentation Mining to Stance Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Argumentation mining and stance classification were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-defined argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classification. Additionally, a manuallyannotated corpus for stance classification and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the benefits of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classification, close to the results of a supervised classifier. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classification shows promising results.", 
    "year": 2015, 
    "venue": "ArgMining@HLT-NAACL", 
    "references": 33, 
    "authors": [
      "Parinaz Sobhani", 
      "D. Inkpen", 
      "S. Matwin"
    ], 
    "topics": [
      "Text mining", 
      "Topic model", 
      "Machine learning", 
      "Latent Dirichlet allocation", 
      "Supervised learning", 
      "Non-negative matrix factorization", 
      "Statistical model", 
      "Text corpus", 
      "Off topic", 
      "One-to-one (data model)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1127", 
    "title": "Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recognizing named entities in a document is a key task in many NLP applications. Although current state-of-the-art approaches to this task reach a high performance on clean text (e.g. newswire genres), those algorithms dramatically degrade when they are moved to noisy environments such as social media domains. We present two systems that address the challenges of processing social media data using character-level phonetics and phonology, word embeddings, and Part-of-Speech tags as features. The first model is a multitask end-to-end Bidirectional Long Short-Term Memory (BLSTM)-Conditional Random Field (CRF) network whose output layer contains two CRF classifiers. The second model uses a multitask BLSTM network as feature extractor that transfers the learning to a CRF classifier for the final prediction. Our systems outperform the current F1 scores of the state of the art on the Workshop on Noisy User-generated Text 2017 dataset by 2.45% and 3.69%, establishing a more suitable approach for social media environments.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 42, 
    "authors": [
      "Gustavo Aguilar", 
      "A. P. L\u00f3pez-Monroy", 
      "F. Gonz\u00e1lez", 
      "T. Solorio"
    ], 
    "topics": [
      "Social media", 
      "Computer multitasking", 
      "Long short-term memory", 
      "Named entity", 
      "Natural language processing", 
      "Stack Exchange", 
      "Categorization", 
      "Conditional random field", 
      "Channel (communications)", 
      "End-to-end principle", 
      "Vocabulary", 
      "Part-of-speech tagging", 
      "Randomness extractor", 
      "Neural Networks", 
      "Algorithm", 
      "Technical standard", 
      "Named-entity recognition"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2079", 
    "title": "Tagging Performance Correlates with Author Age", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Dirk Hovy", 
      "Anders S\u00f8gaard"
    ], 
    "topics": [
      "The Wall Street Journal", 
      "Natural language processing", 
      "Vocabulary", 
      "Part-of-speech tagging", 
      "Experiment", 
      "Downstream (software development)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1013", 
    "title": "Neural Machine Translation with Word Predictions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence. These vectors are generated by parameters which are updated by back-propagation of translation errors through time.We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "Rongxiang Weng", 
      "Shujian Huang", 
      "Zaixiang Zheng", 
      "Xinyu Dai", 
      "Jiajun Chen"
    ], 
    "topics": [
      "Neural machine translation", 
      "Encoder", 
      "BLEU", 
      "Vocabulary", 
      "Codec", 
      "Backpropagation", 
      "End-to-end principle", 
      "Software propagation", 
      "Propagation of uncertainty"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1317", 
    "title": "Multi-Head Attention with Disagreement Regularization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed approach.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "Jian Li", 
      "Zhaopeng Tu", 
      "Baosong Yang", 
      "Michael R. Lyu", 
      "T. Zhang"
    ], 
    "topics": [
      "Manifold regularization", 
      "Universality probability", 
      "YANG"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-1051", 
    "title": "Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Document enrichment focuses on retrieving relevant knowledge from external resources, which is essential because text is generally replete with gaps. Since conventional work primarily relies on special resources, we instead use triples of Subject, Predicate, Object as knowledge and incorporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirichlet Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a ranking problem, our model significantly outperforms multiple strong baselines. Moreover, we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 34, 
    "authors": [
      "Muyu Zhang", 
      "Bing Qin", 
      "Mao Zheng", 
      "Graeme Hirst", 
      "Ting Liu"
    ], 
    "topics": [
      "Distributional semantics", 
      "Gene Ontology Term Enrichment", 
      "Document classification", 
      "Experiment", 
      "Iterative method", 
      "Latent Dirichlet allocation", 
      "Viable system model", 
      "Local-density approximation", 
      "Parsing", 
      "Relevance", 
      "Algorithm", 
      "Word lists by frequency", 
      "Software propagation", 
      "ENCODE", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.324", 
    "title": "A Multi-Persona Chatbot for Hotline Counselor Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a chatbot can be developed to help train human counselors. Such a system needs to simulate intimate situations across multiple practice sessions. Open-domain dialogue systems frequently suffer from generic responses that do not characterize personal stories, so we look to infuse conversations with persona information by mimicking prototype conversations. Towards building a \u201cCrisisbot\u201d hotline visitor simulation, we propose a counseling strategy annotation scheme and a multi-task framework that leverages these counselor strategies to retrieve similar examples, generate diverse sub-utterances, and interleave prototype and generated sub-utterances into complex responses. We evaluate this framework with crowdworkers and experienced hotline counselors. The framework considerably increases response diversity and specificity, with limited impact to coherence. Our results also show a considerable discrepancy between crowdworker and counselor judgements, which emphasizes the importance of including target populations in system development and evaluation.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 44, 
    "authors": [
      "O. Demasi", 
      "Yu Li", 
      "Zhou Yu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5207", 
    "title": "Annotating Claims in the Vaccination Debate", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present annotation experiments with three different annotation schemes for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our corpus, which contains heterogeneous texts from the Web. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3% IAA.", 
    "year": 2018, 
    "venue": "ArgMining@EMNLP", 
    "references": 14, 
    "authors": [
      "Benedetta Torsi", 
      "R. Morante"
    ], 
    "topics": [
      "Error analysis (mathematics)", 
      "Experiment", 
      "Text corpus", 
      "World Wide Web", 
      "Information", 
      "High-level programming language", 
      "Online and offline", 
      "Level of detail", 
      "Text simplification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/W15-3008", 
    "title": "The Karlsruhe Institute of Technology Translation Systems for the WMT 2012", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, the KIT systems submitted to the Shared Translation Task are presented. We participated in two translation directions: from German to English and from English to German. Both translations are generated using phrase-based translation systems. The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters. The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models. In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system. We evaluated the rescoring using only the baseline features as well as using additional computational complex features.", 
    "year": 2011, 
    "venue": "WMT@EMNLP", 
    "references": 36, 
    "authors": [
      "T. Herrmann", 
      "Mohammed Mediani", 
      "J. Niehues", 
      "A. Waibel"
    ], 
    "topics": [
      "Machine translation", 
      "Language model", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.29", 
    "title": "A Joint Model for Document Segmentation and Segment Labeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 38, 
    "authors": [
      "Joe Barrow", 
      "R. Jain", 
      "Vlad I. Morariu", 
      "Varun Manjunatha", 
      "D. Oard", 
      "P. Resnik"
    ], 
    "topics": [
      "Document layout analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1604", 
    "title": "On the Compositionality and Semantic Interpretation of English Noun Compounds", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present a study covering the creation of compositional distributional representations for English noun compounds (e.g. computer science) using two compositional models proposed in the literature. The compositional representations are first evaluated based on their similarity to the corresponding corpus-learned representations and then on the task of automatic classification of semantic relations for English noun compounds. Our experiments show that compositional models are able to build meaningful representations for more than half of the test set compounds. However, using pre-trained compositional models does not lead to the expected performance gains for the semantic relation classification task. Models using compositional representations have a similar performance as a basic classification model, despite the advantage of being pretrained on a large set of compounds.", 
    "year": 2016, 
    "venue": "Rep4NLP@ACL", 
    "references": 29, 
    "authors": [
      "C. Dima"
    ], 
    "topics": [
      "Ontology components", 
      "Semantic interpretation", 
      "Experiment", 
      "Test set", 
      "Computer science", 
      "Text corpus", 
      "Preprocessor", 
      "Vocabulary", 
      "Downstream (software development)", 
      "Semantic network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1080", 
    "title": "Analyzing Biases in Human Perception of User Age and Gender from Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "User traits disclosed through written text, such as age and gender, can be used to personalize applications such as recommender systems or conversational agents. However, human perception of these traits is not perfectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets. We systematically analyze the quality and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice. Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 76, 
    "authors": [
      "Lucie Flekova", 
      "J. Carpenter", 
      "Salvatore Giorgi", 
      "L. Ungar", 
      "Daniel Preotiuc-Pietro"
    ], 
    "topics": [
      "Proxy server", 
      "Amazon Mechanical Turk"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3220", 
    "title": "TECHLIMED$@$QALB-Shared Task 2015: a hybrid Arabic Error Correction System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper reports on the participation of Techlimed in the Second Shared Task on Automatic Arabic Error Correction organized by the Arabic Natural Language Processing Workshop. This year's competition includes two tracks, and, in addition to errors produced by native speakers (L1), also includes correction of texts written by learners of Arabic as a foreign language (L2). Techlimed participated in the L1 track. For our participation in the L1 evaluation task, we developed two systems. The first one is based on the spellchecker Hunspell with specific dictionaries. The second one is a hybrid system based on rules, morphology analysis and statistical machine translation. Our results on the test set show that the hybrid system outperforms the lexicon driven approach with a precision of 71.2%, a recall of 64.94% and an F-measure of 67.93%.", 
    "year": 2015, 
    "venue": "ANLP@ACL", 
    "references": 20, 
    "authors": [
      "D. Mostefa", 
      "Jaber Abualasal", 
      "Omar Asbayou", 
      "Mahmoud Gzawi", 
      "Ramzi Abb\u00e8s"
    ], 
    "topics": [
      "Statistical machine translation", 
      "Hybrid system", 
      "Spell checker", 
      "Lexicon", 
      "Natural language processing", 
      "Satisfiability modulo theories", 
      "QR code", 
      "Lexical database", 
      "Dictionary", 
      "Test set", 
      "F1 score", 
      "Mathematical morphology", 
      "Error detection and correction"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.1162/coli.2009.35.4.35408", 
    "title": "Learning Machine Translation - Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster (editors) The MIT Press, 2009, xii+316 pp; ISBN 978-0-262-07297-7", 
    "fields_of_study": [
      "Art", 
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2009, 
    "venue": "Comput. Linguistics", 
    "references": 0, 
    "authors": [
      "P. Blunsom"
    ], 
    "topics": [
      "International Standard Book Number", 
      "Machine translation", 
      "MARC (archive)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00202", 
    "title": "A Large-Scale Pseudoword-Based Evaluation Framework for State-of-the-Art Word Sense Disambiguation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The evaluation of several tasks in lexical semantics is often limited by the lack of large amounts of manual annotations, not only for training purposes, but also for testing purposes. Word Sense Disambiguation (WSD) is a case in point, as hand-labeled datasets are particularly hard and time-consuming to create. Consequently, evaluations tend to be performed on a small scale, which does not allow for in-depth analysis of the factors that determine a systems' performance.In this paper we address this issue by means of a realistic simulation of large-scale evaluation for the WSD task. We do this by providing two main contributions: First, we put forward two novel approaches to the wide-coverage generation of semantically aware pseudowords (i.e., artificial words capable of modeling real polysemous words); second, we leverage the most suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a large-scale experimental framework for the comparison of state-of-the-art supervised and knowledge-based algorithms. Using this framework, we study the impact of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors which affect their performance.", 
    "year": 2014, 
    "venue": "CL", 
    "references": 101, 
    "authors": [
      "Mohammad Taher Pilehvar", 
      "R. Navigli"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Word sense", 
      "Simulation", 
      "Text corpus", 
      "Algorithm", 
      "Web Services for Devices"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1117794.1117810", 
    "title": "A Machine Learning Approach to Answering Questions for Reading Comprehension Tests", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we report results on answering questions for the reading comprehension task, using a machine learning approach. We evaluated our approach on the Remedia data set, a common data set used in several recent papers on the reading comprehension task. Our learning approach achieves accuracy competitive to previous approaches that rely on hand-crafted, deterministic rules and algorithms. To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests.", 
    "year": 2000, 
    "venue": "EMNLP", 
    "references": 13, 
    "authors": [
      "H. Ng", 
      "Leonghwee Teo", 
      "J. Kwan"
    ], 
    "topics": [
      "Machine learning", 
      "Question answering", 
      "Information retrieval", 
      "Web page", 
      "Internet", 
      "List comprehension", 
      "Information explosion", 
      "Gigabyte", 
      "Document retrieval", 
      "Web search engine", 
      "Algorithm", 
      "Global variable", 
      "Computer program", 
      "Document-oriented database", 
      "Yahoo! Answers", 
      "Screenwriting", 
      "Entropic uncertainty", 
      "Byte", 
      "Digital subscriber line", 
      "Silk Road", 
      "Software quality assurance"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-5222", 
    "title": "NLPRL at WAT2019: Transformer-based Tamil - English Indic Task Neural Machine Translation System", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the Machine Translation system for Tamil-English Indic Task organized at WAT 2019. We use Transformer- based architecture for Neural Machine Translation.", 
    "year": 2019, 
    "venue": "WAT@EMNLP-IJCNLP", 
    "references": 14, 
    "authors": [
      "Amit Kumar", 
      "Anil Kumar Singh"
    ], 
    "topics": [
      "Transformer", 
      "Neural machine translation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-1024", 
    "title": "Metaphor Detection with Cross-Lingual Model Transfer", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets in Spanish, Farsi, and Russian. The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 52, 
    "authors": [
      "Yulia Tsvetkov", 
      "Leonid Boytsov", 
      "A. Gershman", 
      "Eric Nyberg", 
      "Chris Dyer"
    ], 
    "topics": [
      "Bilingual dictionary", 
      "Transfer-based machine translation", 
      "Sparse voxel octree"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1305", 
    "title": "Learning to Decipher Hate Symbols", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Existing computational models to understand hate speech typically frame the problem as a simple classification task, bypassing the understanding of hate symbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we propose a novel task of deciphering hate symbols. To do this, we leveraged the Urban Dictionary and collected a new, symbol-rich Twitter corpus of hate speech. We investigate neural network latent context models for deciphering hate symbols. More specifically, we study Sequence-to-Sequence models and show how they are able to crack the ciphers based on context. Furthermore, we propose a novel Variational Decipher and show how it can generalize better to unseen hate symbols in a more challenging testing setting.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 36, 
    "authors": [
      "Jing Qian", 
      "Mai ElSherief", 
      "E. Belding-Royer", 
      "William Yang Wang"
    ], 
    "topics": [
      "Definition", 
      "Artificial neural network", 
      "Urban Dictionary", 
      "Computational model", 
      "Cipher", 
      "Variational principle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5803", 
    "title": "Acoustic Word Disambiguation with Phonogical Features in Danish ASR", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Phonological features can indicate word class and we can use word class information to disambiguate both homophones and homographs in automatic speech recognition (ASR). We show Danish st\u00f8d can be predicted from speech and used to improve ASR. We discover which acoustic features contain the signal of st\u00f8d, how to use these features to predict st\u00f8d and how we can make use of st\u00f8d and st\u00f8dpredictive acoustic features to improve overall ASR accuracy and decoding speed. In the process, we discover acoustic features that are novel to the phonetic characterisation of st\u00f8d.", 
    "year": 2018, 
    "venue": "", 
    "references": 29, 
    "authors": [
      "Andreas S\u00f8eborg Kirkedal"
    ], 
    "topics": [
      "Acoustic cryptanalysis", 
      "Speech analytics", 
      "Word-sense disambiguation", 
      "Binary classification", 
      "Lexicon", 
      "Automatic speech recognition", 
      "Open-source software", 
      "Test set", 
      "Experiment", 
      "Text corpus", 
      "Languages", 
      "Automated system recovery", 
      "Scientific Publication", 
      "Automatic system recovery", 
      "Acoustic model", 
      "Annotation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1218", 
    "title": "T\u00fcbingen system in VarDial 2017 shared task: experiments with language identification and cross-lingual parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our systems and results on VarDial 2017 shared tasks. Besides three language/dialect discrimination tasks, we also participated in the cross-lingual dependency parsing (CLP) task using a simple methodology which we also briefly describe in this paper. For all the discrimination tasks, we used linear SVMs with character and word features. The system achieves competitive results among other systems in the shared task. We also report additional experiments with neural network models. The performance of neural network models was close but always below the corresponding SVM classifiers in the discrimination tasks. For the cross-lingual parsing task, we experimented with an approach based on automatically translating the source treebank to the target language, and training a parser on the translated treebank. We used off-the-shelf tools for both translation and parsing. Despite achieving better-than-baseline results, our scores in CLP tasks were substantially lower than the scores of the other participants.", 
    "year": 2017, 
    "venue": "VarDial", 
    "references": 41, 
    "authors": [
      "\u00c7agri \u00c7\u00f6ltekin", 
      "Taraka Rama"
    ], 
    "topics": [
      "Parsing", 
      "Experiment", 
      "Treebank", 
      "Language identification", 
      "Artificial neural network", 
      "Compiler"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.368", 
    "title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\u00fctze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 43, 
    "authors": [
      "Timo Schick", 
      "Hinrich Sch\u00fctze"
    ], 
    "topics": [
      "Language model", 
      "Word embedding", 
      "Bag-of-words model", 
      "Downstream (software development)", 
      "Deep learning", 
      "Interconnection", 
      "Artificial neural network", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-2075", 
    "title": "CMILLS: Adapting Semantic Role Labeling Features to Dependency Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe a system for semantic role labeling adapted to a dependency parsing framework. Verb arguments are predicted over nodes in a dependency parse tree instead of nodes in a phrase-structure parse tree. Our system participated in SemEval-2015 shared Task 15, Subtask 1: CPA parsing and achieved an Fscore of 0.516. We adapted features from prior semantic role labeling work to the dependency parsing paradigm, using a series of supervised classifiers to identify arguments of a verb and then assigning syntactic and semantic labels. We found that careful feature selection had a major impact on system performance. However, sparse training data still led rule-based systems like the baseline to be more effective than learning-based approaches.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 13, 
    "authors": [
      "Chad Mills", 
      "Gina-Anne Levow"
    ], 
    "topics": [
      "Semantic role labeling", 
      "Parsing", 
      "Parse tree", 
      "Feature selection", 
      "Supervised learning", 
      "Dependency grammar", 
      "SemEval", 
      "Rule-based system", 
      "Baseline (configuration management)", 
      "Programming paradigm", 
      "Sparse matrix"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5010", 
    "title": "Deep Contextualized Biomedical Abbreviation Expansion", 
    "fields_of_study": [
      "Computer Science", 
      "Biology"
    ], 
    "abstract": "Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as information retrieval and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic. Then it utilizes BioELMo to extract the contextualized features of words, and feed those features to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.", 
    "year": 2019, 
    "venue": "BioNLP@ACL", 
    "references": 27, 
    "authors": [
      "Qiao Jin", 
      "Jinling Liu", 
      "Xinghua Lu"
    ], 
    "topics": [
      "Abbreviations", 
      "PubMed", 
      "Biomedical text mining", 
      "Natural language processing", 
      "Question answering", 
      "Information retrieval", 
      "National Library of Medicine (U.S.)", 
      "Text corpus", 
      "Heuristic", 
      "Human reliability", 
      "Language model", 
      "Automatic identification and data capture", 
      "Body of uterus", 
      "Abstract Summary", 
      "Silo (dataset)", 
      "NetWare Loadable Module", 
      "Deep Vein Thrombosis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-1097", 
    "title": "So similar and yet incompatible: Toward the automated identification of semantically compatible words", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce the challenge of detecting semantically compatible words, that is, words that can potentially refer to the same thing (cat and hindrance are compatible, cat and dog are not), arguing for its central role in many semantic tasks. We present a publicly available data-set of human compatibility ratings, and a neural-network model that takes distributional embeddings of words as input and learns alternative embeddings that perform the compatibility detection task quite well.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 15, 
    "authors": [
      "Germ\u00e1n Kruszewski", 
      "Marco Baroni"
    ], 
    "topics": [
      "Question answering", 
      "Computational linguistics", 
      "Semantic similarity", 
      "Network model", 
      "Plausibility structure", 
      "Sensor", 
      "Computational model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-2405", 
    "title": "Unsupervised Hierarchical Story Infilling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.", 
    "year": 2019, 
    "venue": "", 
    "references": 37, 
    "authors": [
      "Daphne Ippolito", 
      "David Grangier", 
      "Chris Callison-Burch", 
      "D. Eck"
    ], 
    "topics": [
      "Cave Story", 
      "Hierarchical database model", 
      "Language model", 
      "Sampling (signal processing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4759", 
    "title": "Predicting Translation Performance with Referential Translation Machines", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Referential translation machines achieve top performance in both bilingual and monolingual settings without accessing any task or domain specific information or resource. RTMs achieve the 3rd system results for German to English sentence-level prediction of translation quality and the 2nd system results according to root mean squared error. In addition to the new features about substring distances, punctuation tokens, character n-grams, and alignment crossings, and additional learning models, we average prediction scores from different models using weights based on their training performance for improved results.", 
    "year": 2017, 
    "venue": "WMT", 
    "references": 22, 
    "authors": [
      "Ergun Bi\u00e7ici"
    ], 
    "topics": [
      "Mean squared error", 
      "Machine translation", 
      "Substring", 
      "N-gram", 
      "Grams", 
      "Null character"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.482", 
    "title": "ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Linfeng Song", 
      "Kun Xu", 
      "Y. Zhang", 
      "Jianshu Chen", 
      "Dong Yu"
    ], 
    "topics": [
      "Multi-task learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.wnut-1.19", 
    "title": "Truecasing German user-generated conversational text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user queries to a voice-controlled dia- log system. In particular, we compare a rule- based, an n-gram language model (LM) and a recurrent neural network (RNN) approaches, evaluating the results on a German Q&A cor- pus and reporting accuracy for different case categories. We show that while RNNs reach higher accuracy especially on large datasets, character n-gram models with interpolation are still competitive, in particular on mixed- case words where their fall-back mechanisms come into play.", 
    "year": 2020, 
    "venue": "WNUT", 
    "references": 15, 
    "authors": [
      "Yulia Grishina", 
      "Thomas Gueudr\u00e9", 
      "R. Winkler"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.346", 
    "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically w.r.t. the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clusteringbased sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that ClusterFormer achieves state-of-the-art performance on several major QA benchmarks.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 40, 
    "authors": [
      "Shuohang Wang", 
      "Luowei Zhou", 
      "Zhe Gan", 
      "Yen-Chun Chen", 
      "Yuwei Fang", 
      "Siqi Sun", 
      "Yu Cheng", 
      "Jingjing Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.15", 
    "title": "Making the Best Use of Review Summary for Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentiment analysis provides a useful overview of customer review contents. Many review websites allow a user to enter a summary in addition to a full review. Intuitively, summary information may give additional benefit for review sentiment analysis. In this paper, we conduct a study to exploit methods for better use of summary information. We start by finding out that the sentimental signal distribution of a review and that of its corresponding summary are in fact complementary to each other. We thus explore various architectures to better guide the interactions between the two and propose a hierarchically-refined review-centric attention model. Empirical results show that our review-centric model can make better use of user-written summaries for review sentiment analysis, and is also more effective compared to existing methods when the user summary is replaced with summary generated by an automatic summarization system.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 36, 
    "authors": [
      "Sen Yang", 
      "Leyang Cui", 
      "Jun Xie", 
      "Yue Zhang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.358", 
    "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Understanding who blames or supports whom in news text is a critical research question in computational social science. Traditional methods and datasets for sentiment analysis are, however, not suitable for the domain of political text as they do not consider the direction of sentiments expressed between entities. In this paper, we propose a novel NLP task of identifying directed sentiment relationship between political entities from a given news document, which we call directed sentiment extraction. From a million-scale news corpus, we construct a dataset of news sentences where sentiment relations of political entities are manually annotated. We present a simple but effective approach for utilizing a pretrained transformer, which infers the target class by predicting multiple question-answering tasks and combining the outcomes. We demonstrate the utility of our proposed method for social science research questions by analyzing positive and negative opinions between political entities in two major events: 2016 U.S. presidential election and COVID-19. The newly proposed problem, data, and method will facilitate future studies on interdisciplinary NLP methods and applications.1", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 54, 
    "authors": [
      "Kunwoo Park", 
      "Zhufeng Pan", 
      "Jungseock Joo"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.naacl-industry.33", 
    "title": "Ad Headline Generation using Self-Critical Masked Language Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "For any E-commerce website it is a nontrivial problem to build enduring advertisements that attract shoppers. Many sellers often find it hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model-generated headlines outperform human (advertiser) submitted headlines in terms of both grammar and creative quality as determined by audits.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 27, 
    "authors": [
      "Yashal Shakti Kanungo", 
      "Sumit Negi", 
      "Aruna Rajan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5323", 
    "title": "CUNI Systems for the Unsupervised News Translation Task in WMT 2019", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task.", 
    "year": 2019, 
    "venue": "WMT", 
    "references": 26, 
    "authors": [
      "Ivana Kvapil\u00edkov\u00e1", 
      "Dominik Mach\u00e1cek", 
      "Ondrej Bojar"
    ], 
    "topics": [
      "BLEU", 
      "Neural machine translation", 
      "Named entity", 
      "Vocabulary", 
      "Iterative method", 
      "Synthetic intelligence"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.iwslt-1.9", 
    "title": "The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model.", 
    "year": 2021, 
    "venue": "IWSLT", 
    "references": 36, 
    "authors": [
      "Chen Xu", 
      "Xiaoqian Liu", 
      "Xiaowen Liu", 
      "Laohu Wang", 
      "Canan Huang", 
      "Tong Xiao", 
      "Jingbo Zhu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w19-7904", 
    "title": "A quantitative probe into the hierarchical structure of written Chinese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2019, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Heng Chen", 
      "Haitao Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.577", 
    "title": "Biomedical Concept Relatedness \u2013 A large EHR-based benchmark", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A promising application of AI to healthcare is the retrieval of information from electronic health records (EHRs), e.g. to aid clinicians in finding relevant information for a consultation or to recruit suitable patients for a study. This requires search capabilities far beyond simple string matching, including the retrieval of concepts (diagnoses, symptoms, medications, etc.) related to the one in question. The suitability of AI methods for such applications is tested by predicting the relatedness of concepts with known relatedness scores. However, all existing biomedical concept relatedness datasets are notoriously small and consist of hand-picked concept pairs. We open-source a novel concept relatedness benchmark overcoming these issues: it is six times larger than existing datasets and concept pairs are chosen based on co-occurrence in EHRs, ensuring their relevance for the application of interest. We present an in-depth analysis of our new dataset and compare it to existing ones, highlighting that it is not only larger but also complements existing datasets in terms of the types of concepts included. Initial experiments with state-of-the-art embedding methods show that our dataset is a challenging new benchmark for testing concept relatedness models.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 32, 
    "authors": [
      "Claudia Schulz", 
      "Josh Levy-Kramer", 
      "C. V. Assel", 
      "Miklos Kepes", 
      "N. Hammerla"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.SMM4H-1.5", 
    "title": "BERT based Transformers lead the way in Extraction of Health Information from Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our submissions for the Social Media Mining for Health (SMM4H) 2021 shared tasks. We participated in 2 tasks: (1) Classification, extraction and normalization of adverse drug effect (ADE) mentions in English tweets (Task-1) and (2) Classification of COVID-19 tweets containing symptoms (Task-6). Our approach for the first task uses the language representation model RoBERTa with a binary classification head. For the second task, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the pre-trained models for both tasks. The models are placed on top of a custom domain-specific pre-processing pipeline. Our system ranked first among all the submissions for subtask-1(a) with an F1-score of 61%. For subtask-1(b), our system obtained an F1-score of 50% with improvements up to +8% F1 over the median score across all submissions. The BERTweet model achieved an F1 score of 94% on SMM4H 2021 Task-6.", 
    "year": 2021, 
    "venue": "SMM4H", 
    "references": 20, 
    "authors": [
      "R. Sidharth", 
      "Abhiraj Tiwari", 
      "Parthivi Choubey", 
      "Saisha Kashyap", 
      "Sahil Khose", 
      "Kumud Lakara", 
      "Nishesh Singh", 
      "Ujjwal Verma"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-3009", 
    "title": "Multi-document summarization using distortion-rate ratio", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The current work adapts the optimal tree pruning algorithm(BFOS) introduced by Breiman et al.(1984) and extended by Chou et al.(1989) to the multi-document summarization task. BFOS algorithm is used to eliminate redundancy which is one of the main issues in multi-document summarization. Hierarchical Agglomerative Clustering algorithm(HAC) is employed to detect the redundancy. The tree designed by HAC algorithm is successively pruned with the optimal tree pruning algorithm to optimize the distortion vs. rate cost of the resultant tree. Rate parameter is defined to be the number of the sentences in the leaves of the tree. Distortion is the sum of the distances between the representative sentence of the cluster at each node and the other sentences in the same cluster. The sentences assigned to the leaves of the resultant tree are included in the summary. The performance of the proposed system assessed with the Rouge-1 metric is seen to be better than the performance of the DUC-2002 winners on DUC-2002 data set.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 37, 
    "authors": [
      "Ulukbek Attokurov", 
      "U. Bayazit"
    ], 
    "topics": [
      "Automatic summarization", 
      "Distortion", 
      "Algorithm", 
      "Multi-document summarization", 
      "Resultant", 
      "High-availability cluster", 
      "Performance Evaluation", 
      "Cluster analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1213", 
    "title": "Self-disclosure topic model for classifying and analyzing Twitter conversations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Self-disclosure, the act of revealing oneself to others, is an important social behavior that strengthens interpersonal relationships and increases social support. Although there are many social science studies of self-disclosure, they are based on manual coding of small datasets and questionnaires. We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations, a semi-supervised machine learning algorithm, and a computational analysis of the effects of self-disclosure on subsequent conversations. We use a longitudinal dataset of 17 million tweets, all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet, and from dyads with twenty of more conversations each. We develop self-disclosure topic model (SDTM), a variant of latent Dirichlet allocation (LDA) for automatically classifying the level of self-disclosure for each tweet. We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations. Our model significantly outperforms several comparable methods on classifying the level of selfdisclosure, and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between selfdisclosure and conversation frequency and length.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 37, 
    "authors": [
      "JinYeong Bak", 
      "Chin-Yew Lin", 
      "Alice H. Oh"
    ], 
    "topics": [
      "Topic model", 
      "Latent Dirichlet allocation", 
      "Machine learning", 
      "Trigram", 
      "Supervised learning", 
      "Algorithm", 
      "Social support", 
      "Social network", 
      "Computation", 
      "Teh", 
      "MinEd", 
      "Semiconductor industry", 
      "Ground truth"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1309", 
    "title": "Experiments to Improve Named Entity Recognition on Turkish Tweets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Social media texts are significant information sources for several application areas including trend analysis, event monitoring, and opinion mining. Unfortunately, existing solutions for tasks such as named entity recognition that perform well on formal texts usually perform poorly when applied to social media texts. In this paper, we report on experiments that have the purpose of improving named entity recognition on Turkish tweets, using two different annotated data sets. In these experiments, starting with a baseline named entity recognition system, we adapt its recognition rules and resources to better fit Twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources, and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on Turkish tweets. The evaluation results of the system with these different settings are provided with discussions of these results.", 
    "year": 2014, 
    "venue": "ArXiv", 
    "references": 26, 
    "authors": [
      "D. K\u00fc\u00e7\u00fck", 
      "R. Steinberger"
    ], 
    "topics": [
      "Experiment", 
      "Named entity", 
      "Social media", 
      "Event monitoring", 
      "Dictionary", 
      "Logic programming", 
      "Baseline (configuration management)", 
      "Database normalization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.560", 
    "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 19, 
    "authors": [
      "Pratik M. Joshi", 
      "Sebastin Santy", 
      "A. Budhiraja", 
      "Kalika Bali", 
      "M. Choudhury"
    ], 
    "topics": [
      "Natural language processing", 
      "Language technology", 
      "Taxonomy (general)", 
      "Wikipedia", 
      "Binocular disparity", 
      "Apply", 
      "Experiment", 
      "Color gradient", 
      "Relevance", 
      "Linguistic Data Consortium", 
      "Language-independent specification"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.536", 
    "title": "Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 38, 
    "authors": [
      "Jing Lu", 
      "Vincent Ng"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N16-1087", 
    "title": "Generation from Abstract Meaning Representation using Tree Transducers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Abstract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr", 
    "year": 2016, 
    "venue": "NAACL", 
    "references": 25, 
    "authors": [
      "Jeffrey Flanigan", 
      "Chris Dyer", 
      "Noah A. Smith", 
      "J. Carbonell"
    ], 
    "topics": [
      "Transducer", 
      "Natural language generation", 
      "Spanning tree", 
      "Open-source software", 
      "Sparse matrix", 
      "Baseline (configuration management)", 
      "Java", 
      "File spanning", 
      "Adaptive Multi-Rate audio codec"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-2409", 
    "title": "Units in segmentation: a computational investigation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This study investigates the use of syllables and phone(me)s in computational models of segmentation in early language acquisition. We present results of experiments with both syllables and phonemes as the basic unit using a standard state-of-theart segmentation model. We evaluate the model output based on both wordand morpheme-segmented gold standards on child-directed speech corpora from two typologically different languages. Our results do not indicate a clear advantage for one unit or the other. We argue that the computational advantage for the syllable suggested in earlier research may be an artifact of the particular language and/or segmentation strategy used in these studies.", 
    "year": 2015, 
    "venue": "", 
    "references": 61, 
    "authors": [
      "\u00c7agri \u00c7\u00f6ltekin"
    ], 
    "topics": [
      "Computation", 
      "Syllable", 
      "Text corpus", 
      "Lexicon", 
      "Computational model", 
      "Experiment", 
      "Simulation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1256", 
    "title": "Confounds and Consequences in Geotagged Twitter Data", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and text-based geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 44, 
    "authors": [
      "Umashanthi Pavalanathan", 
      "Jacob Eisenstein"
    ], 
    "topics": [
      "Geolocation", 
      "Geotagging", 
      "Latent variable model", 
      "Data acquisition", 
      "Text-based (computing)", 
      "Entity", 
      "Text corpus", 
      "Geographic coordinate system", 
      "Global Positioning System"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.TEACHINGNLP-1.1", 
    "title": "Pedagogical Principles in the Online Teaching of Text Mining: A Retrospection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The ongoing COVID-19 pandemic has brought online education to the forefront of pedagogical discussions. To make this increased interest sustainable in a post-pandemic era, online courses must be built on strong pedagogical foundations. With a long history of pedagogic research, there are many principles, frameworks, and models available to help teachers in doing so. These models cover different teaching perspectives, such as constructive alignment, feedback, and the learning environment. In this paper, we discuss how we designed and implemented our online Natural Language Processing (NLP) course following constructive alignment and adhering to the pedagogical principles of LTU. By examining our course and analyzing student evaluation forms, we show that we have met our goal and successfully delivered the course. Furthermore, we discuss the additional benefits resulting from the current mode of delivery, including the increased reusability of course content and increased potential for collaboration between universities. Lastly, we also discuss where we can and will further improve the current course design.", 
    "year": 2021, 
    "venue": "TEACHINGNLP", 
    "references": 41, 
    "authors": [
      "Rajkumar Saini", 
      "Gy\u00f6rgy Kov\u00e1cs", 
      "Mohamadreza Faridghasemnia", 
      "Hamam Mokayed", 
      "Oluwatosin Adewumi", 
      "Pedro Alonso", 
      "S. Rakesh", 
      "M. Liwicki"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1131", 
    "title": "A Neural Layered Model for Nested Named Entity Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our model dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our dynamic model outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005 datasets, respectively, in terms of F-score.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 45, 
    "authors": [
      "Meizhi Ju", 
      "Makoto Miwa", 
      "S. Ananiadou"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Long short-term memory", 
      "Named entity", 
      "NER model", 
      "Mathematical model", 
      "Embedded system", 
      "Conditional random field", 
      "Deep learning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4601", 
    "title": "Keynote: The Interplay of Discussion, Cognition and Instruction in Computer-Supported Collaborative Learning Environments", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Educational discourse is an important area for impact, which is especially timely given recent attention given to online education. In this talk I will first present a theoretical account of the complex interplay between written or oral discourse, individual cognitive processes, and external guidance in Computer-Supported Collaborative Learning (CSCL) environments. Based on the Script Theory of Guidance I will analyze how cognitive configurations shape discussions, and how participation in discussions may lead to re-configuration of the participating individual student\u2019s cognition. Second, I will give an overview of studies demonstrating the instructional value of specific types of discussion contributions, namely transactive contributions. I will finally elaborate on ways in which transactive contributions to discourse can be facilitated through external guidance, and how technologies may play an important role both in research and in instruction.", 
    "year": 2015, 
    "venue": "SIGDIAL Conference", 
    "references": 0, 
    "authors": [
      "F. Fischer"
    ], 
    "topics": [
      "Cognition", 
      "Script theory"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.212", 
    "title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From Rich Knowledge Graph Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Medical entity retrieval is an integral component for understanding and communicating information across various health systems. Current approaches tend to work well on specific medical domains but generalize poorly to unseen sub-specialties. This is of increasing concern under a public health crisis as new medical conditions and drug treatments come to light frequently. Zero-shot retrieval is challenging due to the high degree of ambiguity and variability in medical corpora, making it difficult to build an accurate similarity measure between mentions and concepts. Medical knowledge graphs (KG), however, contain rich semantics including large numbers of synonyms as well as its curated graphical structures. To take advantage of this valuable information, we propose a suite of learning tasks designed for training efficient zero-shot entity retrieval models. Without requiring any human annotation, our knowledge graph enriched architecture significantly outperforms common zero-shot benchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across multiple major medical ontologies, such as UMLS, SNOMED and ICD-10.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 12, 
    "authors": [
      "Luyang Kong", 
      "Christopher Winestock", 
      "Parminder Bhatia"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-1034", 
    "title": "A State-of-the-Art Mention-Pair Model for Coreference Resolution", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Most recent studies on coreference resolution advocate accurate yet relatively complex models, relying on, for example, entitymention or graph-based representations. As it has been convincingly demonstrated at the recent CoNLL 2012 shared task, such algorithms considerably outperform popular basic approaches, in particular mention-pair models. This study advocates a novel approach that keeps the simplicity of a mention-pair framework, while showing state-of-the-art results. Apart from being very efficient and straightforward to implement, our model facilitates experimental work on the pairwise classifier, in particular on feature engineering. The proposed model achieves the performance level of up to 61.82% (MELA F, v4 scorer) on the CoNLL test data, on par with complex state-of-the-art systems.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 22, 
    "authors": [
      "O. Uryupina", 
      "Alessandro Moschitti"
    ], 
    "topics": [
      "Jaccard index", 
      "Algorithm", 
      "Feature engineering", 
      "Decision tree", 
      "Unified Extensible Firmware Interface", 
      "Interdependence", 
      "Test set", 
      "High- and low-level", 
      "Test data", 
      "Naive Bayes classifier"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.semeval-1.38", 
    "title": "SemEval-2021 Task 8: MeasEval \u2013 Extracting Counts and Measurements and their Related Contexts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities.", 
    "year": 2021, 
    "venue": "SEMEVAL", 
    "references": 40, 
    "authors": [
      "Corey A. Harper", 
      "Jessica Cox", 
      "C. Kohler", 
      "A. Scerri", 
      "R. Daniel", 
      "Paul Groth"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1200", 
    "title": "Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The point of departure of this article is the claim that sense-specific vectors provide an advantage over normal vectors due to the polysemy that they presumably represent. This claim is based on performance gains observed in gold standard evaluation tests such as word similarity tasks. We demonstrate that this claim, at least as it is instantiated in prior art, is unfounded in two ways. Furthermore, we provide empirical data and an analytic discussion that may account for the previously reported improved performance. First, we show that ground-truth polysemy degrades performance in word similarity tasks. Therefore word similarity tasks are not suitable as an evaluation test for polysemy representation. Second, random assignment of words to senses is shown to improve performance in the same task. This and additional results point to the conclusion that performance gains as reported in previous work may be an artifact of random sense assignment, which is equivalent to sub-sampling and multiple estimation of word vector representations. Theoretical analysis shows that this may on its own be beneficial for the estimation of word similarity, by reducing the bias in the estimation of the cosine distance.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 22, 
    "authors": [
      "Haim Dubossarsky", 
      "Eitan Grossman", 
      "D. Weinshall"
    ], 
    "topics": [
      "Cosine similarity", 
      "Cross-validation (statistics)", 
      "Word embedding", 
      "Sampling (signal processing)", 
      "Ground truth"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981344.981363", 
    "title": "Subject-Dependent Co-Occurence and Word Sense Disambiguation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe a method for obtaining subject-dependent word sets relative to some (subject) domain. Using the subject classifications given in the machine-redable version of Longman's Dictionary of Contemporary English, we established subject-dependent co-occurrence links between words of the defining vocabulary to construct these \"neighborhoods\". Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work.", 
    "year": 1991, 
    "venue": "ACL", 
    "references": 17, 
    "authors": [
      "J. A. Guthrie", 
      "Louise Guthrie", 
      "Y. Wilks", 
      "Homa Aidinejad"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Word sense", 
      "Information retrieval", 
      "Defining vocabulary", 
      "Dictionary", 
      "Human-readable medium", 
      "Code"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.wat-1.17", 
    "title": "Improved English to Hindi Multimodal Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a state-of-the-art approach in machine translation, but it requires adequate training data, which is a severe problem for low-resource language pairs translation. The concept of multimodal is introduced in neural machine translation (NMT) by merging textual features with visual features to improve low-resource pair translation. WAT2021 (Workshop on Asian Translation 2021) organizes a shared task of multimodal translation for English to Hindi. We have participated the same with team name CNLP-NITS-PP in two submissions: multimodal and text-only NMT. This work investigates phrase pairs injection via data augmentation approach and attains improvement over our previous work at WAT2020 on the same task in both text-only and multimodal NMT. We have achieved second rank on the challenge test set for English to Hindi multimodal translation where Bilingual Evaluation Understudy (BLEU) score of 39.28, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.792097, and Adequacy-Fluency Metrics (AMFM) score 0.830230 respectively.", 
    "year": 2021, 
    "venue": "WAT", 
    "references": 32, 
    "authors": [
      "Sahinur Rahman Laskar", 
      "Abdullah Faiz Ur Rahman Khilji", 
      "Darsh Kaushik", 
      "Partha Pakray", 
      "Sivaji Bandyopadhyay"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119250.1119273", 
    "title": "A Two-stage Statistical Word Segmentation System for Chinese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and word-formation models. This system was evaluated on Peking University corpora at the First International Chinese Word Segmentation Bakeoff. We also give results and discussions on this evaluation.", 
    "year": 2003, 
    "venue": "SIGHAN", 
    "references": 8, 
    "authors": [
      "G. Fu", 
      "K. Luke"
    ], 
    "topics": [
      "Text segmentation", 
      "Bigram", 
      "Viterbi algorithm", 
      "Microsoft Word for Mac", 
      "Text corpus", 
      "Experiment", 
      "F1 score"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1523", 
    "title": "Word Sense Induction with Neural biLM and Symmetric Patterns", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors. We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 18, 
    "authors": [
      "Asaf Amrami", 
      "Y. Goldberg"
    ], 
    "topics": [
      "Word-sense induction", 
      "Word sense", 
      "Language model", 
      "SemEval", 
      "Lemmatisation", 
      "N-gram", 
      "Tf\u2013idf", 
      "Wafer-scale integration", 
      "Cluster analysis", 
      "Neural oscillation", 
      "Random neural network", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-2343", 
    "title": "Automatic classification of doctor-patient questions for a virtual patient record query task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present the work-in-progress of automating the classification of doctorpatient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the clinical record. We compare \u2018traditional\u2019 machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using semantic annotations, but the neural classifier achieved promising results without it.", 
    "year": 2017, 
    "venue": "BioNLP", 
    "references": 49, 
    "authors": [
      "Leonardo Campillos-Llanos", 
      "S. Rosset", 
      "Pierre Zweigenbaum"
    ], 
    "topics": [
      "Naive Bayes classifier", 
      "Dialog system", 
      "Artificial neural network", 
      "Machine learning", 
      "Natural language", 
      "Support vector machine", 
      "Word embedding", 
      "Logic programming", 
      "Multinomial logistic regression", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-3013", 
    "title": "FAMULUS: Interactive Annotation and Feedback Generation for Teaching Diagnostic Reasoning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Our proposed system FAMULUS helps students learn to diagnose based on automatic feedback in virtual patient simulations, and it supports instructors in labeling training data. \nDiagnosing is an exceptionally difficult skill to obtain but vital for many different professions (e.g., medical doctors, teachers). \nPrevious case simulation systems are limited to multiple-choice questions and thus cannot give constructive individualized feedback on a student's diagnostic reasoning process. \nGiven initially only limited data, we leverage a (replaceable) NLP model to both support experts in their further data annotation with automatic suggestions, and we provide automatic feedback for students. \nWe argue that because the central model consistently improves, our interactive approach encourages both students and instructors to recurrently use the tool, and thus accelerate the speed of data creation and annotation. \nWe show results from two user studies on diagnostic reasoning in medicine and teacher education and outline how our system can be extended to further use cases.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 13, 
    "authors": [
      "Jonas Pfeiffer", 
      "Christian M. Meyer", 
      "Claudia Schulz", 
      "J. Kiesewetter", 
      "J. Zottmann", 
      "M. Sailer", 
      "Elisabeth Bauer", 
      "F. Fischer", 
      "M. Fischer", 
      "Iryna Gurevych"
    ], 
    "topics": [
      "Simulation", 
      "Automated reasoning", 
      "Data acquisition", 
      "Open-source software", 
      "Usability testing", 
      "Entity", 
      "Natural language processing", 
      "Real-time locating system", 
      "Feedback"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1215", 
    "title": "Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 42, 
    "authors": [
      "Xinchi Chen", 
      "Yaqian Zhou", 
      "Chenxi Zhu", 
      "Xipeng Qiu", 
      "Xuanjing Huang"
    ], 
    "topics": [
      "Artificial neural network", 
      "Parsing", 
      "Recursive neural network", 
      "Directed acyclic graph", 
      "Feature engineering", 
      "Benchmark (computing)", 
      "Neuron", 
      "Recursion (computer science)", 
      "Neural network software", 
      "Transition system", 
      "Sparse matrix", 
      "Simulation", 
      "Greedy algorithm", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3301", 
    "title": "Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment. The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection. Multimodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers. In the detection of sentiment in these videos, acoustic and video features provide clarity to otherwise ambiguous transcripts. In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze spoken sentences for sentiment. We discard traditional transcription features in order to minimize human intervention and to maximize the deployability of our model on at-scale real-world data. We select high-level features for our model that have been successful in non-affect domains in order to test their generalizability in the sentiment detection domain. We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, obtaining an F1 score of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out challenge test set.", 
    "year": 2018, 
    "venue": "ArXiv", 
    "references": 32, 
    "authors": [
      "Nathaniel Blanchard", 
      "Daniel Moreira", 
      "Aparna Bharati", 
      "W. Scheirer"
    ], 
    "topics": [
      "Multimodal interaction", 
      "F1 score", 
      "Video blog", 
      "Sentiment analysis", 
      "Subtext", 
      "Acoustic cryptanalysis", 
      "High- and low-level", 
      "Test set", 
      "Oracle Fusion Architecture", 
      "Transcription (software)", 
      "Image fusion"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.5", 
    "title": "BLEU Might Be Guilty but References Are Not Innocent", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 67, 
    "authors": [
      "Markus Freitag", 
      "David Grangier", 
      "Isaac Caswell"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.460", 
    "title": "Rethinking Perturbations in Encoder-Decoders for Fast Training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 48, 
    "authors": [
      "Sho Takase", 
      "Shun Kiyono"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-8002", 
    "title": "Building a treebank for Occitan: what use for Romance UD corpora?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the application of delexicalized cross-lingual parsing on Occitan with a view to building the first dependency treebank of this language. Occitan is a Romance language spoken in the south of France and in parts of Italy and Spain. It is a relatively low-resourced language and does not have a syntactically annotated corpus as of yet. In order to facilitate the manual annotation process, we train parsing models on the existing Romance corpora from the Universal Dependencies project and apply them to Occitan. Special attention is given to the effect of this cross-lingual annotation on the work of human annotators in terms of annotation speed and ease.", 
    "year": 2019, 
    "venue": "", 
    "references": 19, 
    "authors": [
      "A. Miletic", 
      "M. Bras", 
      "Louise Esher", 
      "J. Sibille", 
      "Marianne Vergez-Couret"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-0617", 
    "title": "Dynamics of an idiostyle of a Russian suicidal blogger", 
    "fields_of_study": [
      "Psychology", 
      "Computer Science"
    ], 
    "abstract": "Over 800000 people die of suicide each year. It is es-timated that by the year 2020, this figure will have in-creased to 1.5 million. It is considered to be one of the major causes of mortality during adolescence. Thus there is a growing need for methods of identifying su-icidal individuals. Language analysis is known to be a valuable psychodiagnostic tool, however the material for such an analysis is not easy to obtain. Currently as the Internet communications are developing, there is an opportunity to study texts of suicidal individuals. Such an analysis can provide a useful insight into the peculiarities of suicidal thinking, which can be used to further develop methods for diagnosing the risk of suicidal behavior. The paper analyzes the dynamics of a number of linguistic parameters of an idiostyle of a Russian-language blogger who died by suicide. For the first time such an analysis has been conducted using the material of Russian online texts. For text processing, the LIWC program is used. A correlation analysis was performed to identify the relationship between LIWC variables and number of days prior to suicide. Data visualization, as well as comparison with the results of related studies was performed.", 
    "year": 2018, 
    "venue": "CLPsych@NAACL-HTL", 
    "references": 22, 
    "authors": [
      "Tatiana Litvinova", 
      "O. Litvinova", 
      "P. Seredin"
    ], 
    "topics": [
      "Blogger", 
      "Data visualization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-2157", 
    "title": "LT3: A Multi-modular Approach to Automatic Taxonomy Construction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes our contribution to the SemEval-2015 task 17 on \u201cTaxonomy Extraction Evaluation\u201d. We propose a hypernym detection system combining three modules: a lexico-syntactic pattern matcher, a morphosyntactic analyzer and a module retrieving hypernym relations from structured lexical resources. Our system ranked first in the competition when considering the gold standard and manual evaluation, and second in the overall ranking. In addition, the experimental results show that all modules contribute to finding hypernym relations between terms.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 16, 
    "authors": [
      "Els Lefever"
    ], 
    "topics": [
      "SemEval", 
      "Automatic taxonomy construction", 
      "Lexico", 
      "WordNet", 
      "Text corpus", 
      "Flynn's taxonomy"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-1713", 
    "title": "A Unified Framework for Grammar Error Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe the PKU system for the CoNLL-2014 grammar error correction shared task. We propose a unified framework for correcting all types of errors. We use unlabeled news texts instead of large amount of human annotated texts as training data. Based on these data, a tri-gram language model is used to correct the replacement errors while two extra classification models are trained to correct errors related to determiners and prepositions. Our system achieves 25.32% in f0.5 on the original test data and 29.10% on the revised test data.", 
    "year": 2014, 
    "venue": "CoNLL Shared Task", 
    "references": 17, 
    "authors": [
      "Longkai Zhang", 
      "Houfeng Wang"
    ], 
    "topics": [
      "Error detection and correction", 
      "Unified Framework", 
      "Test data", 
      "Language model", 
      "Triangular function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-1010", 
    "title": "Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 59, 
    "authors": [
      "Shikhar Murty", 
      "Pat Verga", 
      "L. Vilnis", 
      "Irena Radovanovic", 
      "A. McCallum"
    ], 
    "topics": [
      "Typing", 
      "PubMed", 
      "WordNet", 
      "Entity linking", 
      "Ontology (information science)", 
      "Freebase", 
      "Knowledge base", 
      "Experiment", 
      "End-to-end principle", 
      "Shallow parsing", 
      "Benchmark (computing)", 
      "Bilinear filtering"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-2020", 
    "title": "LangPro: Natural Language Theorem Prover", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "LangPro is an automated theorem prover for natural language. Given a set of premises and a hypothesis, it is able to prove semantic relations between them. The prover is based on a version of analytic tableau method specially designed for natural logic. The proof procedure operates on logical forms that preserve linguistic expressions to a large extent. %This property makes the logical forms easily obtainable from syntactic trees. %, in particular, Combinatory Categorial Grammar derivation trees. The nature of proofs is deductive and transparent. On the FraCaS and SICK textual entailment datasets, the prover achieves high results comparable to state-of-the-art.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 14, 
    "authors": [
      "Lasha Abzianidze"
    ], 
    "topics": [
      "Natural language", 
      "Method of analytic tableaux", 
      "Automated theorem proving", 
      "Combinatory categorial grammar", 
      "Textual entailment", 
      "Knowledge acquisition", 
      "HTTPS", 
      "First-order logic", 
      "Image scaling", 
      "Syntactic predicate"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_a_00143", 
    "title": "Computing Lexical Contrast", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not classify opposites into the different kinds, however. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and freezing. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly related. (For example, there exists the pair of opposites hot and cold such that tropical is related to hot, and freezing is related to cold.) We will call this the contrast hypothesis.We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we flesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a Roget-like thesaurus. We show how, using four different data sets, we evaluated our approach on two different tasks, solving \u201cmost contrasting word\u201d questions and distinguishing synonyms from opposites. The results are analyzed across four parts of speech and across five different kinds of opposites. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods.", 
    "year": 2013, 
    "venue": "CL", 
    "references": 75, 
    "authors": [
      "Saif M. Mohammad", 
      "B. Dorr", 
      "Graeme Hirst", 
      "Peter D. Turney"
    ], 
    "topics": [
      "Thesaurus", 
      "Lexicon", 
      "Machine translation", 
      "Lexical substitution", 
      "Crowdsourcing", 
      "Amazon Mechanical Turk", 
      "Natural language processing", 
      "Algorithm", 
      "Pointwise mutual information", 
      "Automatic summarization", 
      "Bilingual dictionary", 
      "Compiler", 
      "Information retrieval", 
      "Sudoku solving algorithms", 
      "The Turk", 
      "Computer performance", 
      "Dialog system", 
      "Word-sense disambiguation", 
      "Microsoft Word for Mac", 
      "Experiment", 
      "Context-sensitive grammar", 
      "Download", 
      "Synonym ring", 
      "Jones calculus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.566", 
    "title": "Train No Evil: Selective Masking for Task-guided Pre-training", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, pre-trained language models mostly follow the pre-training-then-fine-tuning paradigm and have achieved great performances on various downstream tasks. However, due to the aimlessness of pre-training and the small in-domain supervised data scale of fine-tuning, the two-stage models typically cannot capture the domain-specific and task-specific language patterns well. In this paper, we propose a selective masking task-guided pre-training method and add it between the general pre-training and fine-tuning. In this stage, we train the masked language modeling task on in-domain unsupervised data, which enables our model to effectively learn the domain-specific language patterns. To efficiently learn the task-specific language patterns, we adopt a selective masking strategy instead of the conventional random masking, which means we only mask the tokens that are important to the downstream task. Specifically, we define the importance of tokens as their impacts on the final classification results and use a neural model to learn the implicit selecting rules. Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50\\% overall computation cost, which indicates our method is both effective and efficient. The source code will be released in the future.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 31, 
    "authors": [
      "Yuxian Gu", 
      "Zhengyan Zhang", 
      "Xiaozhi Wang", 
      "Zhiyuan Liu", 
      "Maosong Sun"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.challengehml-1.7", 
    "title": "Audio-Visual Understanding of Passenger Intents for In-Cabin Conversational Agents", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Building multimodal dialogue understanding capabilities situated in the in-cabin context is crucial to enhance passenger comfort in autonomous vehicle (AV) interaction systems. To this end, understanding passenger intents from spoken interactions and vehicle vision systems is an important building block for developing contextual and visually grounded conversational agents for AV. Towards this goal, we explore AMIE (Automated-vehicle Multimodal In-cabin Experience), the in-cabin agent responsible for handling multimodal passenger-vehicle interactions. In this work, we discuss the benefits of multimodal understanding of in-cabin utterances by incorporating verbal/language input together with the non-verbal/acoustic and visual input from inside and outside the vehicle. Our experimental results outperformed text-only baselines as we achieved improved performances for intent detection with multimodal approach.", 
    "year": 2020, 
    "venue": "CHALLENGEHML", 
    "references": 28, 
    "authors": [
      "Eda Okur", 
      "Shachi H. Kumar", 
      "Saurav Sahay", 
      "L. Nachman"
    ], 
    "topics": [
      "Multimodal interaction", 
      "Dialog system", 
      "Performance", 
      "Text-based user interface", 
      "Autonomous robot", 
      "Situated", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5353", 
    "title": "Evaluating Conjunction Disambiguation on English-to-German and French-to-German WMT 2019 Translation Hypotheses", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a test set for evaluating an MT system\u2019s capability to translate ambiguous conjunctions depending on the sentence structure. We concentrate on the English conjunction \u201cbut\u201d and its French equivalent \u201cmais\u201d which can be translated into two different German conjunctions. We evaluate all English-to-German and French-to-German submissions to the WMT 2019 shared translation task. The evaluation is done mainly automatically, with additional fast manual inspection of unclear cases. All systems almost perfectly recognise the target conjunction \u201caber\u201d, whereas accuracies for the other target conjunction \u201csondern\u201d range from 78% to 97%, and the errors are mostly caused by replacing it with the alternative conjunction \u201caber\u201d. The best performing system for both language pairs is a multilingual Transformer \u201cTartuNLP\u201d system trained on all WMT 2019 language pairs which use the Latin script, indicating that the multilingual approach is beneficial for conjunction disambiguation. As for other system features, such as using synthetic back-translated data, context-aware, hybrid, etc., no particular (dis)advantages can be observed. Qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency, meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. On the other hand, the low ranked systems generally exhibit lower fluency and poor adequacy.", 
    "year": 2019, 
    "venue": "WMT", 
    "references": 9, 
    "authors": [
      "Maja Popovic"
    ], 
    "topics": [
      "Word-sense disambiguation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P18-2118", 
    "title": "A Co-Matching Model for Multi-choice Reading Comprehension", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multi-choice reading comprehension is a challenging task, which involves the matching between a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance.", 
    "year": 2018, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "Shuohang Wang", 
      "Mo Yu", 
      "Shiyu Chang", 
      "Jing Jiang"
    ], 
    "topics": [
      "List comprehension", 
      "Race condition", 
      "Library (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1077", 
    "title": "Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality \u2013 Revisiting the Issue of Meaning Grounded in Syntax", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We revisit Levin's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments. Grasping the meaning components of Levin-classes is known to be hard. We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes. To this end, we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet, VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification (available at www.ukp.tu-darmstadt.de/modality-verbclasses/).", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 61, 
    "authors": [
      "Judith Eckle-Kohler"
    ], 
    "topics": [
      "FrameNet", 
      "VerbNet", 
      "GermaNet", 
      "WordNet", 
      "Semantics (computer science)", 
      "Lexicon", 
      "Syntactic pattern recognition", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.nlposs-1.3", 
    "title": "CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components \u2013 parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network (GNN) libraries. Additionally, we discuss downstream usage and applications of the library, and how it can accelerate research for the NLP community.", 
    "year": 2020, 
    "venue": "NLPOSS", 
    "references": 39, 
    "authors": [
      "Raeid Saqur", 
      "A. Deshpande"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/089120103321337421", 
    "title": "A Systematic Comparison of Various Statistical Alignment Models", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.", 
    "year": 2003, 
    "venue": "CL", 
    "references": 42, 
    "authors": [
      "F. Och", 
      "H. Ney"
    ], 
    "topics": [
      "Heuristic", 
      "Verbmobil", 
      "Smoothing", 
      "S\u00f8rensen\u2013Dice coefficient", 
      "Bilingual dictionary", 
      "Statistical model", 
      "Text corpus", 
      "Markov chain", 
      "First-order predicate", 
      "Viterbi algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/COLI_r_00081", 
    "title": "Parsing Schemata for Practical Text Analysis Carlos G\u00f3mez Rodr\u00edguez (University of A Coru\u00f1a) London: Imperial College Press (Mathematics, computing, language, and life series, edited by Carlos Martin-Vide, volume 1), 2010, xiv+275 pp; hardbound, ISBN 978-1-84816-560-1, $89.00", 
    "fields_of_study": [
      "Mathematics", 
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2011, 
    "venue": "Computational Linguistics", 
    "references": 18, 
    "authors": [
      "Anoop Sarkar"
    ], 
    "topics": [
      "International Standard Book Number", 
      "Parsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/d16-1243", 
    "title": "Learning to Generate Compositional Color Descriptions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\", \"dull\"), and compositional phrases (\"faded teal\") not seen in training.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 25, 
    "authors": [
      "Will Monroe", 
      "Noah D. Goodman", 
      "Christopher Potts"
    ], 
    "topics": [
      "Color", 
      "Recurrent neural network", 
      "Text corpus", 
      "Language model", 
      "Natural language generation", 
      "Vagueness", 
      "Rich Internet application", 
      "Artificial neural network", 
      "Galaxy morphological classification", 
      "Potts model"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.115", 
    "title": "Larger-Context Tagging: When and Why Does It Work?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 49, 
    "authors": [
      "Jinlan Fu", 
      "Liangjing Feng", 
      "Qi Zhang", 
      "Xuanjing Huang", 
      "Pengfei Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2155", 
    "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.", 
    "year": 2017, 
    "venue": "*SEMEVAL", 
    "references": 14, 
    "authors": [
      "Pedro Saleiro", 
      "E. M. Rodrigues", 
      "Carlos Soares", 
      "Eug\u00e9nio C. Oliveira"
    ], 
    "topics": [
      "SemEval", 
      "Word embedding", 
      "Sentiment analysis", 
      "Cosine similarity", 
      "Bag-of-words model", 
      "Preprocessor", 
      "Performance", 
      "Neural Networks"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4615", 
    "title": "Towards Improving Dialogue Topic Tracking Performances with Wikification of Concept Mentions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Dialogue topic tracking aims at analyzing and maintaining topic transitions in on-going dialogues. This paper proposes to utilize Wikification-based features for providing mention-level correspondences to Wikipedia concepts for dialogue topic tracking. The experimental results show that our proposed features can significantly improve the performances of the task in mixed-initiative human-human dialogues.", 
    "year": 2015, 
    "venue": "SIGDIAL Conference", 
    "references": 18, 
    "authors": [
      "Seokhwan Kim", 
      "Rafael E. Banchs", 
      "Haizhou Li"
    ], 
    "topics": [
      "Performance", 
      "Wikipedia", 
      "Kernel method", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S15-2013", 
    "title": "CDTDS: Predicting Paraphrases in Twitter via Support Vector Regression", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we describe a system that recognizes paraphrases in Twitter for tweets that refer to the same topic. The system participated in Task1 of SEMEVAL-2015 and uses a support vector regression machine to predict the degree of similarity. The similarity is then thresholded to create a binary prediction. The model and experimental results are discussed along with future work that could improve the method.", 
    "year": 2015, 
    "venue": "*SEMEVAL", 
    "references": 24, 
    "authors": [
      "Rafael-Michael Karampatsis"
    ], 
    "topics": [
      "Support vector machine", 
      "Binary classification", 
      "Semantic similarity", 
      "SemEval"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5046", 
    "title": "Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.", 
    "year": 2018, 
    "venue": "SIGDIAL Conference", 
    "references": 24, 
    "authors": [
      "Zahra Rahimi", 
      "D. Litman"
    ], 
    "topics": [
      "Brainwave entrainment", 
      "Utility", 
      "Experiment", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-0511", 
    "title": "UW-Stanford System Description for AESW 2016 Shared Task on Grammatical Error Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This is a report on the methods used and results obtained by the UW-Stanford team for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016 on grammatical error detection. This team developed a symbolic grammar-based system augmented with manually defined mal-rules to accommodate and identify instances of highfrequency grammatical errors. System results were entered both for the probabilistic estimation track, where we ranked second, and for the Boolean decision track, where we ranked fourth.", 
    "year": 2016, 
    "venue": "BEA@NAACL-HLT", 
    "references": 13, 
    "authors": [
      "D. Flickinger", 
      "Michael Wayne Goodman", 
      "W. Packard"
    ], 
    "topics": [
      "Error detection and correction", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2028", 
    "title": "Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present directional skip-gram (DSG), a simple but effective enhancement of the skip-gram model by explicitly distinguishing left and right context in word prediction. In doing so, a direction vector is introduced for each word, whose embedding is thus learned by not only word co-occurrence patterns in its context, but also the directions of its contextual words. Theoretical and empirical studies on complexity illustrate that our model can be trained as efficient as the original skip-gram model, when compared to other extensions of the skip-gram model. Experimental results show that our model outperforms others on different datasets in semantic (word similarity measurement) and syntactic (part-of-speech tagging) evaluations, respectively.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 27, 
    "authors": [
      "Yan Song", 
      "Shuming Shi", 
      "Jing Li", 
      "Haisong Zhang"
    ], 
    "topics": [
      "N-gram", 
      "DOCSIS Set-top Gateway", 
      "Microsoft Word for Mac", 
      "Sparse matrix", 
      "Part-of-speech tagging", 
      "SuicideGirls"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-6604", 
    "title": "Exploring Lexical-Semantic Knowledge in the Generation of Novel Riddles in Portuguese", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe an effort towards the automatic generation of novel riddles in Portuguese, ultimately with humour value. Riddle generation fits in the common architecture of a NLG system and may follow different models, described here, all based on features of a concept, acquired from a lexical-semantic knowledge base. Generated riddles were manually assessed by humans, who rated them as fairly interpretable, surprising, and novel, even if with low humour potential.", 
    "year": 2018, 
    "venue": "", 
    "references": 26, 
    "authors": [
      "Hugo Gon\u00e7alo Oliveira", 
      "R. Rodrigues"
    ], 
    "topics": [
      "Computational creativity", 
      "Natural language generation", 
      "Intelligent agent", 
      "Knowledge base", 
      "Computational linguistics", 
      "Computation", 
      "Iteration", 
      "Humans", 
      "FITS", 
      "Action potential", 
      "Score", 
      "Page (document)", 
      "Low sodium diet", 
      "Congenital neurologic anomalies", 
      "Rating (action)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S18-1043", 
    "title": "TCS Research at SemEval-2018 Task 1: Learning Robust Representations using Multi-Attention Architecture", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents system description of our submission to the SemEval-2018 task-1: Affect in tweets for the English language. We combine three different features generated using deep learning models and traditional methods in support vector machines to create a unified ensemble system. A robust representation of a tweet is learned using a multi-attention based architecture which uses a mixture of different pre-trained embeddings. In addition to this analysis of different features is also presented. Our system ranked 2nd, 5th, and 7th in different subtasks among 75 teams.", 
    "year": 2018, 
    "venue": "*SEMEVAL", 
    "references": 25, 
    "authors": [
      "Hardik Meisheri", 
      "Lipika Dey"
    ], 
    "topics": [
      "SemEval", 
      "Deep learning", 
      "Multi-label classification", 
      "Support vector machine", 
      "Test data", 
      "Lexicon", 
      "Sentiment analysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1338", 
    "title": "An Imitation Learning Approach to Unsupervised Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 33, 
    "authors": [
      "Bowen Li", 
      "Lili Mou", 
      "Frank Keller"
    ], 
    "topics": [
      "Parsing", 
      "Reinforcement learning", 
      "Unsupervised learning", 
      "Softmax function", 
      "Language model", 
      "Natural language", 
      "Heuristic (computer science)", 
      "Hoc (programming language)", 
      "Long short-term memory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4218", 
    "title": "Convolutional neural networks for low-resource morpheme segmentation: baseline or state-of-the-art?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We apply convolutional neural networks to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages. We show that both in fully supervised and semi-supervised settings our model beats previous state-of-the-art approaches. We argue that convolutional neural networks reflect local nature of morpheme segmentation better than other neural approaches. Morpheme segmentation consists in dividing a given word to meaningful individual units, morphs, which are surface realizations of underlying abstract morphemes. For example, a word unexpectedly could be segmented as un-expect-edly, and the morpheme -ed may be also realized as -t like in learn-t. The generated segmentation may be used as input representation for machine translation (Mager et al., 2018) or morphological tagging (Matteson et al., 2018) or for automatic annotation of digital linguistic resources. Briefly, information about internal morpheme structure makes the data less sparse since an out-of-vocabulary word may share its morphemes with other words already present in the training set. This helps to recover semantic and morphological properties of an unknown word, which otherwise will be unaccessible. The task of morpheme segmentation is especially important for agglutinative languages, such as Finnish or Turkish, where a word is formed by attaching a sequence of affixes to its stem. This affixes reflect both derivational and inflectional processes. A common example from Turkish is evlerinizden \u2018from your houses\u2019, which is decomposed as: ev ler iniz den house +PL your+PL +ABL The task of morpheme segmentation is even harder for polysynthetic languages: while in agglutinative languages morphemes are usually in one-to-one correspondence with morphological features, for polysynthetic languages this matching is more complex with no clear bound between compound words and sentences. For example, in Chuckchi language the whole phrase \u2018The house broke\u2019 can be expressed as Ga ra semat \u00ecen +PF house break +PF+3SG Consequently, polysynthetic language demonstrate extremely high morpheme-to-word ratio, which leads to high type-token ratio, which makes their automatic processing harder. Even further, this processing is performed in low-resource setting since most polysynthetic languages have only few hundreds or thousands of speakers and consequently tend to lack annotated digital resources. Hence, the algorithms initially designed for less complex languages with more data (mostly for English) may change significantly their properties when applied to low-resource polysynthetic data. That is especially the case for neural methods, which are (often erroneously1) believed to be more data-hungry than earlier approaches. However, in 2019 it is insufficient to just say \u201cneural networks\u201d in case of NLP, since there are various neural networks whose properties may differ significantly. Leaving aside the immense diversity of network architectures, they can be separated in three main categories: the convolutional ones (CNNs), where convolutional windows capture local regularities; the recurrent ones, where GRUs and LSTMs memorize potentially unbounded context; and sequence-to-sequence (seq2seq) models, which perform string transductions using encoder-decoder approach. Among the three, convolutional neural networks are the least see (Zeman et al., 2018) and (Cotterell et al., 2017) that show that both in morphological tagging and automatic word inflection neural networks are clearly superior, though their architecture should be adapted for the lack of data.", 
    "year": 2019, 
    "venue": "", 
    "references": 26, 
    "authors": [
      "A. Sorokin"
    ], 
    "topics": [
      "Convolutional neural network", 
      "Artificial neural network", 
      "Baseline (configuration management)", 
      "Machine translation", 
      "Supervised learning", 
      "Memory segmentation", 
      "Lexical density", 
      "Semi-supervised learning", 
      "Test set", 
      "Vocabulary", 
      "Encoder", 
      "Microsoft Windows", 
      "Sparse matrix", 
      "Semiconductor industry", 
      "One-to-one (data model)", 
      "Algorithm", 
      "OpenEdge Advanced Business Language (ABL)", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.398", 
    "title": "Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent studies have revealed a number of pathologies of neural machine translation (NMT) systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a model or its training algorithm, maximum likelihood estimation (MLE). Most of this evidence was gathered using maximum a posteriori (MAP) decoding, a decision rule aimed at identifying the highest-scoring translation, i.e. the mode. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that beam search strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT\u2019s statistical assumptions nor MLE. In particular, we show that the most likely translations under the model accumulate so little probability mass that the mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 70, 
    "authors": [
      "Bryan Eikema", 
      "W. Aziz"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-6307", 
    "title": "Surface Realization Shared Task 2019 (MSR19): The Team 6 Approach", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR\u201919) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rule-based and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available.", 
    "year": 2019, 
    "venue": "MSR@EMNLP-IJCNLP", 
    "references": 0, 
    "authors": [
      "Thiago Castro Ferreira", 
      "Chris van der Lee", 
      "E. Krahmer"
    ], 
    "topics": [
      "Statistical machine translation", 
      "INDO", 
      "Preprocessor", 
      "Logic programming", 
      "Cost per mille"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981863.981890", 
    "title": "Chart Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases.", 
    "year": 1996, 
    "venue": "ACL", 
    "references": 11, 
    "authors": [
      "M. Kay"
    ], 
    "topics": [
      "Parsing", 
      "Phrases", 
      "Chart", 
      "Partial"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974499.974508", 
    "title": "Applied Text Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.", 
    "year": 1992, 
    "venue": "ANLP", 
    "references": 30, 
    "authors": [
      "Owen Rambow", 
      "Tanya Korelsky"
    ], 
    "topics": [
      "Dictionary", 
      "Joyce", 
      "Logical connective", 
      "Nick McKeown", 
      "Backdoor (computing)", 
      "User interface", 
      "Color gradient", 
      "CA-Realizer", 
      "Scope (computer science)", 
      "Tracing (software)", 
      "XML schema", 
      "Ulysses III"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981251.981286", 
    "title": "Augmenting a Database Knowledge Representation for Natural Language Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The knowledge representation is an important factor in natural language generation since it limits the semantic capabilities of the generation system. This paper identifies several information types in a knowledge representation that can be used to generate meaningful responses to questions about database structure. Creating such a knowledge representation, however, is a long and tedious process. A system is presented which uses the contents of the database to form part of this knowledge representation automatically. It employs three types of world knowledge axioms to ensure that the representation formed is meaningful and contains salient information.", 
    "year": 1982, 
    "venue": "ACL", 
    "references": 10, 
    "authors": [
      "Kathleen F. McCoy"
    ], 
    "topics": [
      "Knowledge representation and reasoning", 
      "Natural language generation", 
      "Commonsense knowledge (artificial intelligence)", 
      "Kathleen McKeown", 
      "Database design", 
      "Entity", 
      "Nick McKeown", 
      "eric"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1196", 
    "title": "Orthographic Syllable as basic unit for SMT between Related Languages", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 34, 
    "authors": [
      "Anoop Kunchukuttan", 
      "P. Bhattacharyya"
    ], 
    "topics": [
      "Syllable", 
      "Orthographic projection", 
      "Parallel text", 
      "Text corpus"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5419", 
    "title": "Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Transfer Learning and Selective data training are two of the many approaches being extensively investigated to improve the quality of Neural Machine Translation systems. This paper presents a series of experiments by applying transfer learning and selective data training for participation in the Bio-medical shared task of WMT19. We have used Information Retrieval to selectively choose related sentences from out-of-domain data and used them as additional training data using transfer learning. We also report the effect of tokenization on translation model performance.", 
    "year": 2019, 
    "venue": "WMT", 
    "references": 36, 
    "authors": [
      "Noor-e-Hira", 
      "Sadaf Abdul-Rauf", 
      "Kiran Kiani", 
      "Ammara Zafar", 
      "R. Nawaz"
    ], 
    "topics": [
      "Information retrieval", 
      "Tokenization (data security)", 
      "BLEU", 
      "Neural machine translation", 
      "Experiment", 
      "Domain adaptation", 
      "Baseline (configuration management)", 
      "Book", 
      "British Informatics Olympiad"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N16-1178", 
    "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a novel, simple convolution neural network (CNN) architecture - multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of word embeddings for sentence classification. MGNC-CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models.", 
    "year": 2016, 
    "venue": "NAACL", 
    "references": 30, 
    "authors": [
      "Ye Zhang", 
      "Stephen Roller", 
      "Byron C. Wallace"
    ], 
    "topics": [
      "Word embedding", 
      "Concatenation", 
      "Baseline (configuration management)", 
      "Convolution", 
      "Artificial neural network", 
      "Feature vector", 
      "Norm (social)", 
      "Matrix regularization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4747", 
    "title": "DCU System Report on the WMT 2017 Multi-modal Machine Translation Task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We report experiments with multi-modal \nneural machine translation models that incorporate global visual features in different parts of the encoder and decoder, and \nuse the VGG19 network to extract features for all images. In our experiments, \nwe explore both different strategies to include global image features and also how \nensembling different models at inference \ntime impact translations. Our submissions \nranked 3rd best for translating from English into French, always improving considerably over an neural machine translation baseline across all language pair evaluated, e.g. an increase of 7.0\u20139.2 METEOR points.", 
    "year": 2017, 
    "venue": "WMT", 
    "references": 17, 
    "authors": [
      "Iacer Calixto", 
      "Koel Dutta Chowdhury", 
      "Qun Liu"
    ], 
    "topics": [
      "Modal logic", 
      "Meteor", 
      "Neural machine translation", 
      "Natural language processing", 
      "Question answering", 
      "Experiment", 
      "Encoder", 
      "Baseline (configuration management)", 
      "Text-based user interface", 
      "Multimodal interaction"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.180", 
    "title": "Modeling the Severity of Complaints in Social Media", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 45, 
    "authors": [
      "Mali Jin", 
      "Nikolaos Aletras"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.repl4nlp-1.8", 
    "title": "Adversarial Training for Commonsense Inference", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We apply small perturbations to word embeddings and minimize the resultant adversarial risk to regularize the model. We exploit a novel combination of two different approaches to estimate these perturbations: 1) using the true label and 2) using the model prediction. Without relying on any human-crafted features, knowledge bases, or additional datasets other than the target datasets, our model boosts the fine-tuning performance of RoBERTa, achieving competitive results on multiple reading comprehension datasets that require commonsense inference.", 
    "year": 2020, 
    "venue": "REPL4NLP", 
    "references": 37, 
    "authors": [
      "Lis Pereira", 
      "Xiaodong Liu", 
      "Fei Cheng", 
      "Masayuki Asahara", 
      "I. Kobayashi"
    ], 
    "topics": [
      "Algorithm", 
      "Knowledge base", 
      "Perturbation theory", 
      "Resultant", 
      "Mathematical model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1156", 
    "title": "Empty Category Detection using Path Features and Distributed Case Frames", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe an approach for machine learning-based empty category detection that is based on the phrase structure analysis of Japanese. The problem is formalized as tree node classification, and we find that the path feature, the sequence of node labels from the current node to the root, is highly effective. We also find that the set of dot products between the word embeddings for a verb and those for case particles can be used as a substitution for case frames. Experiments show that the proposed method outperforms the previous state-of the art method by 68.6% to 73.2% in terms of F-measure.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 23, 
    "authors": [
      "Shunsuke Takeno", 
      "M. Nagata", 
      "Kazuhide Yamamoto"
    ], 
    "topics": [
      "Machine learning", 
      "Parsing", 
      "Word embedding", 
      "Phrase structure rules"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1125", 
    "title": "Jointly Optimizing Diversity and Relevance in Neural Response Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Although recent neural conversation models have shown great potential, they often generate bland and generic responses. While various approaches have been explored to diversify the output of the conversation model, the improvement often comes at the cost of decreased relevance. In this paper, we propose a SpaceFusion model to jointly optimize diversity and relevance that essentially fuses the latent space of a sequence-to-sequence model and that of an autoencoder model by leveraging novel regularization terms. As a result, our approach induces a latent space in which the distance and direction from the predicted response vector roughly match the relevance and diversity, respectively. This property also lends itself well to an intuitive visualization of the latent space. Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong baselines in both diversity and relevance.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 26, 
    "authors": [
      "Xiang Gao", 
      "Sungjin Lee", 
      "Yizhe Zhang", 
      "Chris Brockett", 
      "Michel Galley", 
      "Jianfeng Gao", 
      "W. Dolan"
    ], 
    "topics": [
      "Relevance", 
      "Autoencoder", 
      "Baseline (configuration management)", 
      "Optimizing compiler", 
      "Interpolation", 
      "Matrix regularization"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2515", 
    "title": "Predicting Fine-grained Social Roles with Selectional Preferences", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Selectional preferences, the tendencies of predicates to select for certain semantic classes of arguments, have been successfully applied to a number of tasks in computational linguistics including word sense disambiguation, semantic role labeling, relation extraction, and textual inference. Here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform Twitter. First person uses of verbs that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role.", 
    "year": 2014, 
    "venue": "LTCSS@ACL", 
    "references": 34, 
    "authors": [
      "Charley Beller", 
      "Craig Harman", 
      "Benjamin Van Durme"
    ], 
    "topics": [
      "Social media", 
      "Semantic role labeling", 
      "Word-sense disambiguation", 
      "Relationship extraction", 
      "Computational linguistics", 
      "Word sense", 
      "Binary classification", 
      "Multimedia framework", 
      "Experiment", 
      "Aggregate data", 
      "IBM Notes"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/D18-1021", 
    "title": "Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Jointly representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpus, and automatically generates comparable data via distant supervision using multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitative and quantitative, demonstrate the significance of our method.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 58, 
    "authors": [
      "Yixin Cao", 
      "Lei Hou", 
      "Juan-Zi Li", 
      "Zhiyuan Liu", 
      "Chengjiang Li", 
      "Xu Chen", 
      "Tiansi Dong"
    ], 
    "topics": [
      "Feature learning", 
      "Parallel text", 
      "Entity linking", 
      "Experiment", 
      "Machine learning", 
      "Kilobyte", 
      "Display resolution", 
      "Text corpus", 
      "Align (company)", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D16-1050", 
    "title": "Variational Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.", 
    "year": 2016, 
    "venue": "EMNLP", 
    "references": 40, 
    "authors": [
      "Biao Zhang", 
      "Deyi Xiong", 
      "Jinsong Su", 
      "Hong Duan", 
      "Min Zhang"
    ], 
    "topics": [
      "Neural machine translation", 
      "Calculus of variations", 
      "Variational principle", 
      "Latent variable model", 
      "Artificial neural network", 
      "End-to-end principle", 
      "Approximation algorithm", 
      "Encoder", 
      "Gradient"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.516", 
    "title": "Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 46, 
    "authors": [
      "Haoyu Song", 
      "Yan Wang", 
      "Weinan Zhang", 
      "Xiaojiang Liu", 
      "Ting Liu"
    ], 
    "topics": [
      "Rewrite (programming)", 
      "Delete (SQL)", 
      "Transformer", 
      "Prototype", 
      "Computer performance", 
      "Experiment", 
      "Automatic control"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1192", 
    "title": "CDE-IIITH at SemEval-2016 Task 12: Extraction of Temporal Information from Clinical documents using Machine Learning techniques", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we demonstrate our approach for identification of events, time expressions and temporal relations among them. This work was carried out as part of SemEval-2016 Challenge Task 12: Clinical TempEval. The task comprises six sub-tasks: identification of event spans, time spans and their attributes, document time relation and the narrative container relations among events and time expressions. We have participated in all six subtasks. We have provided with a manually annotated dataset which comprises of training dataset (293 documents), development dataset (147 documents) and 151 documents as test dataset. We have submitted our work as two systems for the challenge. One system is developed using machine learning techniques, Conditional Random Fields (CRF) and Support Vector machines (SVM) and the other system is developed using deep neural network (DNN) techniques. The results show that both systems have given relatively same performance on these tasks.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 20, 
    "authors": [
      "Veera Raghavendra Chikka"
    ], 
    "topics": [
      "Machine learning", 
      "Deep learning", 
      "SemEval", 
      "Support vector machine", 
      "Conditional random field", 
      "Artificial neural network", 
      "Regular expression"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/E14-3010", 
    "title": "A Graph-Based Approach to String Regeneration", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The string regeneration problem is the problem of generating a fluent sentence from a bag of words. We explore the Ngram language model approach to string regeneration. The approach computes the highest probability permutation of the input bag of words under an N-gram language model. We describe a graph-based approach for finding the optimal permutation. The evaluation of the approach on a number of datasets yielded promising results, which were confirmed by conducting a manual evaluation study.", 
    "year": 2014, 
    "venue": "EACL", 
    "references": 57, 
    "authors": [
      "Matic Horvat", 
      "W. Byrne"
    ], 
    "topics": [
      "Graph (discrete mathematics)", 
      "Language model", 
      "Bag-of-words model", 
      "N-gram"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1150", 
    "title": "Large Margin Neural Language Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the \u201cgood\u201d and \u201cbad\u201d sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "Jiaji Huang", 
      "Y. Li", 
      "Wei Ping", 
      "Liang Huang"
    ], 
    "topics": [
      "Language model", 
      "Perplexity", 
      "Machine translation", 
      "Speech recognition", 
      "Algorithm", 
      "BLEU", 
      "End-to-end principle", 
      "Word error rate"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5110", 
    "title": "The Effects of User Features on Twitter Hate Speech Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classification performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.", 
    "year": 2018, 
    "venue": "ALW", 
    "references": 27, 
    "authors": [
      "Elise Fehn Unsv\u00e5g", 
      "Bj\u00f6rn Gamb\u00e4ck"
    ], 
    "topics": [
      "Experiment", 
      "Languages", 
      "Speech synthesis", 
      "Statistical classification"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-tutorials.2", 
    "title": "Event-Centric Natural Language Processing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This tutorial targets researchers and practitioners who are interested in AI technologies that help machines understand natural language text, particularly real-world events described in the text. These include methods to extract the internal structures of an event regarding its protagonist(s), participant(s) and properties, as well as external structures concerning memberships, temporal and causal relations of multiple events. This tutorial will provide audience with a systematic introduction of (i) knowledge representations of events, (ii) various methods for automated extraction, conceptualization and prediction of events and their relations, (iii) induction of event processes and properties, and (iv) a wide range of NLU and commonsense understanding tasks that benefit from aforementioned techniques. We will conclude the tutorial by outlining emerging research problems in this area.", 
    "year": 2021, 
    "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts", 
    "references": 59, 
    "authors": [
      "Muhao Chen", 
      "Hongming Zhang", 
      "Qiang Ning", 
      "Manling Li", 
      "Heng Ji", 
      "K. McKeown", 
      "D. Roth"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-8015", 
    "title": "Rediscovering Greenberg\u2019s Word Order Universals in UD", 
    "fields_of_study": [
      "Philosophy"
    ], 
    "abstract": "This paper discusses an empirical refoundation of selected Greenbergian word order universals based on a data analysis of the Universal Dependencies project. The nature of the data we work on allows us to extract rich details for testing well-known typological universals and constitutes therefore a valuable basis for validating Greenberg\u2019s universals. Our results show that we can refine some Greenbergian universals in a more empirical and accurate way by means of a data-driven typological analysis.", 
    "year": 2019, 
    "venue": "", 
    "references": 38, 
    "authors": [
      "Kim Gerdes", 
      "Sylvain Kahane", 
      "Xinying Chen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1407", 
    "title": "Pathologies of Neural Models Make Interpretations Difficult", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "One way to interpret neural model predictions is to highlight the most important input features\u2014for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word\u2019s importance is determined by either input perturbation\u2014measuring the decrease in model confidence when that word is removed\u2014or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 50, 
    "authors": [
      "Shi Feng", 
      "Eric Wallace", 
      "Alvin Grissom II", 
      "Mohit Iyyer", 
      "Pedro Rodriguez", 
      "Jordan L. Boyd-Graber"
    ], 
    "topics": [
      "Heat map", 
      "Decision boundary", 
      "Speech recognition", 
      "Machine translation", 
      "Gradient", 
      "Pipeline (computing)", 
      "Entropy (information theory)", 
      "Humans", 
      "Perturbation theory", 
      "Experiment", 
      "Downstream (software development)", 
      "Stress testing (software)", 
      "Natural language processing", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-2072", 
    "title": "Methodical Evaluation of Arabic Word Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Many unsupervised learning techniques have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a benchmark for the Arabic language that can be utilized to perform intrinsic evaluation of different word embeddings. We then perform additional extrinsic evaluations of the embeddings based on two NLP tasks.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 11, 
    "authors": [
      "Mohammed Elrazzaz", 
      "Shady Elbassuoni", 
      "K. Shaban", 
      "Chadi Helwe"
    ], 
    "topics": [
      "Word embedding", 
      "Benchmark (computing)", 
      "Document classification", 
      "Unsupervised learning", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S19-2024", 
    "title": "CLARK at SemEval-2019 Task 3: Exploring the Role of Context to Identify Emotion in a Short Conversation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "With text lacking valuable information avail-able in other modalities, context may provide useful information to better detect emotions. In this paper, we do a systematic exploration of the role of context in recognizing emotion in a conversation. We use a Naive Bayes model to show that inferring the mood of the conversation before classifying individual utterances leads to better performance. Additionally, we find that using context while train-ing the model significantly decreases performance. Our approach has the additional bene-fit that its performance rivals a baseline LSTM model while requiring fewer resources.", 
    "year": 2019, 
    "venue": "*SEMEVAL", 
    "references": 21, 
    "authors": [
      "Joseph Cummings", 
      "Jason R. Wilson"
    ], 
    "topics": [
      "Long short-term memory", 
      "Naive Bayes classifier", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.319", 
    "title": "Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 29, 
    "authors": [
      "Luyu Gao", 
      "Xinyi Wang", 
      "Graham Neubig"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-1605", 
    "title": "Functional Distributional Semantics", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", 
    "year": 2016, 
    "venue": "Rep4NLP@ACL", 
    "references": 304, 
    "authors": [
      "Guy Edward Toh Emerson", 
      "Ann A. Copestake"
    ], 
    "topics": [
      "Distributional semantics", 
      "Treebank", 
      "Machine learning", 
      "Feedforward neural network", 
      "Semantics (computer science)", 
      "Restricted Boltzmann machine", 
      "Entity", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N19-1157", 
    "title": "Quantifying the morphosyntactic content of Brown Clusters", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Brown and Exchange word clusters have long been successfully used as word representations in Natural Language Processing (NLP) systems. Their success has been attributed to their seeming ability to represent both semantic and syntactic information. Using corpora representing several language families, we test the hypothesis that Brown and Exchange word clusters are highly effective at encoding morphosyntactic information. Our experiments show that word clusters are highly capable at distinguishing Parts of Speech. We show that increases in Average Mutual Information, the clustering algorithms\u2019 optimization goal, are highly correlated with improvements in encoding of morphosyntactic information. Our results provide empirical evidence that downstream NLP systems addressing tasks dependent on morphosyntactic information can benefit from word cluster features.", 
    "year": 2019, 
    "venue": "NAACL", 
    "references": 23, 
    "authors": [
      "Manuel R. Ciosici", 
      "Leon Derczynski", 
      "I. Assent"
    ], 
    "topics": [
      "Natural language processing", 
      "Mutual information", 
      "Named-entity recognition", 
      "Mathematical optimization", 
      "Parsing", 
      "Experiment", 
      "Downstream (software development)", 
      "Cluster analysis", 
      "Ratner's theorems", 
      "Part-of-speech tagging", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-4337", 
    "title": "The Second Dialog State Tracking Challenge", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step. This process, termed dialog state tracking, is essential for a successful dialog system as it directly informs the system\u2019s actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features \u2010 a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers.", 
    "year": 2014, 
    "venue": "SIGDIAL Conference", 
    "references": 13, 
    "authors": [
      "Matthew Henderson", 
      "Blaise Thomson", 
      "J. Williams"
    ], 
    "topics": [
      "Dialog system", 
      "Ensemble learning", 
      "Spoken dialog systems", 
      "Baseline (configuration management)", 
      "Institute for Operations Research and the Management Sciences", 
      "Elegant degradation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-1080", 
    "title": "What do Neural Machine Translation Models Learn about Morphology?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 54, 
    "authors": [
      "Yonatan Belinkov", 
      "Nadir Durrani", 
      "Fahim Dalvi", 
      "Hassan Sajjad", 
      "James R. Glass"
    ], 
    "topics": [
      "Neural machine translation", 
      "Encoder", 
      "Galaxy morphological classification", 
      "Mathematical morphology", 
      "Text-based (computing)", 
      "Compiler", 
      "End-to-end principle"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/D14-1116", 
    "title": "Question Answering over Linked Data Using First-order Logic", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Question Answering over Linked Data (QALD) aims to evaluate a question answering system over structured data, the key objective of which is to translate questions posed using natural language into structured queries. This technique can help common users to directly access open-structured knowledge on the Web and, accordingly, has attracted much attention. To this end, we propose a novel method using first-order logic. We formulate the knowledge for resolving the ambiguities in the main three steps of QALD (phrase detection, phrase-tosemantic-item mapping and semantic item grouping) as first-order logic clauses in a Markov Logic Network. All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method.", 
    "year": 2014, 
    "venue": "EMNLP", 
    "references": 49, 
    "authors": [
      "Shizhu He", 
      "Kang Liu", 
      "Yuanzhe Zhang", 
      "Liheng Xu", 
      "Jun Zhao"
    ], 
    "topics": [
      "Question answering", 
      "Linked data", 
      "First-order logic", 
      "Markov logic network", 
      "First-order predicate", 
      "World Wide Web", 
      "Benchmark (computing)", 
      "Natural language", 
      "Unified Framework", 
      "Emoticon", 
      "Markov chain", 
      "Yao graph", 
      "Potts model"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/w16-32", 
    "title": "Proceedings of the 5th Workshop on Vision and Language, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, VL@ACL 2016, August 12, Berlin, Germany", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": null, 
    "year": 2016, 
    "venue": "VL@ACL", 
    "references": 0, 
    "authors": [
      "Desmond Elliott", 
      "Stella Frank", 
      "K. Sima'an", 
      "Lucia Specia"
    ], 
    "topics": [
      "Computational linguistics", 
      "Computation"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1034678.1034681", 
    "title": "The Lexical Component of Natural Language Processing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Of the various problems that natural language processing has revealed, polysemy is probably the most frustrating. People deal with polysemy so easily that potential abiguities are overlooked, whereas computers must work hard to do far less well. A linguistic approach generally involves a parser, a lexicon, and some ad hoc rules for using linguistic context to identify the context-appropriate sense. A statistical approach generally involves the use of word co-occurrence statistics to create a semantic hyperspace where each word, regardless of its polysemy, is represented as a single vector. Each approach has strengths and limitations; some combination is often proposed. Various possibilities will be discussed in terms of their psychological plausibility. \n \nComputational linguistics is generally considered to be the branch of engineering that uses computers to do useful things with linguistic signals, but it can also be viewed as an extended test of computational theories of human cognition; it is this latter perspective that psychologists find most interesting. Language provides a critical test for the hypothesis that physical symbol systems are adequate to perform all human cognitive functions. As yet, no adequate system for natural language processing has approached human levels of performance.", 
    "year": 1999, 
    "venue": "ACL", 
    "references": 0, 
    "authors": [
      "G. Miller"
    ], 
    "topics": [
      "Natural language processing", 
      "Lexicon", 
      "Cognition", 
      "Computational linguistics", 
      "Computer", 
      "Computation", 
      "Physical symbol system", 
      "Plausibility structure", 
      "Theory", 
      "Hoc (programming language)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3325", 
    "title": "DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the DCU-Lingo24 submission to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task.", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 29, 
    "authors": [
      "Xiaofeng Wu", 
      "Rejwanul Haque", 
      "Tsuyoshi Okita", 
      "Piyush Arora", 
      "Andy Way", 
      "Qun Liu"
    ], 
    "topics": [
      "Interpolation", 
      "Language model", 
      "Stemming", 
      "Preprocessor", 
      "Microsoft Word for Mac", 
      "OpenStreetMap", 
      "Align (company)", 
      "Linear matrix inequality"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S17-2041", 
    "title": "Sew-Embed at SemEval-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched Wikipedia", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes Sew-Embed, our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the SemEval-2017 Task 2. We leverage the Wikipedia-based concept representations developed by Raganato et al. (2016), and propose an embedded augmentation of their explicit high-dimensional vectors, which we obtain by plugging in an arbitrary word (or sense) embedding representation, and computing a weighted average in the continuous vector space. We evaluate Sew-Embed with two different off-the-shelf embedding representations, and report their performances across all monolingual and cross-lingual benchmarks available for the task. Despite its simplicity, especially compared with supervised or overly tuned approaches, Sew-Embed achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2, score 0.56).", 
    "year": 2017, 
    "venue": "SemEval@ACL", 
    "references": 22, 
    "authors": [
      "Claudio Delli Bovi", 
      "Alessandro Raganato"
    ], 
    "topics": [
      "Wikipedia", 
      "BabelNet", 
      "Sparse matrix", 
      "Language-independent specification", 
      "Embedded system", 
      "Word2vec", 
      "Hyperlink", 
      "Performance", 
      "Testbed"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5018", 
    "title": "Discovering User Groups for Natural Language Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups. In contrast to previous work, these user groups are not specified beforehand, but learned in training. We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.", 
    "year": 2018, 
    "venue": "SIGDIAL Conference", 
    "references": 18, 
    "authors": [
      "Nikos Engonopoulos", 
      "Christoph Teichmann", 
      "Alexander Koller"
    ], 
    "topics": [
      "Natural language generation", 
      "Dialog system", 
      "Statistical model", 
      "Experiment", 
      "Regular expression", 
      "Artificial neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4726", 
    "title": "Semantic Change in the Language of UK Parliamentary Debates", 
    "fields_of_study": [
      "Political Science"
    ], 
    "abstract": "We investigate changes in the meanings of words used in the UK Parliament across two different epochs. We use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period, and explore different ways of obtaining target words for this purpose. We find that semantic changes are generally in line with those found in other corpora, and little evidence that parliamentary language is more static than general English. It also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain.", 
    "year": 2019, 
    "venue": "", 
    "references": 0, 
    "authors": [
      "Gavin Abercrombie", 
      "R. Batista-Navarro"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-7910", 
    "title": "Full valency and the position of enclitics in the Old Czech", 
    "fields_of_study": [
      "Geography"
    ], 
    "abstract": "The paper is focused on the analysis of the relationship between the full valency of the predicate and the position of enclitics in the clause. For this analysis, ones of the oldest Old Czech prose texts were used. We set up the hypothesis the higher the full valency of the predicate, the lower the probability of the occurrence of the enclitic after the initial phrase of the clause \u2013 and test it. The hypothesis was corroborated only for narrative texts. In the case of poetic texts, the hypothesis was rejected.", 
    "year": 2019, 
    "venue": "", 
    "references": 16, 
    "authors": [
      "Radek Cech", 
      "Pavel Kosek", 
      "Olga Navr\u00e1tilov\u00e1", 
      "J\u00e1n Macutek"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-2035", 
    "title": "Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Abstract Attention based deep learning systems have been demonstrated to be the state of the art approach for aspect-level sentiment analysis, however, end-to-end deep neural networks lack flexibility as one can not easily adjust the network to fix an obvious problem, especially when more training data is not available: e.g. when it always predicts positive when seeing the word disappointed. Meanwhile, it is less stressed that attention mechanism is likely to \u201cover-focus\u201d on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage lexicon information so that the model becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the network to have a broader \u201cfocus\u201d on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "Lingxian Bao", 
      "Patrik Lambert", 
      "Toni Badia"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Lexicon", 
      "Overfitting", 
      "Long short-term memory", 
      "Deep learning", 
      "Artificial neural network", 
      "Negentropy", 
      "End-to-end principle", 
      "Numerical analysis", 
      "Blinding (cryptography)", 
      "Power Glove", 
      "Function-Behaviour-Structure ontology"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-2108", 
    "title": "Higher-Order Coreference Resolution with Coarse-to-Fine Inference", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We introduce a fully-differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 17, 
    "authors": [
      "Kenton Lee", 
      "Luheng He", 
      "Luke Zettlemoyer"
    ], 
    "topics": [
      "Algorithmic efficiency", 
      "Learnability", 
      "Approximation", 
      "Interaction", 
      "End-to-end principle", 
      "Iterative method", 
      "Computation", 
      "Benchmark (computing)", 
      "Bilinear filtering", 
      "Natural language processing", 
      "IBM Notes"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.26615/978-954-452-056-4_128", 
    "title": "Named Entity Recognition in Information Security Domain for Russian", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper we discuss the named entity recognition task for Russian texts related to cybersecurity. First of all, we describe the problems that arise in course of labeling unstructured texts from information security domain. We introduce guidelines for human annotators, according to which a corpus has been marked up. Then, a CRF-based system and different neural architectures have been implemented and applied to the corpus. The named entity recognition systems have been evaluated and compared to determine the most efficient one.", 
    "year": 2019, 
    "venue": "RANLP", 
    "references": 16, 
    "authors": [
      "A. Sirotina", 
      "Natalia V. Loukachevitch"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Information security", 
      "Text corpus", 
      "Computer security", 
      "Lexicon", 
      "Named entity", 
      "Conditional random field", 
      "Correctness (computer science)", 
      "Markup language"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-5120", 
    "title": "Ilfhocail: A Lexicon of Irish MWEs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the categorisation of Irish MWEs, and the construction of the first version of a lexicon of Irish MWEs for NLP purposes (Ilfhocail, meaning \u2018Multiwords\u2019), collected from a number of resources. For the purposes of quality assurance, 530 entries of this lexicon were examined and manually annotated for POS information and MWE category.", 
    "year": 2019, 
    "venue": "MWE-WN@ACL", 
    "references": 22, 
    "authors": [
      "Abigail Walsh", 
      "Teresa Lynn", 
      "Jennifer Foster"
    ], 
    "topics": [
      "Lexicon", 
      "Minimal Working Example", 
      "Point of sale", 
      "Categorization", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.674", 
    "title": "NwQM: A Neural Quality Assessment Framework for Wikipedia", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia QualityMonitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 40, 
    "authors": [
      "Bhanu Prakash Reddy Guda", 
      "Sasi Bhusan", 
      "Soumya Sarkar", 
      "Animesh Mukherjee"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/N15-2007", 
    "title": "Entity/Event-Level Sentiment Detection and Inference", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Sentiment analysis aims at recognizing and understanding opinions expressed in languages. Previous work in sentiment analysis focused on extracting explicit opinions, which are directly expressed via sentiment words. However, opinions may be expressed implicitly via inferences over explicit sentiments. For example, in the sentence It is great that he was promoted. versus It is great that he was fired, there is an explicitly positive sentiment in both sentences because of the positive sentiment word great. Previous work may stop here. However, the sentiment toward he in the former sentence is positive, while the sentiment toward he in the later sentence is negative. The sentiments toward he in both sentences are implicit since there is no sentiment word directly modifying he. The implicit opinions are indicated in the text, and they are important for a sentiment analysis system to fully understand the documents. While previous work cannot recognize such implicit sentiment, this thesis contributes to developing an entity/event-level sentiment analysis system to recognize both explicit and implicit sentiments expressed from entities toward entities and events. \n \nSpecifically, we first give the definitions of the entity/event-level sentiment analysis task. Since this is a new task, we develop two corpora serving as resources for this task. The implicit sentiments cannot be recognized merely relying on sentiment lexicons since the implicit sentiments are not directly associated with sentiment words. Inference rules are needed to recognize the implicit sentiments. Instead of developing a rule-based system to automatically infer implicit opinions, we develop computational models which use the inference rules as soft constraints. What\u2019s more important, the models take into account the information not only from sentiment analysis tasks, but also from other Natural Language Processing tasks including information extraction and semantic role labeling. The models jointly solve different NLP tasks in one single model and improve the performances of the tasks. We also contribute to improving recognizing sources of opinions in this thesis. Finally, we conduct an analysis study showing that the idea of sentiment inference defined in this thesis can be applied to Chinese text as well.", 
    "year": 2015, 
    "venue": "NAACL", 
    "references": 191, 
    "authors": [
      "Lingjia Deng"
    ], 
    "topics": [
      "Sentiment analysis", 
      "Semantic role labeling", 
      "Natural language processing", 
      "Text corpus", 
      "Entity", 
      "Information extraction", 
      "Microsoft Word for Mac", 
      "Rule-based system", 
      "Computational model", 
      "Performance", 
      "Lexicon"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P14-2127", 
    "title": "Hierarchical MT Training using Max-Violation Perceptron", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable \u201cviolation-fixing\u201d perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO.", 
    "year": 2014, 
    "venue": "ACL", 
    "references": 24, 
    "authors": [
      "Kai Zhao", 
      "Liang Huang", 
      "Haitao Mi", 
      "Abraham Ittycheriah"
    ], 
    "topics": [
      "Perceptron", 
      "BLEU", 
      "Multi-Environment Real-Time", 
      "Parallel text", 
      "Structured prediction", 
      "Latent variable", 
      "Statistical machine translation", 
      "Discriminative model", 
      "Distortion", 
      "Algorithm"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3727", 
    "title": "Detecting Simultaneously Chinese Grammar Errors Based on a BiLSTM-CRF Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In the process of learning and using Chinese, many learners of Chinese as foreign language(CFL) may have grammar errors due to negative migration of their native languages. This paper introduces our system that can simultaneously diagnose four types of grammatical errors including redundant (R), missing (M), selection (S), disorder (W) in NLPTEA-5 shared task. We proposed a Bidirectional LSTM CRF neural network (BiLSTM-CRF) that combines BiLSTM and CRF without hand-craft features for Chinese Grammatical Error Diagnosis (CGED). Evaluation includes three levels, which are detection level, identification level and position level. At the detection level and identification level, our system got the third recall scores, and achieved good F1 values.", 
    "year": 2018, 
    "venue": "NLP-TEA@ACL", 
    "references": 20, 
    "authors": [
      "Yajun Liu", 
      "Hongying Zan", 
      "Mengjie Zhong", 
      "Hongchao Ma"
    ], 
    "topics": [
      "Conditional random field", 
      "Collocation", 
      "Artificial neural network", 
      "Long short-term memory", 
      "Sensitivity and specificity", 
      "Error detection and correction", 
      "Network model", 
      "Sensor"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.CLPSYCH-1.5", 
    "title": "Demonstrating the Reliability of Self-Annotated Emotion Data", 
    "fields_of_study": [
      "Psychology"
    ], 
    "abstract": "Vent is a specialised iOS/Android social media platform with the stated goal to encourage people to post about their feelings and explicitly label them. In this paper, we study a snapshot of more than 100 million messages obtained from the developers of Vent, together with the labels assigned by the authors of the messages. We establish the quality of the self-annotated data by conducting a qualitative analysis, a vocabulary based analysis, and by training and testing an emotion classifier. We conclude that the self-annotated labels of our corpus are indeed indicative of the emotional contents expressed in the text and thus can support more detailed analyses of emotion expression on social media, such as emotion trajectories and factors influencing them.", 
    "year": 2021, 
    "venue": "CLPSYCH", 
    "references": 24, 
    "authors": [
      "A. Malko", 
      "C\u00e9cile Paris", 
      "Andreas Duenser", 
      "M. Kangas", 
      "Diego Molla", 
      "R. Sparks", 
      "Stephen Wan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/k17-1025", 
    "title": "Knowledge Tracing in Sequential Learning of Inflected Vocabulary", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a feature-rich knowledge tracing method that captures a student\u2019s acquisition and retention of knowledge during a foreign language phrase learning task. We model the student\u2019s behavior as making predictions under a log-linear model, and adopt a neural gating mechanism to model how the student updates their log-linear parameters in response to feedback. The gating mechanism allows the model to learn complex patterns of retention and acquisition for each feature, while the log-linear parameterization results in an interpretable knowledge state. We collect human data and evaluate several versions of the model.", 
    "year": 2017, 
    "venue": "CoNLL", 
    "references": 21, 
    "authors": [
      "Adithya Renduchintala", 
      "Philipp Koehn", 
      "Jason Eisner"
    ], 
    "topics": [
      "Log-linear model", 
      "Structured prediction", 
      "Vocabulary", 
      "Personalization", 
      "Negative feedback", 
      "Machine learning", 
      "Cognition", 
      "Image noise", 
      "Linear model", 
      "Software feature", 
      "Tracing (software)", 
      "Long short-term memory"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.94", 
    "title": "Make Templates Smarter: A Template Based Data2Text System Powered by Text Stitch Model", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural network (NN) based data2text models achieve state-of-the-art (SOTA) performance in most metrics, but they sometimes drop or modify the information in the input, and it is hard to control the generation contents. Moreover, it requires paired training data that are usually expensive to collect. Template-based methods have good fidelity and controllability but require heavy human involvement. We propose a novel template-based data2text system powered by a text stitch model. It ensures fidelity and controllability by using templates to produce the main contents. In addition, it reduces human involvement in template design by using a text stitch model to automatically stitch adjacent template units, which is a step that usually requires careful template design and limits template reusability. The text stitch model can be trained in self-supervised fashion, which only requires free texts. The experiments on a benchmark dataset show that our system outperforms SOTA NN-based systems in fidelity and surpasses template-based systems in diversity and human involvement.", 
    "year": 2020, 
    "venue": "FINDINGS", 
    "references": 15, 
    "authors": [
      "Bingfeng Luo", 
      "Zuo Bai", 
      "Kunfeng Lai", 
      "Jianping Shen"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D17-1322", 
    "title": "Depression and Self-Harm Risk Assessment in Online Forums", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.", 
    "year": 2017, 
    "venue": "EMNLP", 
    "references": 54, 
    "authors": [
      "Andrew Yates", 
      "Arman Cohan", 
      "Nazli Goharian"
    ], 
    "topics": [
      "Social media", 
      "Risk assessment", 
      "Baseline (configuration management)", 
      "Network architecture", 
      "Artificial neural network", 
      "Test data"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1525", 
    "title": "Span-Level Model for Relation Extraction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 26, 
    "authors": [
      "Kalpit Dixit", 
      "Y. Al-Onaizan"
    ], 
    "topics": [
      "Relationship extraction", 
      "Span and div"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3706", 
    "title": "Ranking Relevant Verb Phrases Extracted from Historical Text", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In this paper, we present three approaches to automatic ranking of relevant verb phrases extracted from historical text. These approaches are based on conditional probability, log likelihood ratio, and bagof-words classification respectively. The aim of the ranking in our study is to present verb phrases that have a high probability of describing work at the top of the results list, but the methods are likely to be applicable to other information needs as well. The results are evaluated by use of three different evaluation metrics: precision at k, R-precision, and average precision. In the best setting, 91 out of the top-100 instances in the list are true positives.", 
    "year": 2015, 
    "venue": "LaTeCH@ACL", 
    "references": 17, 
    "authors": [
      "Eva Pettersson", 
      "Be\u00e1ta Megyesi", 
      "Joakim Nivre"
    ], 
    "topics": [
      "Information needs", 
      "Information retrieval", 
      "Bag-of-words model", 
      "Language-independent specification", 
      "Binary data", 
      "Experiment", 
      "Evaluation function"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-demos.21", 
    "title": "Label Noise in Context", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Label noise\u2014incorrectly or ambiguously labeled training examples\u2014can negatively impact model performance. Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process. Examples incorrectly flagged as noise waste reviewers\u2019 time, and correcting label noise without guidance can be difficult. We propose LNIC, a noise-detection method that uses an example\u2019s neighborhood within the training set to (a) reduce false positives and (b) provide an explanation as to why the ex- ample was flagged as noise. We demonstrate on several short-text classification datasets that LNIC outperforms the state of the art on measures of precision and F0.5-score. We also show how LNIC\u2019s training set context helps a reviewer to understand and correct label noise in a dataset. The LNIC tool lowers the barriers to label noise remediation, increasing its utility for NLP practitioners.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 28, 
    "authors": [
      "Michael Desmond", 
      "Catherine Finegan-Dollak", 
      "Jeffrey Boston", 
      "Matthew Arnold"
    ], 
    "topics": [
      "Test set", 
      "Document classification", 
      "Emoticon", 
      "Natural language processing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-5620", 
    "title": "Listwise temporal ordering of events in clinical notes", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present metrics for listwise temporal ordering of events in clinical notes, as well as a baseline listwise temporal ranking model that generates a timeline of events that can be used in downstream medical natural language processing tasks.", 
    "year": 2018, 
    "venue": "Louhi@EMNLP", 
    "references": 16, 
    "authors": [
      "Serena Jeblee", 
      "Graeme Hirst"
    ], 
    "topics": [
      "Natural language processing", 
      "Downstream (software development)", 
      "Timeline", 
      "Baseline (configuration management)"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2020.winlp-1.29", 
    "title": "The human unlikeness of neural language models in next-word prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The training objective of unidirectional language models (LMs) is similar to a psycholinguistic benchmark known as the cloze task, which measures next-word predictability. However, LMs lack the rich set of experiences that people do, and humans can be highly creative. To assess human parity in these models\u2019 training objective, we compare the predictions of three neural language models to those of human participants in a freely available behavioral dataset (Luke & Christianson, 2016). Our results show that while neural models show a close correspondence to human productions, they nevertheless assign insufficient probability to how often speakers guess upcoming words, especially for open-class content words.", 
    "year": 2020, 
    "venue": "WINLP", 
    "references": 0, 
    "authors": [
      "Cassandra L. Jacobs", 
      "Arya D. McCarthy"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.ACL-MAIN.682", 
    "title": "CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu. We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning. By using diagnostic classifiers, we show that current models\u2019 learned representations are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 54, 
    "authors": [
      "Alessandro Suglia", 
      "Ioannis Konstas", 
      "Andrea Vanzo", 
      "E. Bastianelli", 
      "Desmond Elliott", 
      "Stella Frank", 
      "Oliver Lemon"
    ], 
    "topics": [
      "Situated", 
      "Scene graph", 
      "ENCODE"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.metanlp-1.5", 
    "title": "On the cross-lingual transferability of multilingual prototypical models across NLU tasks", 
    "fields_of_study": null, 
    "abstract": "Supervised deep learning-based approaches have been applied to task-oriented dialog and have proven to be effective for limited domain and language applications when a sufficient number of training examples are available. In practice, these approaches suffer from the drawbacks of domain-driven design and under-resourced languages. Domain and language models are supposed to grow and change as the problem space evolves. On one hand, research on transfer learning has demonstrated the cross-lingual ability of multilingual Transformers-based models to learn semantically rich representations. On the other, in addition to the above approaches, meta-learning have enabled the development of task and language learning algorithms capable of far generalization. Through this context, this article proposes to investigate the cross-lingual transferability of using synergistically few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments in natural language understanding tasks on MultiATIS++ corpus shows that our approach substantially improves the observed transfer learning performances between the low and the high resource languages. More generally our approach confirms that the meaningful latent space learned in a given language can be can be generalized to unseen and under-resourced ones using meta-learning.", 
    "year": 2021, 
    "venue": "METANLP", 
    "references": 35, 
    "authors": [
      "Oralie Cattan", 
      "S. Rosset", 
      "Christophe Servan"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/coli.2007.33.1.63", 
    "title": "Answering Clinical Questions with Knowledge-Based and Statistical Techniques", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidence-based medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline.", 
    "year": 2007, 
    "venue": "CL", 
    "references": 70, 
    "authors": [
      "Dina Demner-Fushman", 
      "Jimmy J. Lin"
    ], 
    "topics": [
      "PubMed", 
      "Question answering", 
      "Algorithm", 
      "Information needs", 
      "MEDLINE", 
      "Relevance", 
      "Baseline (configuration management)", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/K19-1044", 
    "title": "Active Learning via Membership Query Synthesis for Semi-Supervised Sentence Classification", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Active learning (AL) is a technique for reducing manual annotation effort during the annotation of training data for machine learning classifiers. For NLP tasks, pool-based and stream-based sampling techniques have been used to select new instances for AL while gen erating new, artificial instances via Membership Query Synthesis was, up to know, considered to be infeasible for NLP problems. We present the first successfull attempt to use Membership Query Synthesis for generating AL queries, using Variational Autoencoders for query generation. We evaluate our approach in a text classification task and demonstrate that query synthesis shows competitive performance to pool-based AL strategies while substantially reducing annotation time", 
    "year": 2019, 
    "venue": "CoNLL", 
    "references": 40, 
    "authors": [
      "Raphael Schumann"
    ], 
    "topics": [
      "Active learning (machine learning)", 
      "Autoencoder", 
      "Natural language processing", 
      "Document classification", 
      "Machine learning", 
      "Dataspaces", 
      "Computational complexity theory", 
      "Information", 
      "Sampling (signal processing)", 
      "Variational principle", 
      "Semiconductor industry"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/N18-1073", 
    "title": "Efficient Sequence Learning with Group Recurrent Networks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recurrent neural networks have achieved state-of-the-art results in many artificial intelligence tasks, such as language modeling, neural machine translation, speech recognition and so on. One of the key factors to these successes is big models. However, training such big models usually takes days or even weeks of time even if using tens of GPU cards. In this paper, we propose an efficient architecture to improve the efficiency of such RNN model training, which adopts the group strategy for recurrent layers, while exploiting the representation rearrangement strategy between layers as well as time steps. To demonstrate the advantages of our models, we conduct experiments on several datasets and tasks. The results show that our architecture achieves comparable or better accuracy comparing with baselines, with a much smaller number of parameters and at a much lower computational cost.", 
    "year": 2018, 
    "venue": "NAACL", 
    "references": 40, 
    "authors": [
      "Fei Gao", 
      "Lijun Wu", 
      "Li Zhao", 
      "Tao Qin", 
      "Xueqi Cheng", 
      "Tie-Yan Liu"
    ], 
    "topics": [
      "Language model", 
      "Speech recognition", 
      "Recurrent neural network", 
      "Algorithmic efficiency", 
      "Experiment", 
      "Computation", 
      "Baseline (configuration management)", 
      "Artificial intelligence", 
      "Automatic summarization", 
      "Neural machine translation", 
      "Artificial neural network", 
      "Random neural network", 
      "Graphics processing unit"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.bionlp-1.19", 
    "title": "Word-Level Alignment of Paper Documents with their Electronic Full-Text Counterparts", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe a simple procedure for the automatic creation of word-level alignments between printed documents and their respective full-text versions. The procedure is unsupervised, uses standard, off-the-shelf components only, and reaches an F-score of 85.01 in the basic setup and up to 86.63 when using pre- and post-processing. Potential areas of application are manual database curation (incl. document triage) and biomedical expression OCR.", 
    "year": 2021, 
    "venue": "BIONLP", 
    "references": 28, 
    "authors": [
      "Christoph M\u00fcller", 
      "Sucheta Ghosh", 
      "Ulrike Wittig", 
      "Maja Rey"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-2213", 
    "title": "Data Warehouse, Bronze, Gold, STEC, Software", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We are building an analytical data warehouse for linguistic data \u2013 primarily lexicons and phonological data \u2013 for languages in the Asia-Pacific region. This paper briefly outlines the project, making the point that the need for improved technology for endangered and low-density language data extends well beyond completion of fieldwork. We suggest that shared task evaluation challenges (STECs) are an appropriate model to follow for creating this technology, and that stocking data warehouses with clean bronze-standard data and baseline tools \u2013 no mean task \u2013 is an effective way to elicit the broad collaboration from linguists and computer scientists needed to create the gold-standard data that STECs require.", 
    "year": 2014, 
    "venue": "", 
    "references": 25, 
    "authors": [
      "D. Cooper"
    ], 
    "topics": [
      "Languages", 
      "Computer scientist", 
      "Documentation", 
      "Field research", 
      "Linguistics", 
      "Shiga-Toxigenic Escherichia coli", 
      "Grand Challenges", 
      "Downstream (software development)", 
      "Lexicon", 
      "Baseline (configuration management)", 
      "Theory", 
      "Computation", 
      "Outlines (document)", 
      "Image scaling"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.metanlp-1.9", 
    "title": "Meta-learning for Classifying Previously Unseen Data Source into Previously Unseen Emotional Categories", 
    "fields_of_study": null, 
    "abstract": "In this paper, we place ourselves in a classification scenario in which the target classes and data type are not accessible during training. We use a meta-learning approach to determine whether or not meta-trained information from common social network data with fine-grained emotion labels can achieve competitive performance on messages labeled with different emotion categories. We leverage few-shot learning to match with the classification scenario and consider metric learning based meta-learning by setting up Prototypical Networks with a Transformer encoder, trained in an episodic fashion. This approach proves to be effective for capturing meta-information from a source emotional tag set to predict previously unseen emotional tags. Even though shifting the data type triggers an expected performance drop, our meta-learning approach achieves decent results when compared to the fully supervised one.", 
    "year": 2021, 
    "venue": "METANLP", 
    "references": 57, 
    "authors": [
      "Ga\u00ebl Guibon", 
      "Matthieu Labeau", 
      "H\u00e9l\u00e8ne Flamein", 
      "Luce Lefeuvre", 
      "C. Clavel"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-4209", 
    "title": "Improved Abusive Comment Moderation with User Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Experimenting with a dataset of approximately 1.6M user comments from a Greek news sports portal, we explore how a state of the art RNN-based moderation method can be improved by adding user embeddings, user type embeddings, user biases, or user type biases. We observe improvements in all cases, with user embeddings leading to the biggest performance gains.", 
    "year": 2017, 
    "venue": "NLPmJ@EMNLP", 
    "references": 20, 
    "authors": [
      "John Pavlopoulos", 
      "Prodromos Malakasiotis", 
      "Juli Bakagianni", 
      "Ion Androutsopoulos"
    ], 
    "topics": [
      "Experiment", 
      "Random neural network"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1515", 
    "title": "Phrase Grounding by Soft-Label Chain Conditional Random Field", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at github.com/liujch1998/SoftLabelCCRF", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 39, 
    "authors": [
      "Jiachen Liu", 
      "J. Hockenmaier"
    ], 
    "topics": [
      "Conditional random field", 
      "Sequence labeling", 
      "Structured prediction", 
      "End-to-end principle", 
      "Approximation algorithm", 
      "Entity"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1064", 
    "title": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We consider the task of named entity recognition for Chinese social media. The long line of work in Chinese NER has focused on formal domains, and NER for social media has been largely restricted to English. We present a new corpus of Weibo messages annotated for both name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 45, 
    "authors": [
      "Nanyun Peng", 
      "Mark Dredze"
    ], 
    "topics": [
      "Social media", 
      "Named-entity recognition", 
      "Baseline (configuration management)", 
      "Text corpus", 
      "Long line (telecommunications)", 
      "Chinese wall"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/W14-3340", 
    "title": "FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes the joint submission of Fondazione Bruno Kessler, Universitat Politde Val` encia and University of Edinburgh to the Quality Estimation tasks of the Workshop on Statistical Machine Translation 2014. We present our submis- sions for Task 1.2, 1.3 and 2. Our systems ranked first for Task 1.2 and for the Binary and Level1 settings in Task 2.", 
    "year": 2014, 
    "venue": "WMT@ACL", 
    "references": 23, 
    "authors": [
      "Jos\u00e9 G. C. de Souza", 
      "J. Gonz\u00e1lez-Rubio", 
      "C. Buck", 
      "M. Turchi", 
      "M. Negri"
    ], 
    "topics": [
      "Ensemble learning", 
      "Statistical machine translation", 
      "Stacking", 
      "Postediting", 
      "Baseline (configuration management)", 
      "Sparse matrix", 
      "Random neural network", 
      "PC Bruno", 
      "Query expansion"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1119149.1119169", 
    "title": "An Analysis of the Joint Venture Japanese Text Prototype and its Effect on System Performance", 
    "fields_of_study": [
      "Engineering", 
      "Computer Science"
    ], 
    "abstract": "The TIPSTER Data Extraction and Fifth Message Understanding Conference (MUC-5) tasks focused on the process of data extraction. This is a procedure in which prespecified types of information are identified within free text, extracted, and inserted automatically within a template. Three TIPSTER contractors -- BBN, GE/CMU, NMSU/Brandeis -- participated in the August '93 MUC-5 evaluation for both the English joint venture (EJV) and English microelectronics (EME) domains and their Japanese-language counterparts, the JJV and JME applications. Two other contractors -- SRI and SRA -- participated in the EJV and JJV domains alone. CMU's Textract system took part in the Japanese-language domains only. Of the five systems that tested in both English and Japanese, all but one scored higher in the Japanese-language applications according to both the summary error-based scores and recall/precision-based metrics. This overall result has lead some participants and observers to suggest that Japanese is an \"easier\" language than English.", 
    "year": 1993, 
    "venue": "TIPSTER", 
    "references": 2, 
    "authors": [
      "Steve Moiorano"
    ], 
    "topics": [
      "Prototype", 
      "Message Understanding Conference", 
      "Encrypted Media Extensions", 
      "Sequence Read Archive"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-3014", 
    "title": "A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We present a demonstration of a neural interactive-predictive system for tackling multimodal sequence to sequence tasks. The system generates text predictions to different sequence to sequence tasks: machine translation, image and video captioning. These predictions are revised by a human agent, who introduces corrections in the form of characters. The system reacts to each correction, providing alternative hypotheses, compelling with the feedback provided by the user. The final objective is to reduce the human effort required during this correction process. This system is implemented following a client-server architecture. For accessing the system, we developed a website, which communicates with the neural model, hosted in a local server. From this website, the different tasks can be tackled following the interactive\u2013predictive framework. We open-source all the code developed for building this system. The demonstration in hosted in http://casmacat.prhlt.upv.es/interactive-seq2seq.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 38, 
    "authors": [
      "\u00c1lvaro Peris", 
      "F. Casacuberta"
    ], 
    "topics": [
      "Machine translation", 
      "Multimodal interaction", 
      "Server (computing)", 
      "Feedback", 
      "Client\u2013server model", 
      "Open-source software", 
      "Hypertext Transfer Protocol"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P17-2001", 
    "title": "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a \u201ccommon root\u201d assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.).", 
    "year": 2017, 
    "venue": "ACL", 
    "references": 20, 
    "authors": [
      "Fei Cheng", 
      "Yusuke Miyao"
    ], 
    "topics": [
      "Long short-term memory", 
      "Relationship extraction", 
      "Entity", 
      "Baseline (configuration management)", 
      "Artificial neural network", 
      "Common Lisp"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-3107", 
    "title": "HANSpeller: A Unified Framework for Chinese Spelling Correction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The number of people learning Chinese as a Foreign Language (CFL) has been booming in recent decades. The problem of spelling error correction for CFL learners increasingly is becoming important. Compared to the regular text spelling check task, more error types need to be considered in CFL cases. In this paper, we propose a unified framework for Chinese spelling correction. Instead of conventional methods, which focus on rules or statistics separately, our approach is based on extended HMM and ranker-based models, together with a rule-based model for further polishing, and a final decision-making step is adopted to decide whether to output the corrections or not. Experimental results on the test data of foreigner's Chinese essays provided by the SIGHAN 2014 bake-off illustrate the performance of our approach.", 
    "year": 2015, 
    "venue": "Int. J. Comput. Linguistics Chin. Lang. Process.", 
    "references": 43, 
    "authors": [
      "Jinhua Xiong", 
      "Qiao Zhang", 
      "Shuiyuan Zhang", 
      "Jianpeng Hou", 
      "Xueqi Cheng"
    ], 
    "topics": [
      "Unified Framework", 
      "Machine learning", 
      "Spell checker", 
      "Logic programming", 
      "Test data", 
      "Chinese wall", 
      "Asch conformity experiments", 
      "Courant\u2013Friedrichs\u2013Lewy condition", 
      "Rule 110"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/981251.981256", 
    "title": "What's in a Semantic Network?", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Ever since Wood's \"What's in a Link\" paper, there has been a growing concern for formalization in the study of knowledge representation. Several arguments have been made that frame representation languages and semantic-network languages are syntactic variants of the first-order predicate calculus (FOPC). The typical argument proceeds by showing how any given frame or network representation can be mapped to a logically isomorphic FOPC representation. For the past two years we have been studying the formalization of knowledge retrievers as well as the representation languages that they operate on. This paper presents a representation language in the notation of FOPC whose form facilitates the design of a semantic-network-like retriever.", 
    "year": 1982, 
    "venue": "ACL", 
    "references": 37, 
    "authors": [
      "James F. Allen", 
      "Alan M. Frisch"
    ], 
    "topics": [
      "Semantic network", 
      "First-order logic", 
      "Knowledge representation and reasoning"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-3103", 
    "title": "A Corpus Analysis of Social Connections and Social Isolation in Adolescents Suffering from Depressive Disorders", 
    "fields_of_study": [
      "Computer Science", 
      "Psychology"
    ], 
    "abstract": "Social connection and social isolation are associated with depressive symptoms, particularly in adolescents and young adults, but how these concepts are documented in clinical notes is unknown. This pilot study aimed to identify the topics relevant to social connection and isolation by analyzing 145 clinical notes from patients with depression diagnosis. We found that providers, including physicians, nurses, social workers, and psychologists, document descriptions of both social connection and social isolation.", 
    "year": 2017, 
    "venue": "CLPsych@ACL", 
    "references": 13, 
    "authors": [
      "Jia-Wen Guo", 
      "D. Mowery", 
      "Djin Lai", 
      "K. Sward", 
      "Mike Conway"
    ], 
    "topics": [
      "Natural language processing", 
      "Information extraction"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.emnlp-main.34", 
    "title": "Q-learning with Language Model for Edit-based Unsupervised Summarization", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Unsupervised methods are promising for abstractive text summarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.", 
    "year": 2020, 
    "venue": "EMNLP", 
    "references": 26, 
    "authors": [
      "Ryosuke Kohita", 
      "Akifumi Wachi", 
      "Yang Zhao", 
      "Ryuki Tachibana"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.findings-emnlp.343", 
    "title": "Inserting Information Bottlenecks for Attribution in Transformers", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.", 
    "year": 2020, 
    "venue": "EMNLP 2020", 
    "references": 27, 
    "authors": [
      "Zhiying Jiang", 
      "Raphael Tang", 
      "Ji Xin", 
      "Jimmy J. Lin"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.308", 
    "title": "VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-theart Transformer variants on WMT14 Englishto-German and English-to-French translation datasets, with gains of up to 1\u223c2 BLEU. 1", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 30, 
    "authors": [
      "Fuli Luo", 
      "Wei Wang", 
      "Jiahao Liu", 
      "Yijia Liu", 
      "Bin Bi", 
      "Songfang Huang", 
      "Fei Huang", 
      "Luo Si"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2020.COLING-MAIN.382", 
    "title": "The Two Shades of Dubbing in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Dubbing has two shades; synchronisation constraints are applied only when the actor\u2019s mouth is visible on screen, while the translation is unconstrained for off-screen dubbing. Consequently, different synchronisation requirements, and therefore translation strategies, are applied depending on the type of dubbing. In this work, we manually annotate an existing dubbing corpus (Heroes) for this dichotomy. We show that, even though we did not observe distinctive features between on- and off-screen dubbing at the textual level, on-screen dubbing is more difficult for MT (-4 BLEU points). Moreover, synchronisation constraints dramatically decrease translation quality for off-screen dubbing. We conclude that, distinguishing between on-screen and off-screen dubbing is necessary for determining successful strategies for dubbing-customised Machine Translation.", 
    "year": 2020, 
    "venue": "COLING", 
    "references": 26, 
    "authors": [
      "Alina Karakanta", 
      "S. Bhattacharya", 
      "Shravan Nayak", 
      "Timo Baumann", 
      "M. Negri", 
      "M. Turchi"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1075218.1075264", 
    "title": "Processing Optimality-theoretic Syntax by Interleaved Chart Parsing and Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The Earley deduction algorithm is extended for the processing of OT syntax based on feature grammars. Due to faithfulness violations, infinitely many candidates must be compared. With the (reasonable) assumptions (i) that OT constraints are descriptions denoting bounded structures and (ii) that every rule recursion in the base grammar incurs some constraint violation, a chart algorithm can be devised. Interleaving parsing and generation permits the application of generation-based optimization even in the parsing task, i.e., for a string input.", 
    "year": 2000, 
    "venue": "ACL", 
    "references": 8, 
    "authors": [
      "Jonas Kuhn"
    ], 
    "topics": [
      "Parsing", 
      "Theory", 
      "Chart parser"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W17-1913", 
    "title": "Measuring the Italian-English lexical gap for action verbs and its impact on translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper describes a method to measure the lexical gap of action verbs in Italian and English by using the IMAGACT ontology of action. The fine-grained categorization of action concepts of the data source allowed to have wide overview of the relation between concepts in the two languages. The calculated lexical gap for both English and Italian is about 30% of the action concepts, much higher than previous results. Beyond this general numbers a deeper analysis has been performed in order to evaluate the impact that lexical gaps can have on translation. In particular a distinction has been made between the cases in which the presence of a lexical gap affects translation correctness and completeness at a semantic level. The results highlight a high percentage of concepts that can be considered hard to translate (about 18% from English to Italian and 20% from Italian to English) and confirms that action verbs are a critical lexical class for translation tasks.", 
    "year": 2017, 
    "venue": "", 
    "references": 22, 
    "authors": [
      "Lorenzo Gregori", 
      "A. Panunzi"
    ], 
    "topics": [
      "Machine translation", 
      "Lexical substitution", 
      "Computer-assisted translation", 
      "Categorization", 
      "Correctness (computer science)", 
      "Languages", 
      "Genetic Translation Process", 
      "Numbers"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-5123", 
    "title": "Using linguistic features longitudinally to predict clinical scores for Alzheimer\u2019s disease and related dementias", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We use a set of 477 lexicosyntactic, acoustic, and semantic features extracted from 393 speech samples in DementiaBank to predict clinical MMSE scores, an indicator of the severity of cognitive decline associated with dementia. We use a bivariate dynamic Bayes net to represent the longitudinal progression of observed linguistic features and MMSE scores over time, and obtain a mean absolute error (MAE) of 3.83 in predicting MMSE, comparable to within-subject interrater standard deviation of 3.9 to 4.8 [1]. When focusing on individuals with more longitudinal samples, we improve MAE to 2.91, which suggests at the importance of longitudinal data collection. Index Terms- Alzheimer\u2019s disease, dementia, Mini-Mental State Examination (MMSE), dynamic Bayes network, feature selection", 
    "year": 2015, 
    "venue": "SLPAT@Interspeech", 
    "references": 30, 
    "authors": [
      "Maria Yancheva", 
      "Kathleen C. Fraser", 
      "F. Rudzicz"
    ], 
    "topics": [
      "Feature selection", 
      "Bayesian network", 
      "Approximation error", 
      "Color gradient", 
      "Bivariate data", 
      "Mental state", 
      "Acoustic cryptanalysis"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W16-2506", 
    "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.", 
    "year": 2016, 
    "venue": "RepEval@ACL", 
    "references": 49, 
    "authors": [
      "Manaal Faruqui", 
      "Yulia Tsvetkov", 
      "Pushpendre Rastogi", 
      "Chris Dyer"
    ], 
    "topics": [
      "Word embedding", 
      "Word lists by frequency", 
      "Natural language processing", 
      "Usability", 
      "Semantic similarity", 
      "Downstream (software development)", 
      "Skip list", 
      "Judgment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00195", 
    "title": "It\u2019s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Annotated data is prerequisite for many NLP applications. Acquiring large-scale annotated corpora is a major bottleneck, requiring significant time and resources. Recent work has proposed turning annotation into a game to increase its appeal and lower its cost; however, current games are largely text-based and closely resemble traditional annotation tasks. We propose a new linguistic annotation paradigm that produces annotations from playing graphical video games. The effectiveness of this design is demonstrated using two video games: one to create a mapping from WordNet senses to images, and a second game that performs Word Sense Disambiguation. Both games produce accurate results. The first game yields annotation quality equal to that of experts and a cost reduction of 73% over equivalent crowdsourcing; the second game provides a 16.3% improvement in accuracy over current state-of-the-art sense disambiguation games with WordNet.", 
    "year": 2014, 
    "venue": "Transactions of the Association for Computational Linguistics", 
    "references": 48, 
    "authors": [
      "David Jurgens", 
      "R. Navigli"
    ], 
    "topics": [
      "Human-based computation game", 
      "WordNet", 
      "Crowdsourcing", 
      "Word sense", 
      "Word-sense disambiguation", 
      "Text-based (computing)", 
      "Graphical user interface", 
      "Anaphora (linguistics)", 
      "Natural language processing", 
      "Bottleneck (engineering)", 
      "Language-independent specification", 
      "Video game development", 
      "Programming paradigm", 
      "Baseline (configuration management)", 
      "Text corpus", 
      "Moose File System", 
      "Attachments", 
      "Denotational semantics", 
      "Ka band", 
      "Web Services for Devices"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.246", 
    "title": "Detecting Harmful Memes and Their Targets", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Among the various modes of communication in social media, the use of Internet memes has emerged as a powerful means to convey political, psychological, and socio-cultural opinions. Although memes are typically humorous in nature, recent days have witnessed a proliferation of harmful memes targeted to abuse various social entities. As most harmful memes are highly satirical and abstruse without appropriate contexts, off-the-shelf multimodal models may not be adequate to understand their underlying semantics. In this work, we propose two novel problem formulations: detecting harmful memes and the social entities that these harmful memes target. To this end, we present HarMeme, the first benchmark dataset, containing 3, 544 memes related to COVID-19. Each meme went through a rigorous two-stage annotation process. In the first stage, we labeled a meme as very harmful, partially harmful, or harmless; in the second stage, we further annotated the type of target(s) that each harmful meme points to: individual, organization, community, or society/general public/other. The evaluation results using ten unimodal and multimodal models highlight the importance of using multimodal signals for both tasks. We further discuss the limitations of these models and we argue that more research is needed to address these problems.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 64, 
    "authors": [
      "Shraman Pramanick", 
      "Dimitar I. Dimitrov", 
      "Rituparna Mukherjee", 
      "Shivam Sharma", 
      "Md. Shad Akhtar", 
      "Preslav Nakov", 
      "Tanmoy Chakraborty"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/S16-1074", 
    "title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.", 
    "year": 2016, 
    "venue": "*SEMEVAL", 
    "references": 20, 
    "authors": [
      "G. Zarrella", 
      "A. Marsh"
    ], 
    "topics": [
      "Hashtag", 
      "SemEval", 
      "Recurrent neural network", 
      "F1 score", 
      "Experiment", 
      "N-gram", 
      "Artificial neural network", 
      "Test set", 
      "Word2vec"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/974557.974582", 
    "title": "Contextual Spelling Correction Using Latent Semantic Analysis", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier.", 
    "year": 1997, 
    "venue": "ANLP", 
    "references": 19, 
    "authors": [
      "Michael P. Jones", 
      "James H. Martin"
    ], 
    "topics": [
      "Latent semantic analysis", 
      "Spell checker"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.findings-acl.23", 
    "title": "Contrastive Attention for Automatic Chest X-ray Report Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-ofthe-art results on the two public datasets.", 
    "year": 2021, 
    "venue": "FINDINGS", 
    "references": 71, 
    "authors": [
      "Fenglin Liu", 
      "C. Yin", 
      "Xian Wu", 
      "Shen Ge", 
      "Ping Zhang", 
      "Xu Sun"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1464", 
    "title": "An Empirical Study of Span Representations in Argumentation Structure Parsing", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 30, 
    "authors": [
      "Tatsuki Kuribayashi", 
      "Hiroki Ouchi", 
      "Naoya Inoue", 
      "Paul Reisert", 
      "Toshinori Miyoshi", 
      "Jun Suzuki", 
      "Kentaro Inui"
    ], 
    "topics": [
      "Parsing", 
      "Natural language processing", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/V1/2021.NAACL-MAIN.294", 
    "title": "An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Volatility prediction is complex due to the stock market\u2019s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives\u2019 speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender-sensitive audio features and fewer female executives in earnings calls of one of the world\u2019s biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes.", 
    "year": 2021, 
    "venue": "NAACL", 
    "references": 55, 
    "authors": [
      "Ramit Sawhney", 
      "Arshiya Aggarwal", 
      "R. Shah"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/V1/2020.ACL-MAIN.365", 
    "title": "Analysing Lexical Semantic Change with Contextualised Word Representations", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 63, 
    "authors": [
      "Mario Giulianelli", 
      "Marco Del Tredici", 
      "R. Fern'andez"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.acl-main.575", 
    "title": "Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.", 
    "year": 2020, 
    "venue": "ACL", 
    "references": 50, 
    "authors": [
      "Hiroki Ouchi", 
      "Jun Suzuki", 
      "Sosuke Kobayashi", 
      "Sho Yokoi", 
      "Tatsuki Kuribayashi", 
      "Ryuto Konno", 
      "Kentaro Inui"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Structured prediction", 
      "Instance-based learning", 
      "Test set"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P19-1236", 
    "title": "Cross-Domain NER using Cross-Domain Language Modeling", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods.", 
    "year": 2019, 
    "venue": "ACL", 
    "references": 32, 
    "authors": [
      "Chen Jia", 
      "Liang Xiao", 
      "Y. Zhang"
    ], 
    "topics": [
      "Named-entity recognition", 
      "Domain adaptation", 
      "Language model", 
      "Supervised learning", 
      "Named entity", 
      "Experiment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/v1/P15-2064", 
    "title": "Cross-lingual Transfer of Named Entity Recognizers without Parallel Corpora", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We propose an approach to cross-lingual named entity recognition model transfer without the use of parallel corpora. In addition to global de-lexicalized features, we introduce multilingual gazetteers that are generated using graph propagation, and cross-lingual word representation mappings without the use of parallel data. We target the e-commerce domain, which is challenging due to its unstructured and noisy nature. The experiments have shown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese.", 
    "year": 2015, 
    "venue": "ACL", 
    "references": 25, 
    "authors": [
      "Ayah Zirikly"
    ], 
    "topics": [
      "Parallel text", 
      "Text corpus", 
      "Named entity", 
      "NER model", 
      "E-commerce", 
      "Experiment", 
      "Finite-state machine", 
      "Baseline (configuration management)", 
      "Software propagation"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.22", 
    "title": "Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search \u2013 the de facto standard inference algorithm in NMT \u2013 and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 39, 
    "authors": [
      "Mathias M\u00fcller", 
      "Rico Sennrich"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.autosimtrans-1.1", 
    "title": "Dynamic Sentence Boundary Detection for Simultaneous Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Simultaneous Translation is a great challenge in which translation starts before the source sentence finished. Most studies take transcription as input and focus on balancing translation quality and latency for each sentence. However, most ASR systems can not provide accurate sentence boundaries in realtime. Thus it is a key problem to segment sentences for the word streaming before translation. In this paper, we propose a novel method for sentence boundary detection that takes it as a multi-class classification task under the end-to-end pre-training framework. Experiments show significant improvements both in terms of translation quality and latency.", 
    "year": 2020, 
    "venue": "AUTOSIMTRANS", 
    "references": 21, 
    "authors": [
      "Ruiqing Zhang", 
      "Chuanqiang Zhang"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-4732", 
    "title": "Gaussian Process Models of Sound Change in Indo-Aryan Dialectology", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "This paper proposes a Gaussian Process model of sound change targeted toward questions in Indo-Aryan dialectology. Gaussian Processes (GPs) provide a flexible means of expressing covariance between outcomes, and can be extended to a wide variety of probability distributions. We find that GP models fare better in terms of some key posterior predictive checks than models that do not express covariance between sound changes, and outline directions for future work.", 
    "year": 2019, 
    "venue": "", 
    "references": 40, 
    "authors": [
      "C. Cathcart"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W18-3404", 
    "title": "Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication", 
    "fields_of_study": [
      "Medicine", 
      "Computer Science"
    ], 
    "abstract": "Icon-based communication systems are widely used in the field of Augmentative and Alternative Communication. Typically, icon-based systems have lagged behind word- and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models. We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies. We propose a method to generate language models for corpus-less symbol-set.", 
    "year": 2018, 
    "venue": "DeepLo@ACL", 
    "references": 19, 
    "authors": [
      "Shiran Dudy", 
      "Steven Bedrick"
    ], 
    "topics": [
      "Language model", 
      "Text-based (computing)"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D19-1682", 
    "title": "Context-Aware Conversation Thread Detection in Multi-Party Chat", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "In multi-party chat, it is common for multiple conversations to occur concurrently, leading to intermingled conversation threads in chat logs. In this work, we propose a novel Context-Aware Thread Detection (CATD) model that automatically disentangles these conversation threads. We evaluate our model on four real-world datasets and demonstrate an overall im-provement in thread detection accuracy over state-of-the-art benchmarks.", 
    "year": 2019, 
    "venue": "EMNLP/IJCNLP", 
    "references": 23, 
    "authors": [
      "M. Tan", 
      "Dakuo Wang", 
      "Yupeng Gao", 
      "Haoyu Wang", 
      "Saloni Potdar", 
      "Xiaoxiao Guo", 
      "Shiyu Chang", 
      "Mo Yu"
    ], 
    "topics": [
      "Beam search", 
      "Benchmark (computing)", 
      "Chat log"
    ]
  }, 
  {
    "is_open_access": false, 
    "doi": "10.18653/v1/2021.acl-long.558", 
    "title": "SpanNER: Named Entity Re-/Recognition as Span Prediction", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model\u2019s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems\u2019 outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: https:// github.com/neulab/spanner, as well as an online system demo: http://spanner. sh. Our model also has been deployed into the EXPLAINABOARD (Liu et al., 2021) platform, which allows users to flexibly perform the system combination of top-scoring systems in an interactive way: http://explainaboard. nlpedia.ai/leaderboard/task-ner/.", 
    "year": 2021, 
    "venue": "ACL/IJCNLP", 
    "references": 60, 
    "authors": [
      "Jinlan Fu", 
      "Xuanjing Huang", 
      "Pengfei Liu"
    ], 
    "topics": []
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W15-4412", 
    "title": "Grammatical Error Correction Considering Multi-word Expressions", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Multi-word expressions (MWEs) have been recognized as important linguistic information and much research has been conducted especially on their extraction and interpretation. On the other hand, they have hardly been used in real application areas. While those who are learning English as a second language (ESL) use MWEs in their writings just like native speakers, MWEs haven\u2019t been taken into consideration in grammatical error correction tasks. In this paper, we investigate the grammatical error correction method using MWEs. Our method proposes a straightforward application of MWEs to grammatical error correction, but experimental results show that MWEs have a beneficial effect on grammatical error correction.", 
    "year": 2015, 
    "venue": "NLP-TEA@ACL/IJCNLP", 
    "references": 19, 
    "authors": [
      "Tomoya Mizumoto", 
      "Masato Mita", 
      "Yuji Matsumoto"
    ], 
    "topics": [
      "Error detection and correction", 
      "Microsoft Word for Mac", 
      "error correction", 
      "Baseline (configuration management)", 
      "Phrasal template", 
      "Linguistics", 
      "F1 score", 
      "Correction of Hearing Impairment"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.3115/1073012.1073036", 
    "title": "Topological Dependency Trees: A Constraint-Based Account of Linear Precedence", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We describe a new framework for dependency grammar, with a modular decomposition of immediate dependency and linear precedence. Our approach distinguishes two orthogonal yet mutually constraining structures: a syntactic dependency tree and a topological dependency tree. The syntax tree is nonprojective and even non-ordered, while the topological tree is projective and partially ordered.", 
    "year": 2001, 
    "venue": "ACL", 
    "references": 19, 
    "authors": [
      "D. Duchier", 
      "Ralph Debusmann"
    ], 
    "topics": [
      "Dependency grammar", 
      "Constraint satisfaction problem", 
      "Modular decomposition", 
      "Parsing", 
      "Parse tree", 
      "Local consistency", 
      "Linear logic", 
      "Formal grammar", 
      "Prototype", 
      "PDF/A", 
      "Semantics (computer science)", 
      "Emoticon", 
      "Software propagation", 
      "Binary tree"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D18-1358", 
    "title": "Knowledge Graph Embedding with Hierarchical Relation Structure", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.", 
    "year": 2018, 
    "venue": "EMNLP", 
    "references": 24, 
    "authors": [
      "Zhao Zhang", 
      "Fuzhen Zhuang", 
      "Meng Qu", 
      "Fen Lin", 
      "Qing He"
    ], 
    "topics": [
      "Knowledge Graph", 
      "Graph embedding", 
      "Multitier architecture", 
      "Freebase", 
      "WordNet", 
      "Baseline (configuration management)", 
      "Entity", 
      "Programming paradigm", 
      "Extended precision", 
      "Elegant degradation", 
      "Entity\u2013relationship model", 
      "Call of Duty: Black Ops"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/P16-1217", 
    "title": "Automatic Labeling of Topic Models Using Text Summaries", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Labeling topics learned by topic models is a challenging problem. Previous studies have used words, phrases and images to label topics. In this paper, we propose to use text summaries for topic labeling. Several sentences are extracted from the most related documents to form the summary for each topic. In order to obtain summaries with both high relevance, coverage and discrimination for all the topics, we propose an algorithm based on submodular optimization. Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious advantages over the use of words and phrases.", 
    "year": 2016, 
    "venue": "ACL", 
    "references": 25, 
    "authors": [
      "Xiaojun Wan", 
      "Tianming Wang"
    ], 
    "topics": [
      "Automatic summarization", 
      "Algorithm", 
      "Submodular set function", 
      "Topic model", 
      "Relevance", 
      "Mathematical optimization", 
      "Coherence (physics)", 
      "Browsing"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/D15-1192", 
    "title": "Learning to Identify the Best Contexts for Knowledge-based WSD", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "We outline a learning framework that aims at identifying useful contextual cues for knowledge-based word sense disambiguation. The usefulness of individual context words is evaluated based on diverse lexico-statistical and syntactic information, as well as simple word distance. Experiments using two different knowledge-based methods and benchmark datasets show significant improvements due to context modeling, beating the conventional window-based approach.", 
    "year": 2015, 
    "venue": "EMNLP", 
    "references": 35, 
    "authors": [
      "Evgenia Wasserman Pritsker", 
      "William W. Cohen", 
      "Einat Minkov"
    ], 
    "topics": [
      "Word-sense disambiguation", 
      "Lexical substitution", 
      "Word sense", 
      "Protologism", 
      "Lexico", 
      "Part-of-speech tagging", 
      "Web Services for Devices", 
      "Word lists by frequency", 
      "Benchmark (computing)", 
      "Knowledge-based systems"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/2020.iwpt-1.8", 
    "title": "Tensors over Semirings for Latent-Variable Weighted Logic Programs", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Semiring parsing is an elegant framework for describing parsers by using semiring weighted logic programs. In this paper we present a generalization of this concept: latent-variable semiring parsing. With our framework, any semiring weighted logic program can be latentified by transforming weights from scalar values of a semiring to rank-n arrays, or tensors, of semiring values, allowing the modelling of latent-variable models within the semiring parsing framework. Semiring is too strong a notion when dealing with tensors, and we have to resort to a weaker structure: a partial semiring. We prove that this generalization preserves all the desired properties of the original semiring framework while strictly increasing its expressiveness.", 
    "year": 2020, 
    "venue": "IWPT", 
    "references": 23, 
    "authors": [
      "Esma Balkir", 
      "D. Gildea", 
      "Shay B. Cohen"
    ], 
    "topics": [
      "Parsing", 
      "Logic programming", 
      "Latent variable"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.1162/tacl_a_00302", 
    "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.", 
    "year": 2020, 
    "venue": "TACL", 
    "references": 100, 
    "authors": [
      "Jian Guan", 
      "Fei Huang", 
      "Zhihao Zhao", 
      "Xiaoyan Zhu", 
      "Minlie Huang"
    ], 
    "topics": [
      "Commonsense knowledge (artificial intelligence)", 
      "Multi-task learning", 
      "Baseline (configuration management)", 
      "Causality", 
      "Natural language generation", 
      "Computer multitasking", 
      "Entity", 
      "Causal filter", 
      "Global serializability"
    ]
  }, 
  {
    "is_open_access": true, 
    "doi": "10.18653/v1/W19-8620", 
    "title": "On Leveraging the Visual Modality for Neural Machine Translation", 
    "fields_of_study": [
      "Computer Science"
    ], 
    "abstract": "Leveraging the visual modality effectively for Neural Machine Translation (NMT) remains an open problem in computational linguistics. Recently, Caglayan et al. posit that the observed gains are limited mainly due to the very simple, short, repetitive sentences of the Multi30k dataset (the only multimodal MT dataset available at the time), which renders the source text sufficient for context. In this work, we further investigate this hypothesis on a new large scale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times longer mean sentence length than Multi30k and no repetition. We propose and evaluate three novel fusion techniques, each of which is designed to ensure the utilization of visual context at different stages of the Sequence-to-Sequence transduction pipeline, even under full linguistic context. However, we still obtain only marginal gains under full linguistic context and posit that visual embeddings extracted from deep vision models (ResNet for Multi30k, ResNext for How2) do not lend themselves to increasing the discriminativeness between the vocabulary elements at token level prediction in NMT. We demonstrate this qualitatively by analyzing attention distribution and quantitatively through Principal Component Analysis, arriving at the conclusion that it is the quality of the visual embeddings rather than the length of sentences, which need to be improved in existing MMT datasets.", 
    "year": 2019, 
    "venue": "INLG", 
    "references": 23, 
    "authors": [
      "Vikas Raunak", 
      "S. K. Choe", 
      "Quanyang Lu", 
      "Yi Xu", 
      "Florian Metze"
    ], 
    "topics": [
      "Neural machine translation", 
      "Principal component analysis", 
      "Computational linguistics", 
      "MPEG media transport", 
      "Transduction (machine learning)", 
      "Vocabulary", 
      "Multimodal interaction", 
      "Marginal model", 
      "Computation", 
      "Rendering (computer graphics)"
    ]
  }
]